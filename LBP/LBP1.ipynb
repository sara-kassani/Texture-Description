{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "#import gzip\n",
    "#import six.moves.cPickle as pickle\n",
    "from keras.models import Sequential, model_from_json,model_from_yaml\n",
    "from keras.layers import Dense, Activation\n",
    "from keras.utils import np_utils\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.optimizers import SGD, Adagrad, Adadelta, Adamax\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "import pickle\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "\n",
    "import pywt\n",
    "from skimage.feature import greycomatrix, greycoprops\n",
    "from skimage.measure import shannon_entropy\n",
    "from scipy import ndimage, stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage import feature\n",
    "import numpy as np\n",
    " \n",
    "class LocalBinaryPatterns:\n",
    "    def __init__(self, numPoints, radius):\n",
    "        # store the number of points and radius\n",
    "        self.numPoints = numPoints\n",
    "        self.radius = radius\n",
    " \n",
    "    def describe(self, image, eps=1e-7):\n",
    "        # compute the Local Binary Pattern representation\n",
    "        # of the image, and then use the LBP representation\n",
    "        # to build the histogram of patterns\n",
    "        lbp = feature.local_binary_pattern(image, self.numPoints, self.radius, method=\"uniform\")\n",
    "        (hist, _) = np.histogram(lbp.ravel(), bins=np.arange(0, self.numPoints + 3), range=(0, self.numPoints + 2))\n",
    "\n",
    "        # normalize the histogram\n",
    "        hist = hist.astype(\"float\")\n",
    "        hist /= (hist.sum() + eps)\n",
    " \n",
    "        # return the histogram of Local Binary Patterns\n",
    "        return hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    import itertools\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "    \n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_statistics(Zin):\n",
    "    #Input: Z, a 2D array, hopefully containing some sort of peak\n",
    "    #Output: cx,cy,sx,sy,skx,sky,kx,ky\n",
    "    #cx and cy are the coordinates of the centroid\n",
    "    #sx and sy are the stardard deviation in the x and y directions\n",
    "    #skx and sky are the skewness in the x and y directions\n",
    "    #kx and ky are the Kurtosis in the x and y directions\n",
    "    #Note: this is not the excess kurtosis. For a normal distribution\n",
    "    #you expect the kurtosis will be 3.0. Just subtract 3 to get the\n",
    "    #excess kurtosis.\n",
    "    \n",
    "    Z = Zin + Zin.min()\n",
    "\n",
    "    h,w = np.shape(Z)\n",
    "\n",
    "    x = range(w)\n",
    "    y = range(h)\n",
    "\n",
    "\n",
    "    #calculate projections along the x and y axes\n",
    "    yp = np.sum(Z,axis=1)\n",
    "    xp = np.sum(Z,axis=0)\n",
    "\n",
    "    #centroid\n",
    "    cx = np.sum(x*xp)/np.sum(xp)\n",
    "    cy = np.sum(y*yp)/np.sum(yp)\n",
    "\n",
    "    #standard deviation\n",
    "    x2 = (x-cx)**2\n",
    "    y2 = (y-cy)**2\n",
    "\n",
    "    sx = np.sqrt( np.sum(x2*xp)/np.sum(xp) )\n",
    "    sy = np.sqrt( np.sum(y2*yp)/np.sum(yp) )\n",
    "\n",
    "    #skewness\n",
    "    x3 = (x-cx)**3\n",
    "    y3 = (y-cy)**3\n",
    "\n",
    "    skx = np.sum(xp*x3)/(np.sum(xp) * sx**3)\n",
    "    sky = np.sum(yp*y3)/(np.sum(yp) * sy**3)\n",
    "\n",
    "    #Kurtosis\n",
    "    x4 = (x-cx)**4\n",
    "    y4 = (y-cy)**4\n",
    "    kx = np.sum(xp*x4)/(np.sum(xp) * sx**4)\n",
    "    ky = np.sum(yp*y4)/(np.sum(yp) * sy**4)\n",
    "\n",
    "\n",
    "    return cx,cy,sx,sy,skx,sky,kx,ky"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_hist(ax, lbp_im):\n",
    "    n_bins = np.size(lbp_im) \n",
    "    plt.bar(x=range(0, int(n_bins)), height=lbp_im.ravel(), color='r')\n",
    "    ax.set_xlabel('LBP uniform patterns', fontsize=10)\n",
    "    ax.set_ylabel('Percentage', fontsize=10)\n",
    "    ax.grid(True)\n",
    "#    ax.xaxis.label.set_size(20)\n",
    "\n",
    "def process_images(img_folder):\n",
    "    X_train = []\n",
    "    y_train = []\n",
    "    for idx, dirs in enumerate(os.listdir(img_folder)):\n",
    "        print(idx, dirs)\n",
    "        actual_folder = os.path.join(img_folder,dirs)\n",
    "        for name in os.listdir(actual_folder):\n",
    "            print('-- ',name)\n",
    "    \n",
    "            im = cv2.imread(os.path.join(actual_folder,name))\n",
    "#            im_denoised = cv2.fastNlMeansDenoisingColored(im,None,1,10,7,21)\n",
    "#            im_noise = im - im_denoised\n",
    "#            im_gray_noise = cv2.cvtColor(im_noise, cv2.COLOR_BGR2GRAY)\n",
    "#            \n",
    "#            fig, ax = plt.subplots (1,3, sharex = False)\n",
    "#            for axi in ax: axi.axis('off')\n",
    "#            ax[0].imshow(im)\n",
    "#            ax[1].imshow(im_denoised)\n",
    "#            ax[2].imshow(im_noise)\n",
    "#            fig.tight_layout()\n",
    "            \n",
    "            \n",
    "            level=4\n",
    "            distances = [1] #[1, 2, 3]\n",
    "            angles = [0] #[0, np.pi/4, np.pi/2, 3*np.pi/4]\n",
    "            properties = ['energy', 'homogeneity', 'contrast', 'energy', 'correlation']\n",
    "            win_rows, win_cols = 3, 3\n",
    "            sigma0 = 1e5  # degree of noise supression \n",
    "\n",
    "            features_WAV = np.array([])            \n",
    "            coeffs_RGB = coeffs_RGB_filtered = [pywt.wavedec2(im[:,:,i], 'db8', level=level) for i in range(0,3)]\n",
    "            im_filtered = np.empty(im.shape,dtype='uint8')\n",
    "            for i_rgb in range(0,3):\n",
    "                for i_level in range(1,level+1):\n",
    "#                    print(i_level)\n",
    "                    im_denoised = []\n",
    "                    for i_vhd in range(0,3):\n",
    "                        im_actual = coeffs_RGB[i_rgb][i_level][i_vhd]\n",
    "                        \n",
    "                        # High-order wavelet features\n",
    "                        features_WAV = np.append(features_WAV, list(image_statistics(im_actual)))\n",
    "                        \n",
    "                        # Wavelet coefficient co-occurrence statistics\n",
    "#                        im_gray_noise = cv2.cvtColor(coeffs_RGB[i_rgb][i_level][i_vhd], cv2.COLOR_BGR2GRAY)\n",
    "#                        glcm = greycomatrix(coeffs_RGB[i_rgb][i_level][i_vhd], distances=distances, angles=angles, symmetric=True, normed=True)\n",
    "#                        features_WAV = np.append(features_WAV, [greycoprops(coeffs_RGB[i_rgb][i_level][i_vhd], prop).ravel() for prop in properties])\n",
    "#                        features_WAV = np.append(features_WAV, shannon_entropy(im_actual))\n",
    "                        \n",
    "                        # SPN   https://stackoverflow.com/questions/16107671/variance-image-in-python-using-gdal-and-a-running-window-approach\n",
    "                        win_mean = ndimage.uniform_filter(im_actual,(win_rows,win_cols))\n",
    "                        win_sqr_mean = ndimage.uniform_filter(im_actual**2,(win_rows,win_cols))\n",
    "                        win_var = win_sqr_mean - win_mean**2\n",
    "                          \n",
    "                        im_denoised.append( im_actual*win_var**2/(win_var**2+sigma0) )\n",
    "                    \n",
    "                    coeffs_RGB_filtered[i_rgb][i_level] = tuple(im_denoised)\n",
    "                im_filtered[:,:,i_rgb] = pywt.waverec2(coeffs_RGB_filtered[i_rgb], 'db8')\n",
    "\n",
    "            im_noise = im-im_filtered\n",
    "            \n",
    "#            fig, ax = plt.subplots (1,3, sharex = False)\n",
    "#            for axi in ax: axi.axis('off')\n",
    "#            ax[0].imshow(im)\n",
    "#            ax[1].imshow(im_filtered)\n",
    "#            ax[2].imshow(im_noise)\n",
    "#            fig.tight_layout()\n",
    "            \n",
    "            \n",
    "#            lbp = LocalBinaryPatterns(24, 8)\n",
    "#            lbp_im = lbp.describe(cv2.cvtColor(im_noise, cv2.COLOR_BGR2GRAY))\n",
    "#            fig, ax = plt.subplots (1, figsize=(6, 4), sharex = True)\n",
    "#            plot_hist(ax,lbp_im)\n",
    "#            plt.tight_layout()\n",
    "            \n",
    "            features_SPN = np.array([])\n",
    "            features_LBP = np.array([])\n",
    "            lbp = LocalBinaryPatterns(24, 8)\n",
    "            coeffs_RGB = [pywt.wavedec2(im_noise[:,:,i], 'db8', level=1) for i in range(0,3)]\n",
    "            for i_rgb in range(0,3):\n",
    "                for i_vhd in range(0,3):\n",
    "                    for i_mom in range(1,9):\n",
    "                        features_SPN = np.append(features_SPN, stats.moment( coeffs_RGB[i_rgb][1][i_vhd].flatten(), moment=i_mom))\n",
    "                    features_LBP = np.append(features_LBP, lbp.describe(coeffs_RGB[i_rgb][1][i_vhd]))\n",
    "                      \n",
    "            X_train.append(np.concatenate((features_WAV,features_SPN,features_LBP)).tolist())\n",
    "            y_train.append(idx)\n",
    "    X_train = np.array(X_train)\n",
    "    y_train = np.array(y_train)\n",
    "    \n",
    "    # REMOVE INVALID FEATURES\n",
    "    bool_invalid_Features = np.any(np.isfinite(X_train), axis=0)\n",
    "    idx_invalid_Features = np.where(np.logical_not( bool_invalid_Features ))\n",
    "    X_train = np.delete(X_train, idx_invalid_Features, axis=1)\n",
    "    \n",
    "    return (X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_logistic_model(input_dim, output_dim):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(output_dim, input_dim=input_dim, activation='sigmoid'))\n",
    "    model.summary()\n",
    "    # compile the model\n",
    "#    sgd = optimizers.SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "#    adamax = Adamax(lr=0.002, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0)\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['binary_accuracy'])\n",
    "    return model\n",
    "\n",
    "def build_neuralnetwork_model(input_dim, output_dim):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(200, input_dim=input_dim, activation='sigmoid'))\n",
    "    model.add(Dense(output_dim, activation='sigmoid'))\n",
    "    model.summary()\n",
    "    # compile the model\n",
    "    model.compile(optimizer='adam', loss='mean_squared_error', metrics=['binary_accuracy'])\n",
    "    return model\n",
    "\n",
    "\n",
    "def train_model(X_train, y_train):\n",
    "    # Split the data into a training set and a test set\n",
    "    #X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n",
    "      \n",
    "    # NORMALIZE DATA\n",
    "    scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "    X_train_norm = scaler.fit_transform(X_train)\n",
    "    #X_retransformed = scaler.inverse_transform(X_transformed)\n",
    "     \n",
    "    batch_size = X_train_norm.shape[0]\n",
    "    nb_classes = y_train.max() +1\n",
    "    nb_epoch = 300\n",
    "    nb_trains = 100\n",
    "    input_dim = X_train_norm.shape[1]\n",
    "       \n",
    "    print(X_train_norm.shape[0], 'train samples')\n",
    "    \n",
    "    # convert class vectors to binary class matrices\n",
    "    Y_train = np_utils.to_categorical(y_train, nb_classes)\n",
    "       \n",
    "    model = []\n",
    "    history = []\n",
    "    score = []\n",
    "    earlyStopping = EarlyStopping(monitor='val_loss', patience=0, verbose=0, mode='auto')\n",
    "    for i_class in range(nb_classes):\n",
    "        model.append(build_logistic_model(input_dim, 1))\n",
    "    #    model.append(build_neuralnetwork_model(input_dim, 1))\n",
    "        for i_train in range(nb_trains):\n",
    "            model[i_class].fit(X_train_norm, Y_train[:,i_class],\n",
    "                                batch_size=batch_size, epochs=nb_epoch,\n",
    "                                verbose=1, validation_split=0.4,\n",
    "                                callbacks=[earlyStopping])\n",
    "        score.append(model[i_class].evaluate(X_train_norm, Y_train[:,i_class], verbose=0))\n",
    "    return model, scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 benign\n",
      "--  100_1.png\n",
      "--  100_2.png\n",
      "--  100_3.png\n",
      "--  100_4.png\n",
      "--  100_5.png\n",
      "--  100_6.png\n",
      "--  100_7.png\n",
      "--  100_8.png\n",
      "--  100_9.png\n",
      "--  101_1.png\n",
      "--  101_2.png\n",
      "--  101_3.png\n",
      "--  101_4.png\n",
      "--  101_5.png\n",
      "--  101_6.png\n",
      "--  101_7.png\n",
      "--  101_8.png\n",
      "--  101_9.png\n",
      "--  102_1.png\n",
      "--  102_2.png\n",
      "--  102_3.png\n",
      "--  102_4.png\n",
      "--  102_5.png\n",
      "--  102_6.png\n",
      "--  102_7.png\n",
      "--  102_8.png\n",
      "--  102_9.png\n",
      "--  103_1.png\n",
      "--  103_2.png\n",
      "--  103_3.png\n",
      "--  103_4.png\n",
      "--  103_5.png\n",
      "--  103_6.png\n",
      "--  103_7.png\n",
      "--  103_8.png\n",
      "--  103_9.png\n",
      "--  104_1.png\n",
      "--  104_2.png\n",
      "--  104_3.png\n",
      "--  104_4.png\n",
      "--  104_5.png\n",
      "--  104_6.png\n",
      "--  104_7.png\n",
      "--  104_8.png\n",
      "--  104_9.png\n",
      "--  105_1.png\n",
      "--  105_2.png\n",
      "--  105_3.png\n",
      "--  105_4.png\n",
      "--  105_5.png\n",
      "--  105_6.png\n",
      "--  105_7.png\n",
      "--  105_8.png\n",
      "--  105_9.png\n",
      "--  106_1.png\n",
      "--  106_2.png\n",
      "--  106_3.png\n",
      "--  106_4.png\n",
      "--  106_5.png\n",
      "--  106_6.png\n",
      "--  106_7.png\n",
      "--  106_8.png\n",
      "--  106_9.png\n",
      "--  108_1.png\n",
      "--  108_2.png\n",
      "--  108_3.png\n",
      "--  108_4.png\n",
      "--  108_5.png\n",
      "--  108_6.png\n",
      "--  108_7.png\n",
      "--  108_8.png\n",
      "--  108_9.png\n",
      "--  10_1.png\n",
      "--  10_2.png\n",
      "--  10_3.png\n",
      "--  10_4.png\n",
      "--  10_5.png\n",
      "--  10_6.png\n",
      "--  10_7.png\n",
      "--  10_8.png\n",
      "--  10_9.png\n",
      "--  110_1.png\n",
      "--  110_2.png\n",
      "--  110_3.png\n",
      "--  110_4.png\n",
      "--  110_5.png\n",
      "--  110_6.png\n",
      "--  110_7.png\n",
      "--  110_8.png\n",
      "--  110_9.png\n",
      "--  111_1.png\n",
      "--  111_2.png\n",
      "--  111_3.png\n",
      "--  111_4.png\n",
      "--  111_5.png\n",
      "--  111_6.png\n",
      "--  111_7.png\n",
      "--  111_8.png\n",
      "--  111_9.png\n",
      "--  112_1.png\n",
      "--  112_2.png\n",
      "--  112_3.png\n",
      "--  112_4.png\n",
      "--  112_5.png\n",
      "--  112_6.png\n",
      "--  112_7.png\n",
      "--  112_8.png\n",
      "--  112_9.png\n",
      "--  113_1.png\n",
      "--  113_2.png\n",
      "--  113_3.png\n",
      "--  113_4.png\n",
      "--  113_5.png\n",
      "--  113_6.png\n",
      "--  113_7.png\n",
      "--  113_8.png\n",
      "--  113_9.png\n",
      "--  115_1.png\n",
      "--  115_2.png\n",
      "--  115_3.png\n",
      "--  115_4.png\n",
      "--  115_5.png\n",
      "--  115_6.png\n",
      "--  115_7.png\n",
      "--  115_8.png\n",
      "--  115_9.png\n",
      "--  116_1.png\n",
      "--  116_2.png\n",
      "--  116_3.png\n",
      "--  116_4.png\n",
      "--  116_5.png\n",
      "--  116_6.png\n",
      "--  116_7.png\n",
      "--  116_8.png\n",
      "--  116_9.png\n",
      "--  117_1.png\n",
      "--  117_2.png\n",
      "--  117_3.png\n",
      "--  117_4.png\n",
      "--  117_5.png\n",
      "--  117_6.png\n",
      "--  117_7.png\n",
      "--  117_8.png\n",
      "--  117_9.png\n",
      "--  118_1.png\n",
      "--  118_2.png\n",
      "--  118_3.png\n",
      "--  118_4.png\n",
      "--  118_5.png\n",
      "--  118_6.png\n",
      "--  118_7.png\n",
      "--  118_8.png\n",
      "--  118_9.png\n",
      "--  11_1.png\n",
      "--  11_2.png\n",
      "--  11_3.png\n",
      "--  11_4.png\n",
      "--  11_5.png\n",
      "--  11_6.png\n",
      "--  11_7.png\n",
      "--  11_8.png\n",
      "--  11_9.png\n",
      "--  120_1.png\n",
      "--  120_2.png\n",
      "--  120_3.png\n",
      "--  120_4.png\n",
      "--  120_5.png\n",
      "--  120_6.png\n",
      "--  120_7.png\n",
      "--  120_8.png\n",
      "--  120_9.png\n",
      "--  122_1.png\n",
      "--  122_2.png\n",
      "--  122_3.png\n",
      "--  122_4.png\n",
      "--  122_5.png\n",
      "--  122_6.png\n",
      "--  122_7.png\n",
      "--  122_8.png\n",
      "--  122_9.png\n",
      "--  125_1.png\n",
      "--  125_2.png\n",
      "--  125_3.png\n",
      "--  125_4.png\n",
      "--  125_5.png\n",
      "--  125_6.png\n",
      "--  125_7.png\n",
      "--  125_8.png\n",
      "--  125_9.png\n",
      "--  126_1.png\n",
      "--  126_2.png\n",
      "--  126_3.png\n",
      "--  126_4.png\n",
      "--  126_5.png\n",
      "--  126_6.png\n",
      "--  126_7.png\n",
      "--  126_8.png\n",
      "--  126_9.png\n",
      "--  127_1p11.jpg\n",
      "--  127_1p12.jpg\n",
      "--  127_1p13.jpg\n",
      "--  127_1p14.jpg\n",
      "--  127_1p15.jpg\n",
      "--  127_1p16.jpg\n",
      "--  127_1p17.jpg\n",
      "--  127_1p21.jpg\n",
      "--  127_1p22.jpg\n",
      "--  127_1p23.jpg\n",
      "--  127_1p24.jpg\n",
      "--  127_1p25.jpg\n",
      "--  127_1p26.jpg\n",
      "--  127_1p27.jpg\n",
      "--  128_1.png\n",
      "--  128_2.png\n",
      "--  128_3.png\n",
      "--  128_4.png\n",
      "--  128_5.png\n",
      "--  128_6.png\n",
      "--  128_7.png\n",
      "--  128_8.png\n",
      "--  128_9.png\n",
      "--  129_1.png\n",
      "--  129_2.png\n",
      "--  129_3.png\n",
      "--  129_4.png\n",
      "--  129_5.png\n",
      "--  129_6.png\n",
      "--  129_7.png\n",
      "--  129_8.png\n",
      "--  129_9.png\n",
      "--  130_1.png\n",
      "--  130_1p11.jpg\n",
      "--  130_1p12.jpg\n",
      "--  130_1p13.jpg\n",
      "--  130_1p14.jpg\n",
      "--  130_1p15.jpg\n",
      "--  130_1p16.jpg\n",
      "--  130_1p17.jpg\n",
      "--  130_2.png\n",
      "--  130_2p11.jpg\n",
      "--  130_2p12.jpg\n",
      "--  130_2p13.jpg\n",
      "--  130_2p14.jpg\n",
      "--  130_2p15.jpg\n",
      "--  130_2p16.jpg\n",
      "--  130_2p17.jpg\n",
      "--  130_3.png\n",
      "--  130_4.png\n",
      "--  130_5.png\n",
      "--  130_6.png\n",
      "--  130_7.png\n",
      "--  130_8.png\n",
      "--  130_9.png\n",
      "--  131_1.png\n",
      "--  131_2.png\n",
      "--  131_3.png\n",
      "--  131_4.png\n",
      "--  131_5.png\n",
      "--  131_6.png\n",
      "--  131_7.png\n",
      "--  131_8.png\n",
      "--  131_9.png\n",
      "--  133_1.png\n",
      "--  133_2.png\n",
      "--  133_3.png\n",
      "--  133_4.png\n",
      "--  133_5.png\n",
      "--  133_6.png\n",
      "--  133_7.png\n",
      "--  133_8.png\n",
      "--  133_9.png\n",
      "--  134_1p11.jpg\n",
      "--  134_1p12.jpg\n",
      "--  134_1p13.jpg\n",
      "--  134_1p14.jpg\n",
      "--  134_1p15.jpg\n",
      "--  134_1p16.jpg\n",
      "--  134_1p17.jpg\n",
      "--  134_1p21.jpg\n",
      "--  134_1p22.jpg\n",
      "--  134_1p23.jpg\n",
      "--  134_1p24.jpg\n",
      "--  134_1p25.jpg\n",
      "--  134_1p26.jpg\n",
      "--  134_1p27.jpg\n",
      "--  135_1.png\n",
      "--  135_2.png\n",
      "--  135_3.png\n",
      "--  135_4.png\n",
      "--  135_5.png\n",
      "--  135_6.png\n",
      "--  135_7.png\n",
      "--  135_8.png\n",
      "--  135_9.png\n",
      "--  137_1.png\n",
      "--  137_2.png\n",
      "--  137_3.png\n",
      "--  137_4.png\n",
      "--  137_5.png\n",
      "--  137_6.png\n",
      "--  137_7.png\n",
      "--  137_8.png\n",
      "--  137_9.png\n",
      "--  139_1.png\n",
      "--  139_2.png\n",
      "--  139_3.png\n",
      "--  139_4.png\n",
      "--  139_5.png\n",
      "--  139_6.png\n",
      "--  139_7.png\n",
      "--  139_8.png\n",
      "--  139_9.png\n",
      "--  140_1.png\n",
      "--  140_2.png\n",
      "--  140_3.png\n",
      "--  140_4.png\n",
      "--  140_5.png\n",
      "--  140_6.png\n",
      "--  140_7.png\n",
      "--  140_8.png\n",
      "--  140_9.png\n",
      "--  141_1.png\n",
      "--  141_2.png\n",
      "--  141_3.png\n",
      "--  141_4.png\n",
      "--  141_5.png\n",
      "--  141_6.png\n",
      "--  141_7.png\n",
      "--  141_8.png\n",
      "--  141_9.png\n",
      "--  142_1.png\n",
      "--  142_2.png\n",
      "--  142_3.png\n",
      "--  142_4.png\n",
      "--  142_5.png\n",
      "--  142_6.png\n",
      "--  142_7.png\n",
      "--  142_8.png\n",
      "--  142_9.png\n",
      "--  143_1.png\n",
      "--  143_2.png\n",
      "--  143_3.png\n",
      "--  143_4.png\n",
      "--  143_5.png\n",
      "--  143_6.png\n",
      "--  143_7.png\n",
      "--  143_8.png\n",
      "--  143_9.png\n",
      "--  144_1.png\n",
      "--  144_2.png\n",
      "--  144_3.png\n",
      "--  144_4.png\n",
      "--  144_5.png\n",
      "--  144_6.png\n",
      "--  144_7.png\n",
      "--  144_8.png\n",
      "--  144_9.png\n",
      "--  147_1.png\n",
      "--  147_2.png\n",
      "--  147_3.png\n",
      "--  147_4.png\n",
      "--  147_5.png\n",
      "--  147_6.png\n",
      "--  147_7.png\n",
      "--  147_8.png\n",
      "--  147_9.png\n",
      "--  148_1.png\n",
      "--  148_2.png\n",
      "--  148_3.png\n",
      "--  148_4.png\n",
      "--  148_5.png\n",
      "--  148_6.png\n",
      "--  148_7.png\n",
      "--  148_8.png\n",
      "--  148_9.png\n",
      "--  149_1.png\n",
      "--  149_2.png\n",
      "--  149_3.png\n",
      "--  149_4.png\n",
      "--  149_5.png\n",
      "--  149_6.png\n",
      "--  149_7.png\n",
      "--  149_8.png\n",
      "--  149_9.png\n",
      "--  14_1p11.jpg\n",
      "--  14_1p12.jpg\n",
      "--  14_1p13.jpg\n",
      "--  14_1p14.jpg\n",
      "--  14_1p15.jpg\n",
      "--  14_1p16.jpg\n",
      "--  14_1p17.jpg\n",
      "--  14_1p21.jpg\n",
      "--  14_1p22.jpg\n",
      "--  14_1p23.jpg\n",
      "--  14_1p24.jpg\n",
      "--  14_1p25.jpg\n",
      "--  14_1p26.jpg\n",
      "--  14_1p27.jpg\n",
      "--  150_1.png\n",
      "--  150_2.png\n",
      "--  150_3.png\n",
      "--  150_4.png\n",
      "--  150_5.png\n",
      "--  150_6.png\n",
      "--  150_7.png\n",
      "--  150_8.png\n",
      "--  150_9.png\n",
      "--  155_1.png\n",
      "--  155_2.png\n",
      "--  155_3.png\n",
      "--  155_4.png\n",
      "--  155_5.png\n",
      "--  155_6.png\n",
      "--  155_7.png\n",
      "--  155_8.png\n",
      "--  155_9.png\n",
      "--  157_1.png\n",
      "--  157_2.png\n",
      "--  157_3.png\n",
      "--  157_4.png\n",
      "--  157_5.png\n",
      "--  157_6.png\n",
      "--  157_7.png\n",
      "--  157_8.png\n",
      "--  157_9.png\n",
      "--  158_1.png\n",
      "--  158_2.png\n",
      "--  158_3.png\n",
      "--  158_4.png\n",
      "--  158_5.png\n",
      "--  158_6.png\n",
      "--  158_7.png\n",
      "--  158_8.png\n",
      "--  158_9.png\n",
      "--  159_1.png\n",
      "--  159_2.png\n",
      "--  159_3.png\n",
      "--  159_4.png\n",
      "--  159_5.png\n",
      "--  159_6.png\n",
      "--  159_7.png\n",
      "--  159_8.png\n",
      "--  159_9.png\n",
      "--  160_1.png\n",
      "--  160_1p11.jpg\n",
      "--  160_1p12.jpg\n",
      "--  160_1p13.jpg\n",
      "--  160_1p14.jpg\n",
      "--  160_1p15.jpg\n",
      "--  160_1p16.jpg\n",
      "--  160_1p17.jpg\n",
      "--  160_2.png\n",
      "--  160_3.png\n",
      "--  160_4.png\n",
      "--  160_5.png\n",
      "--  160_6.png\n",
      "--  160_7.png\n",
      "--  160_8.png\n",
      "--  160_9.png\n",
      "--  161_1.png\n",
      "--  161_2.png\n",
      "--  161_3.png\n",
      "--  161_4.png\n",
      "--  161_5.png\n",
      "--  161_6.png\n",
      "--  161_7.png\n",
      "--  161_8.png\n",
      "--  161_9.png\n",
      "--  162_1.png\n",
      "--  162_2.png\n",
      "--  162_3.png\n",
      "--  162_4.png\n",
      "--  162_5.png\n",
      "--  162_6.png\n",
      "--  162_7.png\n",
      "--  162_8.png\n",
      "--  162_9.png\n",
      "--  163_1.png\n",
      "--  163_2.png\n",
      "--  163_3.png\n",
      "--  163_4.png\n",
      "--  163_5.png\n",
      "--  163_6.png\n",
      "--  163_7.png\n",
      "--  163_8.png\n",
      "--  163_9.png\n",
      "--  164_1.png\n",
      "--  164_2.png\n",
      "--  164_3.png\n",
      "--  164_4.png\n",
      "--  164_5.png\n",
      "--  164_6.png\n",
      "--  164_7.png\n",
      "--  164_8.png\n",
      "--  164_9.png\n",
      "--  16_1.png\n",
      "--  16_1p11.jpg\n",
      "--  16_1p12.jpg\n",
      "--  16_1p13.jpg\n",
      "--  16_1p14.jpg\n",
      "--  16_1p15.jpg\n",
      "--  16_1p16.jpg\n",
      "--  16_1p17.jpg\n",
      "--  16_2.png\n",
      "--  16_3.png\n",
      "--  16_4.png\n",
      "--  16_5.png\n",
      "--  16_6.png\n",
      "--  16_7.png\n",
      "--  16_8.png\n",
      "--  16_9.png\n",
      "--  17_1p11.jpg\n",
      "--  17_1p12.jpg\n",
      "--  17_1p13.jpg\n",
      "--  17_1p14.jpg\n",
      "--  17_1p15.jpg\n",
      "--  17_1p16.jpg\n",
      "--  17_1p17.jpg\n",
      "--  18_1.png\n",
      "--  18_2.png\n",
      "--  18_3.png\n",
      "--  18_4.png\n",
      "--  18_5.png\n",
      "--  18_6.png\n",
      "--  18_7.png\n",
      "--  18_8.png\n",
      "--  18_9.png\n",
      "--  190_1p11.jpg\n",
      "--  190_1p12.jpg\n",
      "--  190_1p13.jpg\n",
      "--  190_1p14.jpg\n",
      "--  190_1p15.jpg\n",
      "--  190_1p16.jpg\n",
      "--  190_1p17.jpg\n",
      "--  193_1p11.jpg\n",
      "--  193_1p12.jpg\n",
      "--  193_1p13.jpg\n",
      "--  193_1p14.jpg\n",
      "--  193_1p15.jpg\n",
      "--  193_1p16.jpg\n",
      "--  193_1p17.jpg\n",
      "--  1_1.png\n",
      "--  1_2.png\n",
      "--  1_3.png\n",
      "--  1_4.png\n",
      "--  1_5.png\n",
      "--  1_6.png\n",
      "--  1_7.png\n",
      "--  1_8.png\n",
      "--  1_9.png\n",
      "--  204_1p11.jpg\n",
      "--  204_1p12.jpg\n",
      "--  204_1p13.jpg\n",
      "--  204_1p14.jpg\n",
      "--  204_1p15.jpg\n",
      "--  204_1p16.jpg\n",
      "--  204_1p17.jpg\n",
      "--  22_1.png\n",
      "--  22_1p11.jpg\n",
      "--  22_1p12.jpg\n",
      "--  22_1p13.jpg\n",
      "--  22_1p14.jpg\n",
      "--  22_1p15.jpg\n",
      "--  22_1p16.jpg\n",
      "--  22_1p17.jpg\n",
      "--  22_2.png\n",
      "--  22_3.png\n",
      "--  22_4.png\n",
      "--  22_5.png\n",
      "--  22_6.png\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--  22_7.png\n",
      "--  22_8.png\n",
      "--  22_9.png\n",
      "--  23_1.png\n",
      "--  23_2.png\n",
      "--  23_3.png\n",
      "--  23_4.png\n",
      "--  23_5.png\n",
      "--  23_6.png\n",
      "--  23_7.png\n",
      "--  23_8.png\n",
      "--  23_9.png\n",
      "--  247_1p11.jpg\n",
      "--  247_1p12.jpg\n",
      "--  247_1p13.jpg\n",
      "--  247_1p14.jpg\n",
      "--  247_1p15.jpg\n",
      "--  247_1p16.jpg\n",
      "--  247_1p17.jpg\n",
      "--  247_1p21.jpg\n",
      "--  247_1p22.jpg\n",
      "--  247_1p23.jpg\n",
      "--  247_1p24.jpg\n",
      "--  247_1p25.jpg\n",
      "--  247_1p26.jpg\n",
      "--  247_1p27.jpg\n",
      "--  24_1.png\n",
      "--  24_2.png\n",
      "--  24_3.png\n",
      "--  24_4.png\n",
      "--  24_5.png\n",
      "--  24_6.png\n",
      "--  24_7.png\n",
      "--  24_8.png\n",
      "--  24_9.png\n",
      "--  25_1.png\n",
      "--  25_1p11.jpg\n",
      "--  25_1p12.jpg\n",
      "--  25_1p13.jpg\n",
      "--  25_1p14.jpg\n",
      "--  25_1p15.jpg\n",
      "--  25_1p16.jpg\n",
      "--  25_1p17.jpg\n",
      "--  25_2.png\n",
      "--  25_3.png\n",
      "--  25_4.png\n",
      "--  25_5.png\n",
      "--  25_6.png\n",
      "--  25_7.png\n",
      "--  25_8.png\n",
      "--  25_9.png\n",
      "--  26_1.png\n",
      "--  26_2.png\n",
      "--  26_3.png\n",
      "--  26_4.png\n",
      "--  26_5.png\n",
      "--  26_6.png\n",
      "--  26_7.png\n",
      "--  26_8.png\n",
      "--  26_9.png\n",
      "--  270_1p11.jpg\n",
      "--  270_1p12.jpg\n",
      "--  270_1p13.jpg\n",
      "--  270_1p14.jpg\n",
      "--  270_1p15.jpg\n",
      "--  270_1p16.jpg\n",
      "--  270_1p17.jpg\n",
      "--  270_1p21.jpg\n",
      "--  270_1p22.jpg\n",
      "--  270_1p23.jpg\n",
      "--  270_1p24.jpg\n",
      "--  270_1p25.jpg\n",
      "--  270_1p26.jpg\n",
      "--  270_1p27.jpg\n",
      "--  285_1p11.jpg\n",
      "--  285_1p12.jpg\n",
      "--  285_1p13.jpg\n",
      "--  285_1p14.jpg\n",
      "--  285_1p15.jpg\n",
      "--  285_1p16.jpg\n",
      "--  285_1p17.jpg\n",
      "--  292_1p11.jpg\n",
      "--  292_1p12.jpg\n",
      "--  292_1p13.jpg\n",
      "--  292_1p14.jpg\n",
      "--  292_1p15.jpg\n",
      "--  292_1p16.jpg\n",
      "--  292_1p17.jpg\n",
      "--  29_1.png\n",
      "--  29_2.png\n",
      "--  29_3.png\n",
      "--  29_4.png\n",
      "--  29_5.png\n",
      "--  29_6.png\n",
      "--  29_7.png\n",
      "--  29_8.png\n",
      "--  29_9.png\n",
      "--  2_1p11.jpg\n",
      "--  2_1p12.jpg\n",
      "--  2_1p13.jpg\n",
      "--  2_1p14.jpg\n",
      "--  2_1p15.jpg\n",
      "--  2_1p16.jpg\n",
      "--  2_1p17.jpg\n",
      "--  303_1p11.jpg\n",
      "--  303_1p12.jpg\n",
      "--  303_1p13.jpg\n",
      "--  303_1p14.jpg\n",
      "--  303_1p15.jpg\n",
      "--  303_1p16.jpg\n",
      "--  303_1p17.jpg\n",
      "--  305_1p11.jpg\n",
      "--  305_1p12.jpg\n",
      "--  305_1p13.jpg\n",
      "--  305_1p14.jpg\n",
      "--  305_1p15.jpg\n",
      "--  305_1p16.jpg\n",
      "--  305_1p17.jpg\n",
      "--  309_1p11.jpg\n",
      "--  309_1p12.jpg\n",
      "--  309_1p13.jpg\n",
      "--  309_1p14.jpg\n",
      "--  309_1p15.jpg\n",
      "--  309_1p16.jpg\n",
      "--  309_1p17.jpg\n",
      "--  309_1p21.jpg\n",
      "--  309_1p22.jpg\n",
      "--  309_1p23.jpg\n",
      "--  309_1p24.jpg\n",
      "--  309_1p25.jpg\n",
      "--  309_1p26.jpg\n",
      "--  309_1p27.jpg\n",
      "--  30_1.png\n",
      "--  30_2.png\n",
      "--  30_3.png\n",
      "--  30_4.png\n",
      "--  30_5.png\n",
      "--  30_6.png\n",
      "--  30_7.png\n",
      "--  30_8.png\n",
      "--  30_9.png\n",
      "--  310_1p11.jpg\n",
      "--  310_1p12.jpg\n",
      "--  310_1p13.jpg\n",
      "--  310_1p14.jpg\n",
      "--  310_1p15.jpg\n",
      "--  310_1p16.jpg\n",
      "--  310_1p17.jpg\n",
      "--  31_1.png\n",
      "--  31_1p11.jpg\n",
      "--  31_1p12.jpg\n",
      "--  31_1p13.jpg\n",
      "--  31_1p14.jpg\n",
      "--  31_1p15.jpg\n",
      "--  31_1p16.jpg\n",
      "--  31_1p17.jpg\n",
      "--  31_2.png\n",
      "--  31_3.png\n",
      "--  31_4.png\n",
      "--  31_5.png\n",
      "--  31_6.png\n",
      "--  31_7.png\n",
      "--  31_8.png\n",
      "--  31_9.png\n",
      "--  329_1p11.jpg\n",
      "--  329_1p12.jpg\n",
      "--  329_1p13.jpg\n",
      "--  329_1p14.jpg\n",
      "--  329_1p15.jpg\n",
      "--  329_1p16.jpg\n",
      "--  329_1p17.jpg\n",
      "--  32_1.png\n",
      "--  32_2.png\n",
      "--  32_3.png\n",
      "--  32_4.png\n",
      "--  32_5.png\n",
      "--  32_6.png\n",
      "--  32_7.png\n",
      "--  32_8.png\n",
      "--  32_9.png\n",
      "--  331_1p11.jpg\n",
      "--  331_1p12.jpg\n",
      "--  331_1p13.jpg\n",
      "--  331_1p14.jpg\n",
      "--  331_1p15.jpg\n",
      "--  331_1p16.jpg\n",
      "--  331_1p17.jpg\n",
      "--  333_1p11.jpg\n",
      "--  333_1p12.jpg\n",
      "--  333_1p13.jpg\n",
      "--  333_1p14.jpg\n",
      "--  333_1p15.jpg\n",
      "--  333_1p16.jpg\n",
      "--  333_1p17.jpg\n",
      "--  334_1p11.jpg\n",
      "--  334_1p12.jpg\n",
      "--  334_1p13.jpg\n",
      "--  334_1p14.jpg\n",
      "--  334_1p15.jpg\n",
      "--  334_1p16.jpg\n",
      "--  334_1p17.jpg\n",
      "--  334_2p11.jpg\n",
      "--  334_2p12.jpg\n",
      "--  334_2p13.jpg\n",
      "--  334_2p14.jpg\n",
      "--  334_2p15.jpg\n",
      "--  334_2p16.jpg\n",
      "--  334_2p17.jpg\n",
      "--  335_1p11.jpg\n",
      "--  335_1p12.jpg\n",
      "--  335_1p13.jpg\n",
      "--  335_1p14.jpg\n",
      "--  335_1p15.jpg\n",
      "--  335_1p16.jpg\n",
      "--  335_1p17.jpg\n",
      "--  335_2p11.jpg\n",
      "--  335_2p12.jpg\n",
      "--  335_2p13.jpg\n",
      "--  335_2p14.jpg\n",
      "--  335_2p15.jpg\n",
      "--  335_2p16.jpg\n",
      "--  335_2p17.jpg\n",
      "--  336_2p11.jpg\n",
      "--  336_2p12.jpg\n",
      "--  336_2p13.jpg\n",
      "--  336_2p14.jpg\n",
      "--  336_2p15.jpg\n",
      "--  336_2p16.jpg\n",
      "--  336_2p17.jpg\n",
      "--  339_1p11.jpg\n",
      "--  339_1p12.jpg\n",
      "--  339_1p13.jpg\n",
      "--  339_1p14.jpg\n",
      "--  339_1p15.jpg\n",
      "--  339_1p16.jpg\n",
      "--  339_1p17.jpg\n",
      "--  33_1.png\n",
      "--  33_2.png\n",
      "--  33_3.png\n",
      "--  33_4.png\n",
      "--  33_5.png\n",
      "--  33_6.png\n",
      "--  33_7.png\n",
      "--  33_8.png\n",
      "--  33_9.png\n",
      "--  346_1p11.jpg\n",
      "--  346_1p12.jpg\n",
      "--  346_1p13.jpg\n",
      "--  346_1p14.jpg\n",
      "--  346_1p15.jpg\n",
      "--  346_1p16.jpg\n",
      "--  346_1p17.jpg\n",
      "--  347_1p11.jpg\n",
      "--  347_1p12.jpg\n",
      "--  347_1p13.jpg\n",
      "--  347_1p14.jpg\n",
      "--  347_1p15.jpg\n",
      "--  347_1p16.jpg\n",
      "--  347_1p17.jpg\n",
      "--  347_2p11.jpg\n",
      "--  347_2p12.jpg\n",
      "--  347_2p13.jpg\n",
      "--  347_2p14.jpg\n",
      "--  347_2p15.jpg\n",
      "--  347_2p16.jpg\n",
      "--  347_2p17.jpg\n",
      "--  34_1.png\n",
      "--  34_2.png\n",
      "--  34_3.png\n",
      "--  34_4.png\n",
      "--  34_5.png\n",
      "--  34_6.png\n",
      "--  34_7.png\n",
      "--  34_8.png\n",
      "--  34_9.png\n",
      "--  356_1p11.jpg\n",
      "--  356_1p12.jpg\n",
      "--  356_1p13.jpg\n",
      "--  356_1p14.jpg\n",
      "--  356_1p15.jpg\n",
      "--  356_1p16.jpg\n",
      "--  356_1p17.jpg\n",
      "--  359_1p11.jpg\n",
      "--  359_1p12.jpg\n",
      "--  359_1p13.jpg\n",
      "--  359_1p14.jpg\n",
      "--  359_1p15.jpg\n",
      "--  359_1p16.jpg\n",
      "--  359_1p17.jpg\n",
      "--  359_1p21.jpg\n",
      "--  359_1p22.jpg\n",
      "--  359_1p23.jpg\n",
      "--  359_1p24.jpg\n",
      "--  359_1p25.jpg\n",
      "--  359_1p26.jpg\n",
      "--  359_1p27.jpg\n",
      "--  359_2p11.jpg\n",
      "--  359_2p12.jpg\n",
      "--  359_2p13.jpg\n",
      "--  359_2p14.jpg\n",
      "--  359_2p15.jpg\n",
      "--  359_2p16.jpg\n",
      "--  359_2p17.jpg\n",
      "--  359_2p21.jpg\n",
      "--  359_2p22.jpg\n",
      "--  359_2p23.jpg\n",
      "--  359_2p24.jpg\n",
      "--  359_2p25.jpg\n",
      "--  359_2p26.jpg\n",
      "--  359_2p27.jpg\n",
      "--  35_1.png\n",
      "--  35_2.png\n",
      "--  35_3.png\n",
      "--  35_4.png\n",
      "--  35_5.png\n",
      "--  35_6.png\n",
      "--  35_7.png\n",
      "--  35_8.png\n",
      "--  35_9.png\n",
      "--  361_1p11.jpg\n",
      "--  361_1p12.jpg\n",
      "--  361_1p13.jpg\n",
      "--  361_1p14.jpg\n",
      "--  361_1p15.jpg\n",
      "--  361_1p16.jpg\n",
      "--  361_1p17.jpg\n",
      "--  362_1p11.jpg\n",
      "--  362_1p12.jpg\n",
      "--  362_1p13.jpg\n",
      "--  362_1p14.jpg\n",
      "--  362_1p15.jpg\n",
      "--  362_1p16.jpg\n",
      "--  362_1p17.jpg\n",
      "--  362_1p21.jpg\n",
      "--  362_1p22.jpg\n",
      "--  362_1p23.jpg\n",
      "--  362_1p24.jpg\n",
      "--  362_1p25.jpg\n",
      "--  362_1p26.jpg\n",
      "--  362_1p27.jpg\n",
      "--  364_1p11.jpg\n",
      "--  364_1p12.jpg\n",
      "--  364_1p13.jpg\n",
      "--  364_1p14.jpg\n",
      "--  364_1p15.jpg\n",
      "--  364_1p16.jpg\n",
      "--  364_1p17.jpg\n",
      "--  365_1p11.jpg\n",
      "--  365_1p12.jpg\n",
      "--  365_1p13.jpg\n",
      "--  365_1p14.jpg\n",
      "--  365_1p15.jpg\n",
      "--  365_1p16.jpg\n",
      "--  365_1p17.jpg\n",
      "--  366_1p11.jpg\n",
      "--  366_1p12.jpg\n",
      "--  366_1p13.jpg\n",
      "--  366_1p14.jpg\n",
      "--  366_1p15.jpg\n",
      "--  366_1p16.jpg\n",
      "--  366_1p17.jpg\n",
      "--  366_2p11.jpg\n",
      "--  366_2p12.jpg\n",
      "--  366_2p13.jpg\n",
      "--  366_2p14.jpg\n",
      "--  366_2p15.jpg\n",
      "--  366_2p16.jpg\n",
      "--  366_2p17.jpg\n",
      "--  366_3p11.jpg\n",
      "--  366_3p12.jpg\n",
      "--  366_3p13.jpg\n",
      "--  366_3p14.jpg\n",
      "--  366_3p15.jpg\n",
      "--  366_3p16.jpg\n",
      "--  366_3p17.jpg\n",
      "--  369_1p11.jpg\n",
      "--  369_1p12.jpg\n",
      "--  369_1p13.jpg\n",
      "--  369_1p14.jpg\n",
      "--  369_1p15.jpg\n",
      "--  369_1p16.jpg\n",
      "--  369_1p17.jpg\n",
      "--  36_1.png\n",
      "--  36_2.png\n",
      "--  36_3.png\n",
      "--  36_4.png\n",
      "--  36_5.png\n",
      "--  36_6.png\n",
      "--  36_7.png\n",
      "--  36_8.png\n",
      "--  36_9.png\n",
      "--  373_1p11.jpg\n",
      "--  373_1p12.jpg\n",
      "--  373_1p13.jpg\n",
      "--  373_1p14.jpg\n",
      "--  373_1p15.jpg\n",
      "--  373_1p16.jpg\n",
      "--  373_1p17.jpg\n",
      "--  373_2p11.jpg\n",
      "--  373_2p12.jpg\n",
      "--  373_2p13.jpg\n",
      "--  373_2p14.jpg\n",
      "--  373_2p15.jpg\n",
      "--  373_2p16.jpg\n",
      "--  373_2p17.jpg\n",
      "--  375_1p11.jpg\n",
      "--  375_1p12.jpg\n",
      "--  375_1p13.jpg\n",
      "--  375_1p14.jpg\n",
      "--  375_1p15.jpg\n",
      "--  375_1p16.jpg\n",
      "--  375_1p17.jpg\n",
      "--  376_1p11.jpg\n",
      "--  376_1p12.jpg\n",
      "--  376_1p13.jpg\n",
      "--  376_1p14.jpg\n",
      "--  376_1p15.jpg\n",
      "--  376_1p16.jpg\n",
      "--  376_1p17.jpg\n",
      "--  377_1p11.jpg\n",
      "--  377_1p12.jpg\n",
      "--  377_1p13.jpg\n",
      "--  377_1p14.jpg\n",
      "--  377_1p15.jpg\n",
      "--  377_1p16.jpg\n",
      "--  377_1p17.jpg\n",
      "--  378_1p11.jpg\n",
      "--  378_1p12.jpg\n",
      "--  378_1p13.jpg\n",
      "--  378_1p14.jpg\n",
      "--  378_1p15.jpg\n",
      "--  378_1p16.jpg\n",
      "--  378_1p17.jpg\n",
      "--  378_1p21.jpg\n",
      "--  378_1p22.jpg\n",
      "--  378_1p23.jpg\n",
      "--  378_1p24.jpg\n",
      "--  378_1p25.jpg\n",
      "--  378_1p26.jpg\n",
      "--  378_1p27.jpg\n",
      "--  37_1.png\n",
      "--  37_2.png\n",
      "--  37_3.png\n",
      "--  37_4.png\n",
      "--  37_5.png\n",
      "--  37_6.png\n",
      "--  37_7.png\n",
      "--  37_8.png\n",
      "--  37_9.png\n",
      "--  380_1p11.jpg\n",
      "--  380_1p12.jpg\n",
      "--  380_1p13.jpg\n",
      "--  380_1p14.jpg\n",
      "--  380_1p15.jpg\n",
      "--  380_1p16.jpg\n",
      "--  380_1p17.jpg\n",
      "--  382_1p11.jpg\n",
      "--  382_1p12.jpg\n",
      "--  382_1p13.jpg\n",
      "--  382_1p14.jpg\n",
      "--  382_1p15.jpg\n",
      "--  382_1p16.jpg\n",
      "--  382_1p17.jpg\n",
      "--  386_1p11.jpg\n",
      "--  386_1p12.jpg\n",
      "--  386_1p13.jpg\n",
      "--  386_1p14.jpg\n",
      "--  386_1p15.jpg\n",
      "--  386_1p16.jpg\n",
      "--  386_1p17.jpg\n",
      "--  391_1p11.jpg\n",
      "--  391_1p12.jpg\n",
      "--  391_1p13.jpg\n",
      "--  391_1p14.jpg\n",
      "--  391_1p15.jpg\n",
      "--  391_1p16.jpg\n",
      "--  391_1p17.jpg\n",
      "--  391_2p11.jpg\n",
      "--  391_2p12.jpg\n",
      "--  391_2p13.jpg\n",
      "--  391_2p14.jpg\n",
      "--  391_2p15.jpg\n",
      "--  391_2p16.jpg\n",
      "--  391_2p17.jpg\n",
      "--  397_1p11.jpg\n",
      "--  397_1p12.jpg\n",
      "--  397_1p13.jpg\n",
      "--  397_1p14.jpg\n",
      "--  397_1p15.jpg\n",
      "--  397_1p16.jpg\n",
      "--  397_1p17.jpg\n",
      "--  3_1.png\n",
      "--  3_2.png\n",
      "--  3_3.png\n",
      "--  3_4.png\n",
      "--  3_5.png\n",
      "--  3_6.png\n",
      "--  3_7.png\n",
      "--  3_8.png\n",
      "--  3_9.png\n",
      "--  41_1.png\n",
      "--  41_2.png\n",
      "--  41_3.png\n",
      "--  41_4.png\n",
      "--  41_5.png\n",
      "--  41_6.png\n",
      "--  41_7.png\n",
      "--  41_8.png\n",
      "--  41_9.png\n",
      "--  42_1.png\n",
      "--  42_2.png\n",
      "--  42_3.png\n",
      "--  42_4.png\n",
      "--  42_5.png\n",
      "--  42_6.png\n",
      "--  42_7.png\n",
      "--  42_8.png\n",
      "--  42_9.png\n",
      "--  43_1.png\n",
      "--  43_2.png\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--  43_3.png\n",
      "--  43_4.png\n",
      "--  43_5.png\n",
      "--  43_6.png\n",
      "--  43_7.png\n",
      "--  43_8.png\n",
      "--  43_9.png\n",
      "--  49_1.png\n",
      "--  49_2.png\n",
      "--  49_3.png\n",
      "--  49_4.png\n",
      "--  49_5.png\n",
      "--  49_6.png\n",
      "--  49_7.png\n",
      "--  49_8.png\n",
      "--  49_9.png\n",
      "--  4_1.png\n",
      "--  4_2.png\n",
      "--  4_3.png\n",
      "--  4_4.png\n",
      "--  4_5.png\n",
      "--  4_6.png\n",
      "--  4_7.png\n",
      "--  4_8.png\n",
      "--  4_9.png\n",
      "--  50_1.png\n",
      "--  50_2.png\n",
      "--  50_3.png\n",
      "--  50_4.png\n",
      "--  50_5.png\n",
      "--  50_6.png\n",
      "--  50_7.png\n",
      "--  50_8.png\n",
      "--  50_9.png\n",
      "--  51_1.png\n",
      "--  51_1p11.jpg\n",
      "--  51_1p12.jpg\n",
      "--  51_1p13.jpg\n",
      "--  51_1p14.jpg\n",
      "--  51_1p15.jpg\n",
      "--  51_1p16.jpg\n",
      "--  51_1p17.jpg\n",
      "--  51_2.png\n",
      "--  51_3.png\n",
      "--  51_4.png\n",
      "--  51_5.png\n",
      "--  51_6.png\n",
      "--  51_7.png\n",
      "--  51_8.png\n",
      "--  51_9.png\n",
      "--  52_1.png\n",
      "--  52_2.png\n",
      "--  52_3.png\n",
      "--  52_4.png\n",
      "--  52_5.png\n",
      "--  52_6.png\n",
      "--  52_7.png\n",
      "--  52_8.png\n",
      "--  52_9.png\n",
      "--  53_1.png\n",
      "--  53_2.png\n",
      "--  53_3.png\n",
      "--  53_4.png\n",
      "--  53_5.png\n",
      "--  53_6.png\n",
      "--  53_7.png\n",
      "--  53_8.png\n",
      "--  53_9.png\n",
      "--  54_1.png\n",
      "--  54_2.png\n",
      "--  54_3.png\n",
      "--  54_4.png\n",
      "--  54_5.png\n",
      "--  54_6.png\n",
      "--  54_7.png\n",
      "--  54_8.png\n",
      "--  54_9.png\n",
      "--  55_1.png\n",
      "--  55_2.png\n",
      "--  55_3.png\n",
      "--  55_4.png\n",
      "--  55_5.png\n",
      "--  55_6.png\n",
      "--  55_7.png\n",
      "--  55_8.png\n",
      "--  55_9.png\n",
      "--  58_1.png\n",
      "--  58_2.png\n",
      "--  58_3.png\n",
      "--  58_4.png\n",
      "--  58_5.png\n",
      "--  58_6.png\n",
      "--  58_7.png\n",
      "--  58_8.png\n",
      "--  58_9.png\n",
      "--  59_1.png\n",
      "--  59_2.png\n",
      "--  59_3.png\n",
      "--  59_4.png\n",
      "--  59_5.png\n",
      "--  59_6.png\n",
      "--  59_7.png\n",
      "--  59_8.png\n",
      "--  59_9.png\n",
      "--  60_1.png\n",
      "--  60_2.png\n",
      "--  60_3.png\n",
      "--  60_4.png\n",
      "--  60_5.png\n",
      "--  60_6.png\n",
      "--  60_7.png\n",
      "--  60_8.png\n",
      "--  60_9.png\n",
      "--  61_1.png\n",
      "--  61_1p11.jpg\n",
      "--  61_1p12.jpg\n",
      "--  61_1p13.jpg\n",
      "--  61_1p14.jpg\n",
      "--  61_1p15.jpg\n",
      "--  61_1p16.jpg\n",
      "--  61_1p17.jpg\n",
      "--  61_1p21.jpg\n",
      "--  61_1p22.jpg\n",
      "--  61_1p23.jpg\n",
      "--  61_1p24.jpg\n",
      "--  61_1p25.jpg\n",
      "--  61_1p26.jpg\n",
      "--  61_1p27.jpg\n",
      "--  61_2.png\n",
      "--  61_3.png\n",
      "--  61_4.png\n",
      "--  61_5.png\n",
      "--  61_6.png\n",
      "--  61_7.png\n",
      "--  61_8.png\n",
      "--  61_9.png\n",
      "--  64_1.png\n",
      "--  64_2.png\n",
      "--  64_3.png\n",
      "--  64_4.png\n",
      "--  64_5.png\n",
      "--  64_6.png\n",
      "--  64_7.png\n",
      "--  64_8.png\n",
      "--  64_9.png\n",
      "--  65_1.png\n",
      "--  65_2.png\n",
      "--  65_3.png\n",
      "--  65_4.png\n",
      "--  65_5.png\n",
      "--  65_6.png\n",
      "--  65_7.png\n",
      "--  65_8.png\n",
      "--  65_9.png\n",
      "--  66_1.png\n",
      "--  66_1p11.jpg\n",
      "--  66_1p12.jpg\n",
      "--  66_1p13.jpg\n",
      "--  66_1p14.jpg\n",
      "--  66_1p15.jpg\n",
      "--  66_1p16.jpg\n",
      "--  66_1p17.jpg\n",
      "--  66_2.png\n",
      "--  66_3.png\n",
      "--  66_4.png\n",
      "--  66_5.png\n",
      "--  66_6.png\n",
      "--  66_7.png\n",
      "--  66_8.png\n",
      "--  66_9.png\n",
      "--  67_1.png\n",
      "--  67_2.png\n",
      "--  67_3.png\n",
      "--  67_4.png\n",
      "--  67_5.png\n",
      "--  67_6.png\n",
      "--  67_7.png\n",
      "--  67_8.png\n",
      "--  67_9.png\n",
      "--  68_1.png\n",
      "--  68_2.png\n",
      "--  68_3.png\n",
      "--  68_4.png\n",
      "--  68_5.png\n",
      "--  68_6.png\n",
      "--  68_7.png\n",
      "--  68_8.png\n",
      "--  68_9.png\n",
      "--  69_1.png\n",
      "--  69_2.png\n",
      "--  69_3.png\n",
      "--  69_4.png\n",
      "--  69_5.png\n",
      "--  69_6.png\n",
      "--  69_7.png\n",
      "--  69_8.png\n",
      "--  69_9.png\n",
      "--  70_1.png\n",
      "--  70_2.png\n",
      "--  70_3.png\n",
      "--  70_4.png\n",
      "--  70_5.png\n",
      "--  70_6.png\n",
      "--  70_7.png\n",
      "--  70_8.png\n",
      "--  70_9.png\n",
      "--  71_1.png\n",
      "--  71_2.png\n",
      "--  71_3.png\n",
      "--  71_4.png\n",
      "--  71_5.png\n",
      "--  71_6.png\n",
      "--  71_7.png\n",
      "--  71_8.png\n",
      "--  71_9.png\n",
      "--  72_1.png\n",
      "--  72_2.png\n",
      "--  72_3.png\n",
      "--  72_4.png\n",
      "--  72_5.png\n",
      "--  72_6.png\n",
      "--  72_7.png\n",
      "--  72_8.png\n",
      "--  72_9.png\n",
      "--  74_1.png\n",
      "--  74_2.png\n",
      "--  74_3.png\n",
      "--  74_4.png\n",
      "--  74_5.png\n",
      "--  74_6.png\n",
      "--  74_7.png\n",
      "--  74_8.png\n",
      "--  74_9.png\n",
      "--  76_1.png\n",
      "--  76_2.png\n",
      "--  76_3.png\n",
      "--  76_4.png\n",
      "--  76_5.png\n",
      "--  76_6.png\n",
      "--  76_7.png\n",
      "--  76_8.png\n",
      "--  76_9.png\n",
      "--  79_1.png\n",
      "--  79_2.png\n",
      "--  79_3.png\n",
      "--  79_4.png\n",
      "--  79_5.png\n",
      "--  79_6.png\n",
      "--  79_7.png\n",
      "--  79_8.png\n",
      "--  79_9.png\n",
      "--  7_1.png\n",
      "--  7_2.png\n",
      "--  7_3.png\n",
      "--  7_4.png\n",
      "--  7_5.png\n",
      "--  7_6.png\n",
      "--  7_7.png\n",
      "--  7_8.png\n",
      "--  7_9.png\n",
      "--  80_1.png\n",
      "--  80_2.png\n",
      "--  80_3.png\n",
      "--  80_4.png\n",
      "--  80_5.png\n",
      "--  80_6.png\n",
      "--  80_7.png\n",
      "--  80_8.png\n",
      "--  80_9.png\n",
      "--  81_1.png\n",
      "--  81_2.png\n",
      "--  81_3.png\n",
      "--  81_4.png\n",
      "--  81_5.png\n",
      "--  81_6.png\n",
      "--  81_7.png\n",
      "--  81_8.png\n",
      "--  81_9.png\n",
      "--  83_1.png\n",
      "--  83_2.png\n",
      "--  83_3.png\n",
      "--  83_4.png\n",
      "--  83_5.png\n",
      "--  83_6.png\n",
      "--  83_7.png\n",
      "--  83_8.png\n",
      "--  83_9.png\n",
      "--  84_1.png\n",
      "--  84_2.png\n",
      "--  84_3.png\n",
      "--  84_4.png\n",
      "--  84_5.png\n",
      "--  84_6.png\n",
      "--  84_7.png\n",
      "--  84_8.png\n",
      "--  84_9.png\n",
      "--  85_1.png\n",
      "--  85_2.png\n",
      "--  85_3.png\n",
      "--  85_4.png\n",
      "--  85_5.png\n",
      "--  85_6.png\n",
      "--  85_7.png\n",
      "--  85_8.png\n",
      "--  85_9.png\n",
      "--  86_1.png\n",
      "--  86_2.png\n",
      "--  86_3.png\n",
      "--  86_4.png\n",
      "--  86_5.png\n",
      "--  86_6.png\n",
      "--  86_7.png\n",
      "--  86_8.png\n",
      "--  86_9.png\n",
      "--  87_1.png\n",
      "--  87_2.png\n",
      "--  87_3.png\n",
      "--  87_4.png\n",
      "--  87_5.png\n",
      "--  87_6.png\n",
      "--  87_7.png\n",
      "--  87_8.png\n",
      "--  87_9.png\n",
      "--  88_1.png\n",
      "--  88_2.png\n",
      "--  88_3.png\n",
      "--  88_4.png\n",
      "--  88_5.png\n",
      "--  88_6.png\n",
      "--  88_7.png\n",
      "--  88_8.png\n",
      "--  88_9.png\n",
      "--  89_1.png\n",
      "--  89_2.png\n",
      "--  89_3.png\n",
      "--  89_4.png\n",
      "--  89_5.png\n",
      "--  89_6.png\n",
      "--  89_7.png\n",
      "--  89_8.png\n",
      "--  89_9.png\n",
      "--  90_1.png\n",
      "--  90_2.png\n",
      "--  90_3.png\n",
      "--  90_4.png\n",
      "--  90_5.png\n",
      "--  90_6.png\n",
      "--  90_7.png\n",
      "--  90_8.png\n",
      "--  90_9.png\n",
      "--  92_1.png\n",
      "--  92_2.png\n",
      "--  92_3.png\n",
      "--  92_4.png\n",
      "--  92_5.png\n",
      "--  92_6.png\n",
      "--  92_7.png\n",
      "--  92_8.png\n",
      "--  92_9.png\n",
      "--  96_1.png\n",
      "--  96_2.png\n",
      "--  96_3.png\n",
      "--  96_4.png\n",
      "--  96_5.png\n",
      "--  96_6.png\n",
      "--  96_7.png\n",
      "--  96_8.png\n",
      "--  96_9.png\n",
      "--  98_1.png\n",
      "--  98_2.png\n",
      "--  98_3.png\n",
      "--  98_4.png\n",
      "--  98_5.png\n",
      "--  98_6.png\n",
      "--  98_7.png\n",
      "--  98_8.png\n",
      "--  98_9.png\n",
      "--  99_1.png\n",
      "--  99_2.png\n",
      "--  99_3.png\n",
      "--  99_4.png\n",
      "--  99_5.png\n",
      "--  99_6.png\n",
      "--  99_7.png\n",
      "--  99_8.png\n",
      "--  99_9.png\n",
      "--  9_1.png\n",
      "--  9_2.png\n",
      "--  9_3.png\n",
      "--  9_4.png\n",
      "--  9_5.png\n",
      "--  9_6.png\n",
      "--  9_7.png\n",
      "--  9_8.png\n",
      "--  9_9.png\n",
      "1 malignant\n",
      "--  100_1p11.jpg\n",
      "--  100_1p12.jpg\n",
      "--  100_1p13.jpg\n",
      "--  100_1p14.jpg\n",
      "--  100_1p15.jpg\n",
      "--  100_1p16.jpg\n",
      "--  100_1p17.jpg\n",
      "--  100_1p21.jpg\n",
      "--  100_1p22.jpg\n",
      "--  100_1p23.jpg\n",
      "--  100_1p24.jpg\n",
      "--  100_1p25.jpg\n",
      "--  100_1p26.jpg\n",
      "--  100_1p27.jpg\n",
      "--  101_1p11.jpg\n",
      "--  101_1p12.jpg\n",
      "--  101_1p13.jpg\n",
      "--  101_1p14.jpg\n",
      "--  101_1p15.jpg\n",
      "--  101_1p16.jpg\n",
      "--  101_1p17.jpg\n",
      "--  101_1p21.jpg\n",
      "--  101_1p22.jpg\n",
      "--  101_1p23.jpg\n",
      "--  101_1p24.jpg\n",
      "--  101_1p25.jpg\n",
      "--  101_1p26.jpg\n",
      "--  101_1p27.jpg\n",
      "--  102_1p11.jpg\n",
      "--  102_1p12.jpg\n",
      "--  102_1p13.jpg\n",
      "--  102_1p14.jpg\n",
      "--  102_1p15.jpg\n",
      "--  102_1p16.jpg\n",
      "--  102_1p17.jpg\n",
      "--  102_1p21.jpg\n",
      "--  102_1p22.jpg\n",
      "--  102_1p23.jpg\n",
      "--  102_1p24.jpg\n",
      "--  102_1p25.jpg\n",
      "--  102_1p26.jpg\n",
      "--  102_1p27.jpg\n",
      "--  103_1p11.jpg\n",
      "--  103_1p12.jpg\n",
      "--  103_1p13.jpg\n",
      "--  103_1p14.jpg\n",
      "--  103_1p15.jpg\n",
      "--  103_1p16.jpg\n",
      "--  103_1p17.jpg\n",
      "--  103_1p21.jpg\n",
      "--  103_1p22.jpg\n",
      "--  103_1p23.jpg\n",
      "--  103_1p24.jpg\n",
      "--  103_1p25.jpg\n",
      "--  103_1p26.jpg\n",
      "--  103_1p27.jpg\n",
      "--  105_1p11.jpg\n",
      "--  105_1p12.jpg\n",
      "--  105_1p13.jpg\n",
      "--  105_1p14.jpg\n",
      "--  105_1p15.jpg\n",
      "--  105_1p16.jpg\n",
      "--  105_1p17.jpg\n",
      "--  105_1p21.jpg\n",
      "--  105_1p22.jpg\n",
      "--  105_1p23.jpg\n",
      "--  105_1p24.jpg\n",
      "--  105_1p25.jpg\n",
      "--  105_1p26.jpg\n",
      "--  105_1p27.jpg\n",
      "--  107_1p11.jpg\n",
      "--  107_1p12.jpg\n",
      "--  107_1p13.jpg\n",
      "--  107_1p14.jpg\n",
      "--  107_1p15.jpg\n",
      "--  107_1p16.jpg\n",
      "--  107_1p17.jpg\n",
      "--  107_1p21.jpg\n",
      "--  107_1p22.jpg\n",
      "--  107_1p23.jpg\n",
      "--  107_1p24.jpg\n",
      "--  107_1p25.jpg\n",
      "--  107_1p26.jpg\n",
      "--  107_1p27.jpg\n",
      "--  108_1p11.jpg\n",
      "--  108_1p12.jpg\n",
      "--  108_1p13.jpg\n",
      "--  108_1p14.jpg\n",
      "--  108_1p15.jpg\n",
      "--  108_1p16.jpg\n",
      "--  108_1p17.jpg\n",
      "--  109_1p11.jpg\n",
      "--  109_1p12.jpg\n",
      "--  109_1p13.jpg\n",
      "--  109_1p14.jpg\n",
      "--  109_1p15.jpg\n",
      "--  109_1p16.jpg\n",
      "--  109_1p17.jpg\n",
      "--  10_1p11.jpg\n",
      "--  10_1p12.jpg\n",
      "--  10_1p13.jpg\n",
      "--  10_1p14.jpg\n",
      "--  10_1p15.jpg\n",
      "--  10_1p16.jpg\n",
      "--  10_1p17.jpg\n",
      "--  110_1p11.jpg\n",
      "--  110_1p12.jpg\n",
      "--  110_1p13.jpg\n",
      "--  110_1p14.jpg\n",
      "--  110_1p15.jpg\n",
      "--  110_1p16.jpg\n",
      "--  110_1p17.jpg\n",
      "--  110_1p21.jpg\n",
      "--  110_1p22.jpg\n",
      "--  110_1p23.jpg\n",
      "--  110_1p24.jpg\n",
      "--  110_1p25.jpg\n",
      "--  110_1p26.jpg\n",
      "--  110_1p27.jpg\n",
      "--  111_1p11.jpg\n",
      "--  111_1p12.jpg\n",
      "--  111_1p13.jpg\n",
      "--  111_1p14.jpg\n",
      "--  111_1p15.jpg\n",
      "--  111_1p16.jpg\n",
      "--  111_1p17.jpg\n",
      "--  112_1p11.jpg\n",
      "--  112_1p12.jpg\n",
      "--  112_1p13.jpg\n",
      "--  112_1p14.jpg\n",
      "--  112_1p15.jpg\n",
      "--  112_1p16.jpg\n",
      "--  112_1p17.jpg\n",
      "--  113_1p11.jpg\n",
      "--  113_1p12.jpg\n",
      "--  113_1p13.jpg\n",
      "--  113_1p14.jpg\n",
      "--  113_1p15.jpg\n",
      "--  113_1p16.jpg\n",
      "--  113_1p17.jpg\n",
      "--  115_1p11.jpg\n",
      "--  115_1p12.jpg\n",
      "--  115_1p13.jpg\n",
      "--  115_1p14.jpg\n",
      "--  115_1p15.jpg\n",
      "--  115_1p16.jpg\n",
      "--  115_1p17.jpg\n",
      "--  115_1p21.jpg\n",
      "--  115_1p22.jpg\n",
      "--  115_1p23.jpg\n",
      "--  115_1p24.jpg\n",
      "--  115_1p25.jpg\n",
      "--  115_1p26.jpg\n",
      "--  115_1p27.jpg\n",
      "--  118_1p11.jpg\n",
      "--  118_1p12.jpg\n",
      "--  118_1p13.jpg\n",
      "--  118_1p14.jpg\n",
      "--  118_1p15.jpg\n",
      "--  118_1p16.jpg\n",
      "--  118_1p17.jpg\n",
      "--  118_1p21.jpg\n",
      "--  118_1p22.jpg\n",
      "--  118_1p23.jpg\n",
      "--  118_1p24.jpg\n",
      "--  118_1p25.jpg\n",
      "--  118_1p26.jpg\n",
      "--  118_1p27.jpg\n",
      "--  119_1p11.jpg\n",
      "--  119_1p12.jpg\n",
      "--  119_1p13.jpg\n",
      "--  119_1p14.jpg\n",
      "--  119_1p15.jpg\n",
      "--  119_1p16.jpg\n",
      "--  119_1p17.jpg\n",
      "--  11_1p11.jpg\n",
      "--  11_1p12.jpg\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--  11_1p13.jpg\n",
      "--  11_1p14.jpg\n",
      "--  11_1p15.jpg\n",
      "--  11_1p16.jpg\n",
      "--  11_1p17.jpg\n",
      "--  121_1p11.jpg\n",
      "--  121_1p12.jpg\n",
      "--  121_1p13.jpg\n",
      "--  121_1p14.jpg\n",
      "--  121_1p15.jpg\n",
      "--  121_1p16.jpg\n",
      "--  121_1p17.jpg\n",
      "--  121_1p21.jpg\n",
      "--  121_1p22.jpg\n",
      "--  121_1p23.jpg\n",
      "--  121_1p24.jpg\n",
      "--  121_1p25.jpg\n",
      "--  121_1p26.jpg\n",
      "--  121_1p27.jpg\n",
      "--  122_1p11.jpg\n",
      "--  122_1p12.jpg\n",
      "--  122_1p13.jpg\n",
      "--  122_1p14.jpg\n",
      "--  122_1p15.jpg\n",
      "--  122_1p16.jpg\n",
      "--  122_1p17.jpg\n",
      "--  122_1p21.jpg\n",
      "--  122_1p22.jpg\n",
      "--  122_1p23.jpg\n",
      "--  122_1p24.jpg\n",
      "--  122_1p25.jpg\n",
      "--  122_1p26.jpg\n",
      "--  122_1p27.jpg\n",
      "--  122_2p11.jpg\n",
      "--  122_2p12.jpg\n",
      "--  122_2p13.jpg\n",
      "--  122_2p14.jpg\n",
      "--  122_2p15.jpg\n",
      "--  122_2p16.jpg\n",
      "--  122_2p17.jpg\n",
      "--  122_2p21.jpg\n",
      "--  122_2p22.jpg\n",
      "--  122_2p23.jpg\n",
      "--  122_2p24.jpg\n",
      "--  122_2p25.jpg\n",
      "--  122_2p26.jpg\n",
      "--  122_2p27.jpg\n",
      "--  123_1.png\n",
      "--  123_1p11.jpg\n",
      "--  123_1p12.jpg\n",
      "--  123_1p13.jpg\n",
      "--  123_1p14.jpg\n",
      "--  123_1p15.jpg\n",
      "--  123_1p16.jpg\n",
      "--  123_1p17.jpg\n",
      "--  123_1p21.jpg\n",
      "--  123_1p22.jpg\n",
      "--  123_1p23.jpg\n",
      "--  123_1p24.jpg\n",
      "--  123_1p25.jpg\n",
      "--  123_1p26.jpg\n",
      "--  123_1p27.jpg\n",
      "--  123_2.png\n",
      "--  123_3.png\n",
      "--  123_4.png\n",
      "--  123_5.png\n",
      "--  123_6.png\n",
      "--  123_7.png\n",
      "--  123_8.png\n",
      "--  123_9.png\n",
      "--  125_1p11.jpg\n",
      "--  125_1p12.jpg\n",
      "--  125_1p13.jpg\n",
      "--  125_1p14.jpg\n",
      "--  125_1p15.jpg\n",
      "--  125_1p16.jpg\n",
      "--  125_1p17.jpg\n",
      "--  125_1p21.jpg\n",
      "--  125_1p22.jpg\n",
      "--  125_1p23.jpg\n",
      "--  125_1p24.jpg\n",
      "--  125_1p25.jpg\n",
      "--  125_1p26.jpg\n",
      "--  125_1p27.jpg\n",
      "--  126_1p11.jpg\n",
      "--  126_1p12.jpg\n",
      "--  126_1p13.jpg\n",
      "--  126_1p14.jpg\n",
      "--  126_1p15.jpg\n",
      "--  126_1p16.jpg\n",
      "--  126_1p17.jpg\n",
      "--  126_1p21.jpg\n",
      "--  126_1p22.jpg\n",
      "--  126_1p23.jpg\n",
      "--  126_1p24.jpg\n",
      "--  126_1p25.jpg\n",
      "--  126_1p26.jpg\n",
      "--  126_1p27.jpg\n",
      "--  128_1p11.jpg\n",
      "--  128_1p12.jpg\n",
      "--  128_1p13.jpg\n",
      "--  128_1p14.jpg\n",
      "--  128_1p15.jpg\n",
      "--  128_1p16.jpg\n",
      "--  128_1p17.jpg\n",
      "--  128_1p21.jpg\n",
      "--  128_1p22.jpg\n",
      "--  128_1p23.jpg\n",
      "--  128_1p24.jpg\n",
      "--  128_1p25.jpg\n",
      "--  128_1p26.jpg\n",
      "--  128_1p27.jpg\n",
      "--  129_1p11.jpg\n",
      "--  129_1p12.jpg\n",
      "--  129_1p13.jpg\n",
      "--  129_1p14.jpg\n",
      "--  129_1p15.jpg\n",
      "--  129_1p16.jpg\n",
      "--  129_1p17.jpg\n",
      "--  129_1p21.jpg\n",
      "--  129_1p22.jpg\n",
      "--  129_1p23.jpg\n",
      "--  129_1p24.jpg\n",
      "--  129_1p25.jpg\n",
      "--  129_1p26.jpg\n",
      "--  129_1p27.jpg\n",
      "--  129_2p11.jpg\n",
      "--  129_2p12.jpg\n",
      "--  129_2p13.jpg\n",
      "--  129_2p14.jpg\n",
      "--  129_2p15.jpg\n",
      "--  129_2p16.jpg\n",
      "--  129_2p17.jpg\n",
      "--  129_2p21.jpg\n",
      "--  129_2p22.jpg\n",
      "--  129_2p23.jpg\n",
      "--  129_2p24.jpg\n",
      "--  129_2p25.jpg\n",
      "--  129_2p26.jpg\n",
      "--  129_2p27.jpg\n",
      "--  12_1.png\n",
      "--  12_1p11.jpg\n",
      "--  12_1p12.jpg\n",
      "--  12_1p13.jpg\n",
      "--  12_1p14.jpg\n",
      "--  12_1p15.jpg\n",
      "--  12_1p16.jpg\n",
      "--  12_1p17.jpg\n",
      "--  12_2.png\n",
      "--  12_3.png\n",
      "--  12_4.png\n",
      "--  12_5.png\n",
      "--  12_6.png\n",
      "--  12_7.png\n",
      "--  12_8.png\n",
      "--  12_9.png\n",
      "--  131_1p11.jpg\n",
      "--  131_1p12.jpg\n",
      "--  131_1p13.jpg\n",
      "--  131_1p14.jpg\n",
      "--  131_1p15.jpg\n",
      "--  131_1p16.jpg\n",
      "--  131_1p17.jpg\n",
      "--  131_1p21.jpg\n",
      "--  131_1p22.jpg\n",
      "--  131_1p23.jpg\n",
      "--  131_1p24.jpg\n",
      "--  131_1p25.jpg\n",
      "--  131_1p26.jpg\n",
      "--  131_1p27.jpg\n",
      "--  132_1.png\n",
      "--  132_2.png\n",
      "--  132_3.png\n",
      "--  132_4.png\n",
      "--  132_5.png\n",
      "--  132_6.png\n",
      "--  132_7.png\n",
      "--  132_8.png\n",
      "--  132_9.png\n",
      "--  135_1p11.jpg\n",
      "--  135_1p12.jpg\n",
      "--  135_1p13.jpg\n",
      "--  135_1p14.jpg\n",
      "--  135_1p15.jpg\n",
      "--  135_1p16.jpg\n",
      "--  135_1p17.jpg\n",
      "--  135_1p21.jpg\n",
      "--  135_1p22.jpg\n",
      "--  135_1p23.jpg\n",
      "--  135_1p24.jpg\n",
      "--  135_1p25.jpg\n",
      "--  135_1p26.jpg\n",
      "--  135_1p27.jpg\n",
      "--  137_1p11.jpg\n",
      "--  137_1p12.jpg\n",
      "--  137_1p13.jpg\n",
      "--  137_1p14.jpg\n",
      "--  137_1p15.jpg\n",
      "--  137_1p16.jpg\n",
      "--  137_1p17.jpg\n",
      "--  13_1.png\n",
      "--  13_1p11.jpg\n",
      "--  13_1p12.jpg\n",
      "--  13_1p13.jpg\n",
      "--  13_1p14.jpg\n",
      "--  13_1p15.jpg\n",
      "--  13_1p16.jpg\n",
      "--  13_1p17.jpg\n",
      "--  13_2.png\n",
      "--  13_3.png\n",
      "--  13_4.png\n",
      "--  13_5.png\n",
      "--  13_6.png\n",
      "--  13_7.png\n",
      "--  13_8.png\n",
      "--  13_9.png\n",
      "--  141_1p11.jpg\n",
      "--  141_1p12.jpg\n",
      "--  141_1p13.jpg\n",
      "--  141_1p14.jpg\n",
      "--  141_1p15.jpg\n",
      "--  141_1p16.jpg\n",
      "--  141_1p17.jpg\n",
      "--  142_1p11.jpg\n",
      "--  142_1p12.jpg\n",
      "--  142_1p13.jpg\n",
      "--  142_1p14.jpg\n",
      "--  142_1p15.jpg\n",
      "--  142_1p16.jpg\n",
      "--  142_1p17.jpg\n",
      "--  142_1p21.jpg\n",
      "--  142_1p22.jpg\n",
      "--  142_1p23.jpg\n",
      "--  142_1p24.jpg\n",
      "--  142_1p25.jpg\n",
      "--  142_1p26.jpg\n",
      "--  142_1p27.jpg\n",
      "--  143_1p11.jpg\n",
      "--  143_1p12.jpg\n",
      "--  143_1p13.jpg\n",
      "--  143_1p14.jpg\n",
      "--  143_1p15.jpg\n",
      "--  143_1p16.jpg\n",
      "--  143_1p17.jpg\n",
      "--  144_1p11.jpg\n",
      "--  144_1p12.jpg\n",
      "--  144_1p13.jpg\n",
      "--  144_1p14.jpg\n",
      "--  144_1p15.jpg\n",
      "--  144_1p16.jpg\n",
      "--  144_1p17.jpg\n",
      "--  144_1p21.jpg\n",
      "--  144_1p22.jpg\n",
      "--  144_1p23.jpg\n",
      "--  144_1p24.jpg\n",
      "--  144_1p25.jpg\n",
      "--  144_1p26.jpg\n",
      "--  144_1p27.jpg\n",
      "--  145_1.png\n",
      "--  145_2.png\n",
      "--  145_3.png\n",
      "--  145_4.png\n",
      "--  145_5.png\n",
      "--  145_6.png\n",
      "--  145_7.png\n",
      "--  145_8.png\n",
      "--  145_9.png\n",
      "--  146_1.png\n",
      "--  146_2.png\n",
      "--  146_3.png\n",
      "--  146_4.png\n",
      "--  146_5.png\n",
      "--  146_6.png\n",
      "--  146_7.png\n",
      "--  146_8.png\n",
      "--  146_9.png\n",
      "--  148_1p11.jpg\n",
      "--  148_1p12.jpg\n",
      "--  148_1p13.jpg\n",
      "--  148_1p14.jpg\n",
      "--  148_1p15.jpg\n",
      "--  148_1p16.jpg\n",
      "--  148_1p17.jpg\n",
      "--  14_1.png\n",
      "--  14_2.png\n",
      "--  14_3.png\n",
      "--  14_4.png\n",
      "--  14_5.png\n",
      "--  14_6.png\n",
      "--  14_7.png\n",
      "--  14_8.png\n",
      "--  14_9.png\n",
      "--  151_1.png\n",
      "--  151_1p11.jpg\n",
      "--  151_1p12.jpg\n",
      "--  151_1p13.jpg\n",
      "--  151_1p14.jpg\n",
      "--  151_1p15.jpg\n",
      "--  151_1p16.jpg\n",
      "--  151_1p17.jpg\n",
      "--  151_2.png\n",
      "--  151_3.png\n",
      "--  151_4.png\n",
      "--  151_5.png\n",
      "--  151_6.png\n",
      "--  151_7.png\n",
      "--  151_8.png\n",
      "--  151_9.png\n",
      "--  152_1.png\n",
      "--  152_2.png\n",
      "--  152_3.png\n",
      "--  152_4.png\n",
      "--  152_5.png\n",
      "--  152_6.png\n",
      "--  152_7.png\n",
      "--  152_8.png\n",
      "--  152_9.png\n",
      "--  153_1.png\n",
      "--  153_1p11.jpg\n",
      "--  153_1p12.jpg\n",
      "--  153_1p13.jpg\n",
      "--  153_1p14.jpg\n",
      "--  153_1p15.jpg\n",
      "--  153_1p16.jpg\n",
      "--  153_1p17.jpg\n",
      "--  153_2.png\n",
      "--  153_3.png\n",
      "--  153_4.png\n",
      "--  153_5.png\n",
      "--  153_6.png\n",
      "--  153_7.png\n",
      "--  153_8.png\n",
      "--  153_9.png\n",
      "--  154_1.png\n",
      "--  154_1p11.jpg\n",
      "--  154_1p12.jpg\n",
      "--  154_1p13.jpg\n",
      "--  154_1p14.jpg\n",
      "--  154_1p15.jpg\n",
      "--  154_1p16.jpg\n",
      "--  154_1p17.jpg\n",
      "--  154_1p21.jpg\n",
      "--  154_1p22.jpg\n",
      "--  154_1p23.jpg\n",
      "--  154_1p24.jpg\n",
      "--  154_1p25.jpg\n",
      "--  154_1p26.jpg\n",
      "--  154_1p27.jpg\n",
      "--  154_2.png\n",
      "--  154_3.png\n",
      "--  154_4.png\n",
      "--  154_5.png\n",
      "--  154_6.png\n",
      "--  154_7.png\n",
      "--  154_8.png\n",
      "--  154_9.png\n",
      "--  155_1p11.jpg\n",
      "--  155_1p12.jpg\n",
      "--  155_1p13.jpg\n",
      "--  155_1p14.jpg\n",
      "--  155_1p15.jpg\n",
      "--  155_1p16.jpg\n",
      "--  155_1p17.jpg\n",
      "--  155_1p21.jpg\n",
      "--  155_1p22.jpg\n",
      "--  155_1p23.jpg\n",
      "--  155_1p24.jpg\n",
      "--  155_1p25.jpg\n",
      "--  155_1p26.jpg\n",
      "--  155_1p27.jpg\n",
      "--  156_1p11.jpg\n",
      "--  156_1p12.jpg\n",
      "--  156_1p13.jpg\n",
      "--  156_1p14.jpg\n",
      "--  156_1p15.jpg\n",
      "--  156_1p16.jpg\n",
      "--  156_1p17.jpg\n",
      "--  156_1p21.jpg\n",
      "--  156_1p22.jpg\n",
      "--  156_1p23.jpg\n",
      "--  156_1p24.jpg\n",
      "--  156_1p25.jpg\n",
      "--  156_1p26.jpg\n",
      "--  156_1p27.jpg\n",
      "--  156_2p11.jpg\n",
      "--  156_2p12.jpg\n",
      "--  156_2p13.jpg\n",
      "--  156_2p14.jpg\n",
      "--  156_2p15.jpg\n",
      "--  156_2p16.jpg\n",
      "--  156_2p17.jpg\n",
      "--  15_1p11.jpg\n",
      "--  15_1p12.jpg\n",
      "--  15_1p13.jpg\n",
      "--  15_1p14.jpg\n",
      "--  15_1p15.jpg\n",
      "--  15_1p16.jpg\n",
      "--  15_1p17.jpg\n",
      "--  162_1p11.jpg\n",
      "--  162_1p12.jpg\n",
      "--  162_1p13.jpg\n",
      "--  162_1p14.jpg\n",
      "--  162_1p15.jpg\n",
      "--  162_1p16.jpg\n",
      "--  162_1p17.jpg\n",
      "--  162_1p21.jpg\n",
      "--  162_1p22.jpg\n",
      "--  162_1p23.jpg\n",
      "--  162_1p24.jpg\n",
      "--  162_1p25.jpg\n",
      "--  162_1p26.jpg\n",
      "--  162_1p27.jpg\n",
      "--  164_1p11.jpg\n",
      "--  164_1p12.jpg\n",
      "--  164_1p13.jpg\n",
      "--  164_1p14.jpg\n",
      "--  164_1p15.jpg\n",
      "--  164_1p16.jpg\n",
      "--  164_1p17.jpg\n",
      "--  164_1p21.jpg\n",
      "--  164_1p22.jpg\n",
      "--  164_1p23.jpg\n",
      "--  164_1p24.jpg\n",
      "--  164_1p25.jpg\n",
      "--  164_1p26.jpg\n",
      "--  164_1p27.jpg\n",
      "--  166_1p11.jpg\n",
      "--  166_1p12.jpg\n",
      "--  166_1p13.jpg\n",
      "--  166_1p14.jpg\n",
      "--  166_1p15.jpg\n",
      "--  166_1p16.jpg\n",
      "--  166_1p17.jpg\n",
      "--  167_1p11.jpg\n",
      "--  167_1p12.jpg\n",
      "--  167_1p13.jpg\n",
      "--  167_1p14.jpg\n",
      "--  167_1p15.jpg\n",
      "--  167_1p16.jpg\n",
      "--  167_1p17.jpg\n",
      "--  167_1p21.jpg\n",
      "--  167_1p22.jpg\n",
      "--  167_1p23.jpg\n",
      "--  167_1p24.jpg\n",
      "--  167_1p25.jpg\n",
      "--  167_1p26.jpg\n",
      "--  167_1p27.jpg\n",
      "--  168_1p11.jpg\n",
      "--  168_1p12.jpg\n",
      "--  168_1p13.jpg\n",
      "--  168_1p14.jpg\n",
      "--  168_1p15.jpg\n",
      "--  168_1p16.jpg\n",
      "--  168_1p17.jpg\n",
      "--  168_1p21.jpg\n",
      "--  168_1p22.jpg\n",
      "--  168_1p23.jpg\n",
      "--  168_1p24.jpg\n",
      "--  168_1p25.jpg\n",
      "--  168_1p26.jpg\n",
      "--  168_1p27.jpg\n",
      "--  170_1p11.jpg\n",
      "--  170_1p12.jpg\n",
      "--  170_1p13.jpg\n",
      "--  170_1p14.jpg\n",
      "--  170_1p15.jpg\n",
      "--  170_1p16.jpg\n",
      "--  170_1p17.jpg\n",
      "--  170_1p21.jpg\n",
      "--  170_1p22.jpg\n",
      "--  170_1p23.jpg\n",
      "--  170_1p24.jpg\n",
      "--  170_1p25.jpg\n",
      "--  170_1p26.jpg\n",
      "--  170_1p27.jpg\n",
      "--  171_1p11.jpg\n",
      "--  171_1p12.jpg\n",
      "--  171_1p13.jpg\n",
      "--  171_1p14.jpg\n",
      "--  171_1p15.jpg\n",
      "--  171_1p16.jpg\n",
      "--  171_1p17.jpg\n",
      "--  171_1p21.jpg\n",
      "--  171_1p22.jpg\n",
      "--  171_1p23.jpg\n",
      "--  171_1p24.jpg\n",
      "--  171_1p25.jpg\n",
      "--  171_1p26.jpg\n",
      "--  171_1p27.jpg\n",
      "--  172_1p11.jpg\n",
      "--  172_1p12.jpg\n",
      "--  172_1p13.jpg\n",
      "--  172_1p14.jpg\n",
      "--  172_1p15.jpg\n",
      "--  172_1p16.jpg\n",
      "--  172_1p17.jpg\n",
      "--  174_1p11.jpg\n",
      "--  174_1p12.jpg\n",
      "--  174_1p13.jpg\n",
      "--  174_1p14.jpg\n",
      "--  174_1p15.jpg\n",
      "--  174_1p16.jpg\n",
      "--  174_1p17.jpg\n",
      "--  176_1p11.jpg\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--  176_1p12.jpg\n",
      "--  176_1p13.jpg\n",
      "--  176_1p14.jpg\n",
      "--  176_1p15.jpg\n",
      "--  176_1p16.jpg\n",
      "--  176_1p17.jpg\n",
      "--  177_1p11.jpg\n",
      "--  177_1p12.jpg\n",
      "--  177_1p13.jpg\n",
      "--  177_1p14.jpg\n",
      "--  177_1p15.jpg\n",
      "--  177_1p16.jpg\n",
      "--  177_1p17.jpg\n",
      "--  177_2p11.jpg\n",
      "--  177_2p12.jpg\n",
      "--  177_2p13.jpg\n",
      "--  177_2p14.jpg\n",
      "--  177_2p15.jpg\n",
      "--  177_2p16.jpg\n",
      "--  177_2p17.jpg\n",
      "--  178_1p11.jpg\n",
      "--  178_1p12.jpg\n",
      "--  178_1p13.jpg\n",
      "--  178_1p14.jpg\n",
      "--  178_1p15.jpg\n",
      "--  178_1p16.jpg\n",
      "--  178_1p17.jpg\n",
      "--  179_1p11.jpg\n",
      "--  179_1p12.jpg\n",
      "--  179_1p13.jpg\n",
      "--  179_1p14.jpg\n",
      "--  179_1p15.jpg\n",
      "--  179_1p16.jpg\n",
      "--  179_1p17.jpg\n",
      "--  179_1p21.jpg\n",
      "--  179_1p22.jpg\n",
      "--  179_1p23.jpg\n",
      "--  179_1p24.jpg\n",
      "--  179_1p25.jpg\n",
      "--  179_1p26.jpg\n",
      "--  179_1p27.jpg\n",
      "--  180_1p11.jpg\n",
      "--  180_1p12.jpg\n",
      "--  180_1p13.jpg\n",
      "--  180_1p14.jpg\n",
      "--  180_1p15.jpg\n",
      "--  180_1p16.jpg\n",
      "--  180_1p17.jpg\n",
      "--  180_1p21.jpg\n",
      "--  180_1p22.jpg\n",
      "--  180_1p23.jpg\n",
      "--  180_1p24.jpg\n",
      "--  180_1p25.jpg\n",
      "--  180_1p26.jpg\n",
      "--  180_1p27.jpg\n",
      "--  184_1p11.jpg\n",
      "--  184_1p12.jpg\n",
      "--  184_1p13.jpg\n",
      "--  184_1p14.jpg\n",
      "--  184_1p15.jpg\n",
      "--  184_1p16.jpg\n",
      "--  184_1p17.jpg\n",
      "--  184_1p21.jpg\n",
      "--  184_1p22.jpg\n",
      "--  184_1p23.jpg\n",
      "--  184_1p24.jpg\n",
      "--  184_1p25.jpg\n",
      "--  184_1p26.jpg\n",
      "--  184_1p27.jpg\n",
      "--  185_1p11.jpg\n",
      "--  185_1p12.jpg\n",
      "--  185_1p13.jpg\n",
      "--  185_1p14.jpg\n",
      "--  185_1p15.jpg\n",
      "--  185_1p16.jpg\n",
      "--  185_1p17.jpg\n",
      "--  185_1p21.jpg\n",
      "--  185_1p22.jpg\n",
      "--  185_1p23.jpg\n",
      "--  185_1p24.jpg\n",
      "--  185_1p25.jpg\n",
      "--  185_1p26.jpg\n",
      "--  185_1p27.jpg\n",
      "--  186_1p11.jpg\n",
      "--  186_1p12.jpg\n",
      "--  186_1p13.jpg\n",
      "--  186_1p14.jpg\n",
      "--  186_1p15.jpg\n",
      "--  186_1p16.jpg\n",
      "--  186_1p17.jpg\n",
      "--  186_1p21.jpg\n",
      "--  186_1p22.jpg\n",
      "--  186_1p23.jpg\n",
      "--  186_1p24.jpg\n",
      "--  186_1p25.jpg\n",
      "--  186_1p26.jpg\n",
      "--  186_1p27.jpg\n",
      "--  187_1p11.jpg\n",
      "--  187_1p12.jpg\n",
      "--  187_1p13.jpg\n",
      "--  187_1p14.jpg\n",
      "--  187_1p15.jpg\n",
      "--  187_1p16.jpg\n",
      "--  187_1p17.jpg\n",
      "--  187_1p21.jpg\n",
      "--  187_1p22.jpg\n",
      "--  187_1p23.jpg\n",
      "--  187_1p24.jpg\n",
      "--  187_1p25.jpg\n",
      "--  187_1p26.jpg\n",
      "--  187_1p27.jpg\n",
      "--  18_1p11.jpg\n",
      "--  18_1p12.jpg\n",
      "--  18_1p13.jpg\n",
      "--  18_1p14.jpg\n",
      "--  18_1p15.jpg\n",
      "--  18_1p16.jpg\n",
      "--  18_1p17.jpg\n",
      "--  192_1p11.jpg\n",
      "--  192_1p12.jpg\n",
      "--  192_1p13.jpg\n",
      "--  192_1p14.jpg\n",
      "--  192_1p15.jpg\n",
      "--  192_1p16.jpg\n",
      "--  192_1p17.jpg\n",
      "--  192_1p21.jpg\n",
      "--  192_1p22.jpg\n",
      "--  192_1p23.jpg\n",
      "--  192_1p24.jpg\n",
      "--  192_1p25.jpg\n",
      "--  192_1p26.jpg\n",
      "--  192_1p27.jpg\n",
      "--  194_1p11.jpg\n",
      "--  194_1p12.jpg\n",
      "--  194_1p13.jpg\n",
      "--  194_1p14.jpg\n",
      "--  194_1p15.jpg\n",
      "--  194_1p16.jpg\n",
      "--  194_1p17.jpg\n",
      "--  195_1p11.jpg\n",
      "--  195_1p12.jpg\n",
      "--  195_1p13.jpg\n",
      "--  195_1p14.jpg\n",
      "--  195_1p15.jpg\n",
      "--  195_1p16.jpg\n",
      "--  195_1p17.jpg\n",
      "--  195_1p21.jpg\n",
      "--  195_1p22.jpg\n",
      "--  195_1p23.jpg\n",
      "--  195_1p24.jpg\n",
      "--  195_1p25.jpg\n",
      "--  195_1p26.jpg\n",
      "--  195_1p27.jpg\n",
      "--  197_1p11.jpg\n",
      "--  197_1p12.jpg\n",
      "--  197_1p13.jpg\n",
      "--  197_1p14.jpg\n",
      "--  197_1p15.jpg\n",
      "--  197_1p16.jpg\n",
      "--  197_1p17.jpg\n",
      "--  197_1p21.jpg\n",
      "--  197_1p22.jpg\n",
      "--  197_1p23.jpg\n",
      "--  197_1p24.jpg\n",
      "--  197_1p25.jpg\n",
      "--  197_1p26.jpg\n",
      "--  197_1p27.jpg\n",
      "--  198_1p11.jpg\n",
      "--  198_1p12.jpg\n",
      "--  198_1p13.jpg\n",
      "--  198_1p14.jpg\n",
      "--  198_1p15.jpg\n",
      "--  198_1p16.jpg\n",
      "--  198_1p17.jpg\n",
      "--  198_1p21.jpg\n",
      "--  198_1p22.jpg\n",
      "--  198_1p23.jpg\n",
      "--  198_1p24.jpg\n",
      "--  198_1p25.jpg\n",
      "--  198_1p26.jpg\n",
      "--  198_1p27.jpg\n",
      "--  198_2p11.jpg\n",
      "--  198_2p12.jpg\n",
      "--  198_2p13.jpg\n",
      "--  198_2p14.jpg\n",
      "--  198_2p15.jpg\n",
      "--  198_2p16.jpg\n",
      "--  198_2p17.jpg\n",
      "--  19_1.png\n",
      "--  19_1p11.jpg\n",
      "--  19_1p12.jpg\n",
      "--  19_1p13.jpg\n",
      "--  19_1p14.jpg\n",
      "--  19_1p15.jpg\n",
      "--  19_1p16.jpg\n",
      "--  19_1p17.jpg\n",
      "--  19_2.png\n",
      "--  19_3.png\n",
      "--  19_4.png\n",
      "--  19_5.png\n",
      "--  19_6.png\n",
      "--  19_7.png\n",
      "--  19_8.png\n",
      "--  19_9.png\n",
      "--  201_1p11.jpg\n",
      "--  201_1p12.jpg\n",
      "--  201_1p13.jpg\n",
      "--  201_1p14.jpg\n",
      "--  201_1p15.jpg\n",
      "--  201_1p16.jpg\n",
      "--  201_1p17.jpg\n",
      "--  201_1p21.jpg\n",
      "--  201_1p22.jpg\n",
      "--  201_1p23.jpg\n",
      "--  201_1p24.jpg\n",
      "--  201_1p25.jpg\n",
      "--  201_1p26.jpg\n",
      "--  201_1p27.jpg\n",
      "--  202_1p11.jpg\n",
      "--  202_1p12.jpg\n",
      "--  202_1p13.jpg\n",
      "--  202_1p14.jpg\n",
      "--  202_1p15.jpg\n",
      "--  202_1p16.jpg\n",
      "--  202_1p17.jpg\n",
      "--  202_1p21.jpg\n",
      "--  202_1p22.jpg\n",
      "--  202_1p23.jpg\n",
      "--  202_1p24.jpg\n",
      "--  202_1p25.jpg\n",
      "--  202_1p26.jpg\n",
      "--  202_1p27.jpg\n",
      "--  203_1p11.jpg\n",
      "--  203_1p12.jpg\n",
      "--  203_1p13.jpg\n",
      "--  203_1p14.jpg\n",
      "--  203_1p15.jpg\n",
      "--  203_1p16.jpg\n",
      "--  203_1p17.jpg\n",
      "--  205_1p11.jpg\n",
      "--  205_1p12.jpg\n",
      "--  205_1p13.jpg\n",
      "--  205_1p14.jpg\n",
      "--  205_1p15.jpg\n",
      "--  205_1p16.jpg\n",
      "--  205_1p17.jpg\n",
      "--  205_1p21.jpg\n",
      "--  205_1p22.jpg\n",
      "--  205_1p23.jpg\n",
      "--  205_1p24.jpg\n",
      "--  205_1p25.jpg\n",
      "--  205_1p26.jpg\n",
      "--  205_1p27.jpg\n",
      "--  206_1p11.jpg\n",
      "--  206_1p12.jpg\n",
      "--  206_1p13.jpg\n",
      "--  206_1p14.jpg\n",
      "--  206_1p15.jpg\n",
      "--  206_1p16.jpg\n",
      "--  206_1p17.jpg\n",
      "--  206_1p21.jpg\n",
      "--  206_1p22.jpg\n",
      "--  206_1p23.jpg\n",
      "--  206_1p24.jpg\n",
      "--  206_1p25.jpg\n",
      "--  206_1p26.jpg\n",
      "--  206_1p27.jpg\n",
      "--  207_1p11.jpg\n",
      "--  207_1p12.jpg\n",
      "--  207_1p13.jpg\n",
      "--  207_1p14.jpg\n",
      "--  207_1p15.jpg\n",
      "--  207_1p16.jpg\n",
      "--  207_1p17.jpg\n",
      "--  209_1p11.jpg\n",
      "--  209_1p12.jpg\n",
      "--  209_1p13.jpg\n",
      "--  209_1p14.jpg\n",
      "--  209_1p15.jpg\n",
      "--  209_1p16.jpg\n",
      "--  209_1p17.jpg\n",
      "--  209_1p21.jpg\n",
      "--  209_1p22.jpg\n",
      "--  209_1p23.jpg\n",
      "--  209_1p24.jpg\n",
      "--  209_1p25.jpg\n",
      "--  209_1p26.jpg\n",
      "--  209_1p27.jpg\n",
      "--  209_2p11.jpg\n",
      "--  209_2p12.jpg\n",
      "--  209_2p13.jpg\n",
      "--  209_2p14.jpg\n",
      "--  209_2p15.jpg\n",
      "--  209_2p16.jpg\n",
      "--  209_2p17.jpg\n",
      "--  20_1.png\n",
      "--  20_1p11.jpg\n",
      "--  20_1p12.jpg\n",
      "--  20_1p13.jpg\n",
      "--  20_1p14.jpg\n",
      "--  20_1p15.jpg\n",
      "--  20_1p16.jpg\n",
      "--  20_1p17.jpg\n",
      "--  20_2.png\n",
      "--  20_3.png\n",
      "--  20_4.png\n",
      "--  20_5.png\n",
      "--  20_6.png\n",
      "--  20_7.png\n",
      "--  20_8.png\n",
      "--  20_9.png\n",
      "--  211_1p11.jpg\n",
      "--  211_1p12.jpg\n",
      "--  211_1p13.jpg\n",
      "--  211_1p14.jpg\n",
      "--  211_1p15.jpg\n",
      "--  211_1p16.jpg\n",
      "--  211_1p17.jpg\n",
      "--  211_1p21.jpg\n",
      "--  211_1p22.jpg\n",
      "--  211_1p23.jpg\n",
      "--  211_1p24.jpg\n",
      "--  211_1p25.jpg\n",
      "--  211_1p26.jpg\n",
      "--  211_1p27.jpg\n",
      "--  212_1p11.jpg\n",
      "--  212_1p12.jpg\n",
      "--  212_1p13.jpg\n",
      "--  212_1p14.jpg\n",
      "--  212_1p15.jpg\n",
      "--  212_1p16.jpg\n",
      "--  212_1p17.jpg\n",
      "--  212_1p21.jpg\n",
      "--  212_1p22.jpg\n",
      "--  212_1p23.jpg\n",
      "--  212_1p24.jpg\n",
      "--  212_1p25.jpg\n",
      "--  212_1p26.jpg\n",
      "--  212_1p27.jpg\n",
      "--  214_1p11.jpg\n",
      "--  214_1p12.jpg\n",
      "--  214_1p13.jpg\n",
      "--  214_1p14.jpg\n",
      "--  214_1p15.jpg\n",
      "--  214_1p16.jpg\n",
      "--  214_1p17.jpg\n",
      "--  214_1p21.jpg\n",
      "--  214_1p22.jpg\n",
      "--  214_1p23.jpg\n",
      "--  214_1p24.jpg\n",
      "--  214_1p25.jpg\n",
      "--  214_1p26.jpg\n",
      "--  214_1p27.jpg\n",
      "--  215_1p11.jpg\n",
      "--  215_1p12.jpg\n",
      "--  215_1p13.jpg\n",
      "--  215_1p14.jpg\n",
      "--  215_1p15.jpg\n",
      "--  215_1p16.jpg\n",
      "--  215_1p17.jpg\n",
      "--  215_1p21.jpg\n",
      "--  215_1p22.jpg\n",
      "--  215_1p23.jpg\n",
      "--  215_1p24.jpg\n",
      "--  215_1p25.jpg\n",
      "--  215_1p26.jpg\n",
      "--  215_1p27.jpg\n",
      "--  216_1p11.jpg\n",
      "--  216_1p12.jpg\n",
      "--  216_1p13.jpg\n",
      "--  216_1p14.jpg\n",
      "--  216_1p15.jpg\n",
      "--  216_1p16.jpg\n",
      "--  216_1p17.jpg\n",
      "--  217_1p11.jpg\n",
      "--  217_1p12.jpg\n",
      "--  217_1p13.jpg\n",
      "--  217_1p14.jpg\n",
      "--  217_1p15.jpg\n",
      "--  217_1p16.jpg\n",
      "--  217_1p17.jpg\n",
      "--  217_1p21.jpg\n",
      "--  217_1p22.jpg\n",
      "--  217_1p23.jpg\n",
      "--  217_1p24.jpg\n",
      "--  217_1p25.jpg\n",
      "--  217_1p26.jpg\n",
      "--  217_1p27.jpg\n",
      "--  219_1p11.jpg\n",
      "--  219_1p12.jpg\n",
      "--  219_1p13.jpg\n",
      "--  219_1p14.jpg\n",
      "--  219_1p15.jpg\n",
      "--  219_1p16.jpg\n",
      "--  219_1p17.jpg\n",
      "--  21_1p11.jpg\n",
      "--  21_1p12.jpg\n",
      "--  21_1p13.jpg\n",
      "--  21_1p14.jpg\n",
      "--  21_1p15.jpg\n",
      "--  21_1p16.jpg\n",
      "--  21_1p17.jpg\n",
      "--  220_1p11.jpg\n",
      "--  220_1p12.jpg\n",
      "--  220_1p13.jpg\n",
      "--  220_1p14.jpg\n",
      "--  220_1p15.jpg\n",
      "--  220_1p16.jpg\n",
      "--  220_1p17.jpg\n",
      "--  221_1p11.jpg\n",
      "--  221_1p12.jpg\n",
      "--  221_1p13.jpg\n",
      "--  221_1p14.jpg\n",
      "--  221_1p15.jpg\n",
      "--  221_1p16.jpg\n",
      "--  221_1p17.jpg\n",
      "--  221_1p21.jpg\n",
      "--  221_1p22.jpg\n",
      "--  221_1p23.jpg\n",
      "--  221_1p24.jpg\n",
      "--  221_1p25.jpg\n",
      "--  221_1p26.jpg\n",
      "--  221_1p27.jpg\n",
      "--  224_1p11.jpg\n",
      "--  224_1p12.jpg\n",
      "--  224_1p13.jpg\n",
      "--  224_1p14.jpg\n",
      "--  224_1p15.jpg\n",
      "--  224_1p16.jpg\n",
      "--  224_1p17.jpg\n",
      "--  224_1p21.jpg\n",
      "--  224_1p22.jpg\n",
      "--  224_1p23.jpg\n",
      "--  224_1p24.jpg\n",
      "--  224_1p25.jpg\n",
      "--  224_1p26.jpg\n",
      "--  224_1p27.jpg\n",
      "--  225_1p11.jpg\n",
      "--  225_1p12.jpg\n",
      "--  225_1p13.jpg\n",
      "--  225_1p14.jpg\n",
      "--  225_1p15.jpg\n",
      "--  225_1p16.jpg\n",
      "--  225_1p17.jpg\n",
      "--  225_1p21.jpg\n",
      "--  225_1p22.jpg\n",
      "--  225_1p23.jpg\n",
      "--  225_1p24.jpg\n",
      "--  225_1p25.jpg\n",
      "--  225_1p26.jpg\n",
      "--  225_1p27.jpg\n",
      "--  225_2p11.jpg\n",
      "--  225_2p12.jpg\n",
      "--  225_2p13.jpg\n",
      "--  225_2p14.jpg\n",
      "--  225_2p15.jpg\n",
      "--  225_2p16.jpg\n",
      "--  225_2p17.jpg\n",
      "--  225_2p21.jpg\n",
      "--  225_2p22.jpg\n",
      "--  225_2p23.jpg\n",
      "--  225_2p24.jpg\n",
      "--  225_2p25.jpg\n",
      "--  225_2p26.jpg\n",
      "--  225_2p27.jpg\n",
      "--  227_1p11.jpg\n",
      "--  227_1p12.jpg\n",
      "--  227_1p13.jpg\n",
      "--  227_1p14.jpg\n",
      "--  227_1p15.jpg\n",
      "--  227_1p16.jpg\n",
      "--  227_1p17.jpg\n",
      "--  227_1p21.jpg\n",
      "--  227_1p22.jpg\n",
      "--  227_1p23.jpg\n",
      "--  227_1p24.jpg\n",
      "--  227_1p25.jpg\n",
      "--  227_1p26.jpg\n",
      "--  227_1p27.jpg\n",
      "--  227_2p11.jpg\n",
      "--  227_2p12.jpg\n",
      "--  227_2p13.jpg\n",
      "--  227_2p14.jpg\n",
      "--  227_2p15.jpg\n",
      "--  227_2p16.jpg\n",
      "--  227_2p17.jpg\n",
      "--  228_1p11.jpg\n",
      "--  228_1p12.jpg\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--  228_1p13.jpg\n",
      "--  228_1p14.jpg\n",
      "--  228_1p15.jpg\n",
      "--  228_1p16.jpg\n",
      "--  228_1p17.jpg\n",
      "--  231_1p11.jpg\n",
      "--  231_1p12.jpg\n",
      "--  231_1p13.jpg\n",
      "--  231_1p14.jpg\n",
      "--  231_1p15.jpg\n",
      "--  231_1p16.jpg\n",
      "--  231_1p17.jpg\n",
      "--  231_1p21.jpg\n",
      "--  231_1p22.jpg\n",
      "--  231_1p23.jpg\n",
      "--  231_1p24.jpg\n",
      "--  231_1p25.jpg\n",
      "--  231_1p26.jpg\n",
      "--  231_1p27.jpg\n",
      "--  233_1p11.jpg\n",
      "--  233_1p12.jpg\n",
      "--  233_1p13.jpg\n",
      "--  233_1p14.jpg\n",
      "--  233_1p15.jpg\n",
      "--  233_1p16.jpg\n",
      "--  233_1p17.jpg\n",
      "--  233_2p11.jpg\n",
      "--  233_2p12.jpg\n",
      "--  233_2p13.jpg\n",
      "--  233_2p14.jpg\n",
      "--  233_2p15.jpg\n",
      "--  233_2p16.jpg\n",
      "--  233_2p17.jpg\n",
      "--  236_1p11.jpg\n",
      "--  236_1p12.jpg\n",
      "--  236_1p13.jpg\n",
      "--  236_1p14.jpg\n",
      "--  236_1p15.jpg\n",
      "--  236_1p16.jpg\n",
      "--  236_1p17.jpg\n",
      "--  236_1p21.jpg\n",
      "--  236_1p22.jpg\n",
      "--  236_1p23.jpg\n",
      "--  236_1p24.jpg\n",
      "--  236_1p25.jpg\n",
      "--  236_1p26.jpg\n",
      "--  236_1p27.jpg\n",
      "--  23_1p11.jpg\n",
      "--  23_1p12.jpg\n",
      "--  23_1p13.jpg\n",
      "--  23_1p14.jpg\n",
      "--  23_1p15.jpg\n",
      "--  23_1p16.jpg\n",
      "--  23_1p17.jpg\n",
      "--  242_1p11.jpg\n",
      "--  242_1p12.jpg\n",
      "--  242_1p13.jpg\n",
      "--  242_1p14.jpg\n",
      "--  242_1p15.jpg\n",
      "--  242_1p16.jpg\n",
      "--  242_1p17.jpg\n",
      "--  243_1p11.jpg\n",
      "--  243_1p12.jpg\n",
      "--  243_1p13.jpg\n",
      "--  243_1p14.jpg\n",
      "--  243_1p15.jpg\n",
      "--  243_1p16.jpg\n",
      "--  243_1p17.jpg\n",
      "--  243_1p21.jpg\n",
      "--  243_1p22.jpg\n",
      "--  243_1p23.jpg\n",
      "--  243_1p24.jpg\n",
      "--  243_1p25.jpg\n",
      "--  243_1p26.jpg\n",
      "--  243_1p27.jpg\n",
      "--  244_1p11.jpg\n",
      "--  244_1p12.jpg\n",
      "--  244_1p13.jpg\n",
      "--  244_1p14.jpg\n",
      "--  244_1p15.jpg\n",
      "--  244_1p16.jpg\n",
      "--  244_1p17.jpg\n",
      "--  244_1p21.jpg\n",
      "--  244_1p22.jpg\n",
      "--  244_1p23.jpg\n",
      "--  244_1p24.jpg\n",
      "--  244_1p25.jpg\n",
      "--  244_1p26.jpg\n",
      "--  244_1p27.jpg\n",
      "--  246_1p11.jpg\n",
      "--  246_1p12.jpg\n",
      "--  246_1p13.jpg\n",
      "--  246_1p14.jpg\n",
      "--  246_1p15.jpg\n",
      "--  246_1p16.jpg\n",
      "--  246_1p17.jpg\n",
      "--  246_1p21.jpg\n",
      "--  246_1p22.jpg\n",
      "--  246_1p23.jpg\n",
      "--  246_1p24.jpg\n",
      "--  246_1p25.jpg\n",
      "--  246_1p26.jpg\n",
      "--  246_1p27.jpg\n",
      "--  248_1p11.jpg\n",
      "--  248_1p12.jpg\n",
      "--  248_1p13.jpg\n",
      "--  248_1p14.jpg\n",
      "--  248_1p15.jpg\n",
      "--  248_1p16.jpg\n",
      "--  248_1p17.jpg\n",
      "--  248_1p21.jpg\n",
      "--  248_1p22.jpg\n",
      "--  248_1p23.jpg\n",
      "--  248_1p24.jpg\n",
      "--  248_1p25.jpg\n",
      "--  248_1p26.jpg\n",
      "--  248_1p27.jpg\n",
      "--  24_1p11.jpg\n",
      "--  24_1p12.jpg\n",
      "--  24_1p13.jpg\n",
      "--  24_1p14.jpg\n",
      "--  24_1p15.jpg\n",
      "--  24_1p16.jpg\n",
      "--  24_1p17.jpg\n",
      "--  251_1p11.jpg\n",
      "--  251_1p12.jpg\n",
      "--  251_1p13.jpg\n",
      "--  251_1p14.jpg\n",
      "--  251_1p15.jpg\n",
      "--  251_1p16.jpg\n",
      "--  251_1p17.jpg\n",
      "--  251_1p21.jpg\n",
      "--  251_1p22.jpg\n",
      "--  251_1p23.jpg\n",
      "--  251_1p24.jpg\n",
      "--  251_1p25.jpg\n",
      "--  251_1p26.jpg\n",
      "--  251_1p27.jpg\n",
      "--  252_1p11.jpg\n",
      "--  252_1p12.jpg\n",
      "--  252_1p13.jpg\n",
      "--  252_1p14.jpg\n",
      "--  252_1p15.jpg\n",
      "--  252_1p16.jpg\n",
      "--  252_1p17.jpg\n",
      "--  255_1p11.jpg\n",
      "--  255_1p12.jpg\n",
      "--  255_1p13.jpg\n",
      "--  255_1p14.jpg\n",
      "--  255_1p15.jpg\n",
      "--  255_1p16.jpg\n",
      "--  255_1p17.jpg\n",
      "--  255_1p21.jpg\n",
      "--  255_1p22.jpg\n",
      "--  255_1p23.jpg\n",
      "--  255_1p24.jpg\n",
      "--  255_1p25.jpg\n",
      "--  255_1p26.jpg\n",
      "--  255_1p27.jpg\n",
      "--  257_1p11.jpg\n",
      "--  257_1p12.jpg\n",
      "--  257_1p13.jpg\n",
      "--  257_1p14.jpg\n",
      "--  257_1p15.jpg\n",
      "--  257_1p16.jpg\n",
      "--  257_1p17.jpg\n",
      "--  257_1p21.jpg\n",
      "--  257_1p22.jpg\n",
      "--  257_1p23.jpg\n",
      "--  257_1p24.jpg\n",
      "--  257_1p25.jpg\n",
      "--  257_1p26.jpg\n",
      "--  257_1p27.jpg\n",
      "--  258_1p11.jpg\n",
      "--  258_1p12.jpg\n",
      "--  258_1p13.jpg\n",
      "--  258_1p14.jpg\n",
      "--  258_1p15.jpg\n",
      "--  258_1p16.jpg\n",
      "--  258_1p17.jpg\n",
      "--  258_1p21.jpg\n",
      "--  258_1p22.jpg\n",
      "--  258_1p23.jpg\n",
      "--  258_1p24.jpg\n",
      "--  258_1p25.jpg\n",
      "--  258_1p26.jpg\n",
      "--  258_1p27.jpg\n",
      "--  259_1p11.jpg\n",
      "--  259_1p12.jpg\n",
      "--  259_1p13.jpg\n",
      "--  259_1p14.jpg\n",
      "--  259_1p15.jpg\n",
      "--  259_1p16.jpg\n",
      "--  259_1p17.jpg\n",
      "--  259_1p21.jpg\n",
      "--  259_1p22.jpg\n",
      "--  259_1p23.jpg\n",
      "--  259_1p24.jpg\n",
      "--  259_1p25.jpg\n",
      "--  259_1p26.jpg\n",
      "--  259_1p27.jpg\n",
      "--  261_1p11.jpg\n",
      "--  261_1p12.jpg\n",
      "--  261_1p13.jpg\n",
      "--  261_1p14.jpg\n",
      "--  261_1p15.jpg\n",
      "--  261_1p16.jpg\n",
      "--  261_1p17.jpg\n",
      "--  263_1p11.jpg\n",
      "--  263_1p12.jpg\n",
      "--  263_1p13.jpg\n",
      "--  263_1p14.jpg\n",
      "--  263_1p15.jpg\n",
      "--  263_1p16.jpg\n",
      "--  263_1p17.jpg\n",
      "--  263_1p21.jpg\n",
      "--  263_1p22.jpg\n",
      "--  263_1p23.jpg\n",
      "--  263_1p24.jpg\n",
      "--  263_1p25.jpg\n",
      "--  263_1p26.jpg\n",
      "--  263_1p27.jpg\n",
      "--  264_1p11.jpg\n",
      "--  264_1p12.jpg\n",
      "--  264_1p13.jpg\n",
      "--  264_1p14.jpg\n",
      "--  264_1p15.jpg\n",
      "--  264_1p16.jpg\n",
      "--  264_1p17.jpg\n",
      "--  265_1p11.jpg\n",
      "--  265_1p12.jpg\n",
      "--  265_1p13.jpg\n",
      "--  265_1p14.jpg\n",
      "--  265_1p15.jpg\n",
      "--  265_1p16.jpg\n",
      "--  265_1p17.jpg\n",
      "--  265_1p21.jpg\n",
      "--  265_1p22.jpg\n",
      "--  265_1p23.jpg\n",
      "--  265_1p24.jpg\n",
      "--  265_1p25.jpg\n",
      "--  265_1p26.jpg\n",
      "--  265_1p27.jpg\n",
      "--  267_1p11.jpg\n",
      "--  267_1p12.jpg\n",
      "--  267_1p13.jpg\n",
      "--  267_1p14.jpg\n",
      "--  267_1p15.jpg\n",
      "--  267_1p16.jpg\n",
      "--  267_1p17.jpg\n",
      "--  268_1p11.jpg\n",
      "--  268_1p12.jpg\n",
      "--  268_1p13.jpg\n",
      "--  268_1p14.jpg\n",
      "--  268_1p15.jpg\n",
      "--  268_1p16.jpg\n",
      "--  268_1p17.jpg\n",
      "--  268_1p21.jpg\n",
      "--  268_1p22.jpg\n",
      "--  268_1p23.jpg\n",
      "--  268_1p24.jpg\n",
      "--  268_1p25.jpg\n",
      "--  268_1p26.jpg\n",
      "--  268_1p27.jpg\n",
      "--  269_1p11.jpg\n",
      "--  269_1p12.jpg\n",
      "--  269_1p13.jpg\n",
      "--  269_1p14.jpg\n",
      "--  269_1p15.jpg\n",
      "--  269_1p16.jpg\n",
      "--  269_1p17.jpg\n",
      "--  26_1p11.jpg\n",
      "--  26_1p12.jpg\n",
      "--  26_1p13.jpg\n",
      "--  26_1p14.jpg\n",
      "--  26_1p15.jpg\n",
      "--  26_1p16.jpg\n",
      "--  26_1p17.jpg\n",
      "--  271_1p11.jpg\n",
      "--  271_1p12.jpg\n",
      "--  271_1p13.jpg\n",
      "--  271_1p14.jpg\n",
      "--  271_1p15.jpg\n",
      "--  271_1p16.jpg\n",
      "--  271_1p17.jpg\n",
      "--  272_1p11.jpg\n",
      "--  272_1p12.jpg\n",
      "--  272_1p13.jpg\n",
      "--  272_1p14.jpg\n",
      "--  272_1p15.jpg\n",
      "--  272_1p16.jpg\n",
      "--  272_1p17.jpg\n",
      "--  275_1p11.jpg\n",
      "--  275_1p12.jpg\n",
      "--  275_1p13.jpg\n",
      "--  275_1p14.jpg\n",
      "--  275_1p15.jpg\n",
      "--  275_1p16.jpg\n",
      "--  275_1p17.jpg\n",
      "--  277_1p11.jpg\n",
      "--  277_1p12.jpg\n",
      "--  277_1p13.jpg\n",
      "--  277_1p14.jpg\n",
      "--  277_1p15.jpg\n",
      "--  277_1p16.jpg\n",
      "--  277_1p17.jpg\n",
      "--  278_1p11.jpg\n",
      "--  278_1p12.jpg\n",
      "--  278_1p13.jpg\n",
      "--  278_1p14.jpg\n",
      "--  278_1p15.jpg\n",
      "--  278_1p16.jpg\n",
      "--  278_1p17.jpg\n",
      "--  278_1p21.jpg\n",
      "--  278_1p22.jpg\n",
      "--  278_1p23.jpg\n",
      "--  278_1p24.jpg\n",
      "--  278_1p25.jpg\n",
      "--  278_1p26.jpg\n",
      "--  278_1p27.jpg\n",
      "--  279_1p11.jpg\n",
      "--  279_1p12.jpg\n",
      "--  279_1p13.jpg\n",
      "--  279_1p14.jpg\n",
      "--  279_1p15.jpg\n",
      "--  279_1p16.jpg\n",
      "--  279_1p17.jpg\n",
      "--  279_1p21.jpg\n",
      "--  279_1p22.jpg\n",
      "--  279_1p23.jpg\n",
      "--  279_1p24.jpg\n",
      "--  279_1p25.jpg\n",
      "--  279_1p26.jpg\n",
      "--  279_1p27.jpg\n",
      "--  27_1.png\n",
      "--  27_1p11.jpg\n",
      "--  27_1p12.jpg\n",
      "--  27_1p13.jpg\n",
      "--  27_1p14.jpg\n",
      "--  27_1p15.jpg\n",
      "--  27_1p16.jpg\n",
      "--  27_1p17.jpg\n",
      "--  27_2.png\n",
      "--  27_3.png\n",
      "--  27_4.png\n",
      "--  27_5.png\n",
      "--  27_6.png\n",
      "--  27_7.png\n",
      "--  27_8.png\n",
      "--  27_9.png\n",
      "--  281_1p11.jpg\n",
      "--  281_1p12.jpg\n",
      "--  281_1p13.jpg\n",
      "--  281_1p14.jpg\n",
      "--  281_1p15.jpg\n",
      "--  281_1p16.jpg\n",
      "--  281_1p17.jpg\n",
      "--  281_1p21.jpg\n",
      "--  281_1p22.jpg\n",
      "--  281_1p23.jpg\n",
      "--  281_1p24.jpg\n",
      "--  281_1p25.jpg\n",
      "--  281_1p26.jpg\n",
      "--  281_1p27.jpg\n",
      "--  282_1p11.jpg\n",
      "--  282_1p12.jpg\n",
      "--  282_1p13.jpg\n",
      "--  282_1p14.jpg\n",
      "--  282_1p15.jpg\n",
      "--  282_1p16.jpg\n",
      "--  282_1p17.jpg\n",
      "--  283_1p11.jpg\n",
      "--  283_1p12.jpg\n",
      "--  283_1p13.jpg\n",
      "--  283_1p14.jpg\n",
      "--  283_1p15.jpg\n",
      "--  283_1p16.jpg\n",
      "--  283_1p17.jpg\n",
      "--  283_1p21.jpg\n",
      "--  283_1p22.jpg\n",
      "--  283_1p23.jpg\n",
      "--  283_1p24.jpg\n",
      "--  283_1p25.jpg\n",
      "--  283_1p26.jpg\n",
      "--  283_1p27.jpg\n",
      "--  284_1p11.jpg\n",
      "--  284_1p12.jpg\n",
      "--  284_1p13.jpg\n",
      "--  284_1p14.jpg\n",
      "--  284_1p15.jpg\n",
      "--  284_1p16.jpg\n",
      "--  284_1p17.jpg\n",
      "--  284_1p21.jpg\n",
      "--  284_1p22.jpg\n",
      "--  284_1p23.jpg\n",
      "--  284_1p24.jpg\n",
      "--  284_1p25.jpg\n",
      "--  284_1p26.jpg\n",
      "--  284_1p27.jpg\n",
      "--  286_1p11.jpg\n",
      "--  286_1p12.jpg\n",
      "--  286_1p13.jpg\n",
      "--  286_1p14.jpg\n",
      "--  286_1p15.jpg\n",
      "--  286_1p16.jpg\n",
      "--  286_1p17.jpg\n",
      "--  286_1p21.jpg\n",
      "--  286_1p22.jpg\n",
      "--  286_1p23.jpg\n",
      "--  286_1p24.jpg\n",
      "--  286_1p25.jpg\n",
      "--  286_1p26.jpg\n",
      "--  286_1p27.jpg\n",
      "--  288_1p11.jpg\n",
      "--  288_1p12.jpg\n",
      "--  288_1p13.jpg\n",
      "--  288_1p14.jpg\n",
      "--  288_1p15.jpg\n",
      "--  288_1p16.jpg\n",
      "--  288_1p17.jpg\n",
      "--  289_1p11.jpg\n",
      "--  289_1p12.jpg\n",
      "--  289_1p13.jpg\n",
      "--  289_1p14.jpg\n",
      "--  289_1p15.jpg\n",
      "--  289_1p16.jpg\n",
      "--  289_1p17.jpg\n",
      "--  28_1.png\n",
      "--  28_1p11.jpg\n",
      "--  28_1p12.jpg\n",
      "--  28_1p13.jpg\n",
      "--  28_1p14.jpg\n",
      "--  28_1p15.jpg\n",
      "--  28_1p16.jpg\n",
      "--  28_1p17.jpg\n",
      "--  28_2.png\n",
      "--  28_3.png\n",
      "--  28_4.png\n",
      "--  28_5.png\n",
      "--  28_6.png\n",
      "--  28_7.png\n",
      "--  28_8.png\n",
      "--  28_9.png\n",
      "--  290_1p11.jpg\n",
      "--  290_1p12.jpg\n",
      "--  290_1p13.jpg\n",
      "--  290_1p14.jpg\n",
      "--  290_1p15.jpg\n",
      "--  290_1p16.jpg\n",
      "--  290_1p17.jpg\n",
      "--  291_1p11.jpg\n",
      "--  291_1p12.jpg\n",
      "--  291_1p13.jpg\n",
      "--  291_1p14.jpg\n",
      "--  291_1p15.jpg\n",
      "--  291_1p16.jpg\n",
      "--  291_1p17.jpg\n",
      "--  293_1p11.jpg\n",
      "--  293_1p12.jpg\n",
      "--  293_1p13.jpg\n",
      "--  293_1p14.jpg\n",
      "--  293_1p15.jpg\n",
      "--  293_1p16.jpg\n",
      "--  293_1p17.jpg\n",
      "--  294_1p11.jpg\n",
      "--  294_1p12.jpg\n",
      "--  294_1p13.jpg\n",
      "--  294_1p14.jpg\n",
      "--  294_1p15.jpg\n",
      "--  294_1p16.jpg\n",
      "--  294_1p17.jpg\n",
      "--  294_1p21.jpg\n",
      "--  294_1p22.jpg\n",
      "--  294_1p23.jpg\n",
      "--  294_1p24.jpg\n",
      "--  294_1p25.jpg\n",
      "--  294_1p26.jpg\n",
      "--  294_1p27.jpg\n",
      "--  294_2p11.jpg\n",
      "--  294_2p12.jpg\n",
      "--  294_2p13.jpg\n",
      "--  294_2p14.jpg\n",
      "--  294_2p15.jpg\n",
      "--  294_2p16.jpg\n",
      "--  294_2p17.jpg\n",
      "--  294_3p11.jpg\n",
      "--  294_3p12.jpg\n",
      "--  294_3p13.jpg\n",
      "--  294_3p14.jpg\n",
      "--  294_3p15.jpg\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--  294_3p16.jpg\n",
      "--  294_3p17.jpg\n",
      "--  295_1p11.jpg\n",
      "--  295_1p12.jpg\n",
      "--  295_1p13.jpg\n",
      "--  295_1p14.jpg\n",
      "--  295_1p15.jpg\n",
      "--  295_1p16.jpg\n",
      "--  295_1p17.jpg\n",
      "--  296_1p11.jpg\n",
      "--  296_1p12.jpg\n",
      "--  296_1p13.jpg\n",
      "--  296_1p14.jpg\n",
      "--  296_1p15.jpg\n",
      "--  296_1p16.jpg\n",
      "--  296_1p17.jpg\n",
      "--  296_2p11.jpg\n",
      "--  296_2p12.jpg\n",
      "--  296_2p13.jpg\n",
      "--  296_2p14.jpg\n",
      "--  296_2p15.jpg\n",
      "--  296_2p16.jpg\n",
      "--  296_2p17.jpg\n",
      "--  296_3p11.jpg\n",
      "--  296_3p12.jpg\n",
      "--  296_3p13.jpg\n",
      "--  296_3p14.jpg\n",
      "--  296_3p15.jpg\n",
      "--  296_3p16.jpg\n",
      "--  296_3p17.jpg\n",
      "--  296_4p11.jpg\n",
      "--  296_4p12.jpg\n",
      "--  296_4p13.jpg\n",
      "--  296_4p14.jpg\n",
      "--  296_4p15.jpg\n",
      "--  296_4p16.jpg\n",
      "--  296_4p17.jpg\n",
      "--  296_5p11.jpg\n",
      "--  296_5p12.jpg\n",
      "--  296_5p13.jpg\n",
      "--  296_5p14.jpg\n",
      "--  296_5p15.jpg\n",
      "--  296_5p16.jpg\n",
      "--  296_5p17.jpg\n",
      "--  297_1p11.jpg\n",
      "--  297_1p12.jpg\n",
      "--  297_1p13.jpg\n",
      "--  297_1p14.jpg\n",
      "--  297_1p15.jpg\n",
      "--  297_1p16.jpg\n",
      "--  297_1p17.jpg\n",
      "--  298_1p11.jpg\n",
      "--  298_1p12.jpg\n",
      "--  298_1p13.jpg\n",
      "--  298_1p14.jpg\n",
      "--  298_1p15.jpg\n",
      "--  298_1p16.jpg\n",
      "--  298_1p17.jpg\n",
      "--  299_1p11.jpg\n",
      "--  299_1p12.jpg\n",
      "--  299_1p13.jpg\n",
      "--  299_1p14.jpg\n",
      "--  299_1p15.jpg\n",
      "--  299_1p16.jpg\n",
      "--  299_1p17.jpg\n",
      "--  29_1p11.jpg\n",
      "--  29_1p12.jpg\n",
      "--  29_1p13.jpg\n",
      "--  29_1p14.jpg\n",
      "--  29_1p15.jpg\n",
      "--  29_1p16.jpg\n",
      "--  29_1p17.jpg\n",
      "--  300_1p11.jpg\n",
      "--  300_1p12.jpg\n",
      "--  300_1p13.jpg\n",
      "--  300_1p14.jpg\n",
      "--  300_1p15.jpg\n",
      "--  300_1p16.jpg\n",
      "--  300_1p17.jpg\n",
      "--  301_1p11.jpg\n",
      "--  301_1p12.jpg\n",
      "--  301_1p13.jpg\n",
      "--  301_1p14.jpg\n",
      "--  301_1p15.jpg\n",
      "--  301_1p16.jpg\n",
      "--  301_1p17.jpg\n",
      "--  302_1p11.jpg\n",
      "--  302_1p12.jpg\n",
      "--  302_1p13.jpg\n",
      "--  302_1p14.jpg\n",
      "--  302_1p15.jpg\n",
      "--  302_1p16.jpg\n",
      "--  302_1p17.jpg\n",
      "--  304_1p11.jpg\n",
      "--  304_1p12.jpg\n",
      "--  304_1p13.jpg\n",
      "--  304_1p14.jpg\n",
      "--  304_1p15.jpg\n",
      "--  304_1p16.jpg\n",
      "--  304_1p17.jpg\n",
      "--  306_1p11.jpg\n",
      "--  306_1p12.jpg\n",
      "--  306_1p13.jpg\n",
      "--  306_1p14.jpg\n",
      "--  306_1p15.jpg\n",
      "--  306_1p16.jpg\n",
      "--  306_1p17.jpg\n",
      "--  307_1p11.jpg\n",
      "--  307_1p12.jpg\n",
      "--  307_1p13.jpg\n",
      "--  307_1p14.jpg\n",
      "--  307_1p15.jpg\n",
      "--  307_1p16.jpg\n",
      "--  307_1p17.jpg\n",
      "--  308_1p11.jpg\n",
      "--  308_1p12.jpg\n",
      "--  308_1p13.jpg\n",
      "--  308_1p14.jpg\n",
      "--  308_1p15.jpg\n",
      "--  308_1p16.jpg\n",
      "--  308_1p17.jpg\n",
      "--  30_1p11.jpg\n",
      "--  30_1p12.jpg\n",
      "--  30_1p13.jpg\n",
      "--  30_1p14.jpg\n",
      "--  30_1p15.jpg\n",
      "--  30_1p16.jpg\n",
      "--  30_1p17.jpg\n",
      "--  311_1p11.jpg\n",
      "--  311_1p12.jpg\n",
      "--  311_1p13.jpg\n",
      "--  311_1p14.jpg\n",
      "--  311_1p15.jpg\n",
      "--  311_1p16.jpg\n",
      "--  311_1p17.jpg\n",
      "--  313_1p11.jpg\n",
      "--  313_1p12.jpg\n",
      "--  313_1p13.jpg\n",
      "--  313_1p14.jpg\n",
      "--  313_1p15.jpg\n",
      "--  313_1p16.jpg\n",
      "--  313_1p17.jpg\n",
      "--  313_2p11.jpg\n",
      "--  313_2p12.jpg\n",
      "--  313_2p13.jpg\n",
      "--  313_2p14.jpg\n",
      "--  313_2p15.jpg\n",
      "--  313_2p16.jpg\n",
      "--  313_2p17.jpg\n",
      "--  314_1p11.jpg\n",
      "--  314_1p12.jpg\n",
      "--  314_1p13.jpg\n",
      "--  314_1p14.jpg\n",
      "--  314_1p15.jpg\n",
      "--  314_1p16.jpg\n",
      "--  314_1p17.jpg\n",
      "--  314_1p21.jpg\n",
      "--  314_1p22.jpg\n",
      "--  314_1p23.jpg\n",
      "--  314_1p24.jpg\n",
      "--  314_1p25.jpg\n",
      "--  314_1p26.jpg\n",
      "--  314_1p27.jpg\n",
      "--  315_1p11.jpg\n",
      "--  315_1p12.jpg\n",
      "--  315_1p13.jpg\n",
      "--  315_1p14.jpg\n",
      "--  315_1p15.jpg\n",
      "--  315_1p16.jpg\n",
      "--  315_1p17.jpg\n",
      "--  315_2p11.jpg\n",
      "--  315_2p12.jpg\n",
      "--  315_2p13.jpg\n",
      "--  315_2p14.jpg\n",
      "--  315_2p15.jpg\n",
      "--  315_2p16.jpg\n",
      "--  315_2p17.jpg\n",
      "--  316_1p11.jpg\n",
      "--  316_1p12.jpg\n",
      "--  316_1p13.jpg\n",
      "--  316_1p14.jpg\n",
      "--  316_1p15.jpg\n",
      "--  316_1p16.jpg\n",
      "--  316_1p17.jpg\n",
      "--  317_1p11.jpg\n",
      "--  317_1p12.jpg\n",
      "--  317_1p13.jpg\n",
      "--  317_1p14.jpg\n",
      "--  317_1p15.jpg\n",
      "--  317_1p16.jpg\n",
      "--  317_1p17.jpg\n",
      "--  317_2p11.jpg\n",
      "--  317_2p12.jpg\n",
      "--  317_2p13.jpg\n",
      "--  317_2p14.jpg\n",
      "--  317_2p15.jpg\n",
      "--  317_2p16.jpg\n",
      "--  317_2p17.jpg\n",
      "--  318_1p11.jpg\n",
      "--  318_1p12.jpg\n",
      "--  318_1p13.jpg\n",
      "--  318_1p14.jpg\n",
      "--  318_1p15.jpg\n",
      "--  318_1p16.jpg\n",
      "--  318_1p17.jpg\n",
      "--  318_2p11.jpg\n",
      "--  318_2p12.jpg\n",
      "--  318_2p13.jpg\n",
      "--  318_2p14.jpg\n",
      "--  318_2p15.jpg\n",
      "--  318_2p16.jpg\n",
      "--  318_2p17.jpg\n",
      "--  319_1p11.jpg\n",
      "--  319_1p12.jpg\n",
      "--  319_1p13.jpg\n",
      "--  319_1p14.jpg\n",
      "--  319_1p15.jpg\n",
      "--  319_1p16.jpg\n",
      "--  319_1p17.jpg\n",
      "--  319_2p11.jpg\n",
      "--  319_2p12.jpg\n",
      "--  319_2p13.jpg\n",
      "--  319_2p14.jpg\n",
      "--  319_2p15.jpg\n",
      "--  319_2p16.jpg\n",
      "--  319_2p17.jpg\n",
      "--  320_1p11.jpg\n",
      "--  320_1p12.jpg\n",
      "--  320_1p13.jpg\n",
      "--  320_1p14.jpg\n",
      "--  320_1p15.jpg\n",
      "--  320_1p16.jpg\n",
      "--  320_1p17.jpg\n",
      "--  321_1p11.jpg\n",
      "--  321_1p12.jpg\n",
      "--  321_1p13.jpg\n",
      "--  321_1p14.jpg\n",
      "--  321_1p15.jpg\n",
      "--  321_1p16.jpg\n",
      "--  321_1p17.jpg\n",
      "--  32_1p11.jpg\n",
      "--  32_1p12.jpg\n",
      "--  32_1p13.jpg\n",
      "--  32_1p14.jpg\n",
      "--  32_1p15.jpg\n",
      "--  32_1p16.jpg\n",
      "--  32_1p17.jpg\n",
      "--  330_1p11.jpg\n",
      "--  330_1p12.jpg\n",
      "--  330_1p13.jpg\n",
      "--  330_1p14.jpg\n",
      "--  330_1p15.jpg\n",
      "--  330_1p16.jpg\n",
      "--  330_1p17.jpg\n",
      "--  330_2p11.jpg\n",
      "--  330_2p12.jpg\n",
      "--  330_2p13.jpg\n",
      "--  330_2p14.jpg\n",
      "--  330_2p15.jpg\n",
      "--  330_2p16.jpg\n",
      "--  330_2p17.jpg\n",
      "--  337_1p11.jpg\n",
      "--  337_1p12.jpg\n",
      "--  337_1p13.jpg\n",
      "--  337_1p14.jpg\n",
      "--  337_1p15.jpg\n",
      "--  337_1p16.jpg\n",
      "--  337_1p17.jpg\n",
      "--  337_2p11.jpg\n",
      "--  337_2p12.jpg\n",
      "--  337_2p13.jpg\n",
      "--  337_2p14.jpg\n",
      "--  337_2p15.jpg\n",
      "--  337_2p16.jpg\n",
      "--  337_2p17.jpg\n",
      "--  337_3p11.jpg\n",
      "--  337_3p12.jpg\n",
      "--  337_3p13.jpg\n",
      "--  337_3p14.jpg\n",
      "--  337_3p15.jpg\n",
      "--  337_3p16.jpg\n",
      "--  337_3p17.jpg\n",
      "--  338_1p11.jpg\n",
      "--  338_1p12.jpg\n",
      "--  338_1p13.jpg\n",
      "--  338_1p14.jpg\n",
      "--  338_1p15.jpg\n",
      "--  338_1p16.jpg\n",
      "--  338_1p17.jpg\n",
      "--  338_2p11.jpg\n",
      "--  338_2p12.jpg\n",
      "--  338_2p13.jpg\n",
      "--  338_2p14.jpg\n",
      "--  338_2p15.jpg\n",
      "--  338_2p16.jpg\n",
      "--  338_2p17.jpg\n",
      "--  338_2p21.jpg\n",
      "--  338_2p22.jpg\n",
      "--  338_2p23.jpg\n",
      "--  338_2p24.jpg\n",
      "--  338_2p25.jpg\n",
      "--  338_2p26.jpg\n",
      "--  338_2p27.jpg\n",
      "--  33_1p11.jpg\n",
      "--  33_1p12.jpg\n",
      "--  33_1p13.jpg\n",
      "--  33_1p14.jpg\n",
      "--  33_1p15.jpg\n",
      "--  33_1p16.jpg\n",
      "--  33_1p17.jpg\n",
      "--  340_1p11.jpg\n",
      "--  340_1p12.jpg\n",
      "--  340_1p13.jpg\n",
      "--  340_1p14.jpg\n",
      "--  340_1p15.jpg\n",
      "--  340_1p16.jpg\n",
      "--  340_1p17.jpg\n",
      "--  341_1p11.jpg\n",
      "--  341_1p12.jpg\n",
      "--  341_1p13.jpg\n",
      "--  341_1p14.jpg\n",
      "--  341_1p15.jpg\n",
      "--  341_1p16.jpg\n",
      "--  341_1p17.jpg\n",
      "--  342_1p11.jpg\n",
      "--  342_1p12.jpg\n",
      "--  342_1p13.jpg\n",
      "--  342_1p14.jpg\n",
      "--  342_1p15.jpg\n",
      "--  342_1p16.jpg\n",
      "--  342_1p17.jpg\n",
      "--  343_1p11.jpg\n",
      "--  343_1p12.jpg\n",
      "--  343_1p13.jpg\n",
      "--  343_1p14.jpg\n",
      "--  343_1p15.jpg\n",
      "--  343_1p16.jpg\n",
      "--  343_1p17.jpg\n",
      "--  344_1p11.jpg\n",
      "--  344_1p12.jpg\n",
      "--  344_1p13.jpg\n",
      "--  344_1p14.jpg\n",
      "--  344_1p15.jpg\n",
      "--  344_1p16.jpg\n",
      "--  344_1p17.jpg\n",
      "--  345_1p11.jpg\n",
      "--  345_1p12.jpg\n",
      "--  345_1p13.jpg\n",
      "--  345_1p14.jpg\n",
      "--  345_1p15.jpg\n",
      "--  345_1p16.jpg\n",
      "--  345_1p17.jpg\n",
      "--  348_1p11.jpg\n",
      "--  348_1p12.jpg\n",
      "--  348_1p13.jpg\n",
      "--  348_1p14.jpg\n",
      "--  348_1p15.jpg\n",
      "--  348_1p16.jpg\n",
      "--  348_1p17.jpg\n",
      "--  348_1p21.jpg\n",
      "--  348_1p22.jpg\n",
      "--  348_1p23.jpg\n",
      "--  348_1p24.jpg\n",
      "--  348_1p25.jpg\n",
      "--  348_1p26.jpg\n",
      "--  348_1p27.jpg\n",
      "--  34_1p11.jpg\n",
      "--  34_1p12.jpg\n",
      "--  34_1p13.jpg\n",
      "--  34_1p14.jpg\n",
      "--  34_1p15.jpg\n",
      "--  34_1p16.jpg\n",
      "--  34_1p17.jpg\n",
      "--  350_1p11.jpg\n",
      "--  350_1p12.jpg\n",
      "--  350_1p13.jpg\n",
      "--  350_1p14.jpg\n",
      "--  350_1p15.jpg\n",
      "--  350_1p16.jpg\n",
      "--  350_1p17.jpg\n",
      "--  350_2p11.jpg\n",
      "--  350_2p12.jpg\n",
      "--  350_2p13.jpg\n",
      "--  350_2p14.jpg\n",
      "--  350_2p15.jpg\n",
      "--  350_2p16.jpg\n",
      "--  350_2p17.jpg\n",
      "--  350_2p21.jpg\n",
      "--  350_2p22.jpg\n",
      "--  350_2p23.jpg\n",
      "--  350_2p24.jpg\n",
      "--  350_2p25.jpg\n",
      "--  350_2p26.jpg\n",
      "--  350_2p27.jpg\n",
      "--  351_1p11.jpg\n",
      "--  351_1p12.jpg\n",
      "--  351_1p13.jpg\n",
      "--  351_1p14.jpg\n",
      "--  351_1p15.jpg\n",
      "--  351_1p16.jpg\n",
      "--  351_1p17.jpg\n",
      "--  351_2p11.jpg\n",
      "--  351_2p12.jpg\n",
      "--  351_2p13.jpg\n",
      "--  351_2p14.jpg\n",
      "--  351_2p15.jpg\n",
      "--  351_2p16.jpg\n",
      "--  351_2p17.jpg\n",
      "--  352_1p11.jpg\n",
      "--  352_1p12.jpg\n",
      "--  352_1p13.jpg\n",
      "--  352_1p14.jpg\n",
      "--  352_1p15.jpg\n",
      "--  352_1p16.jpg\n",
      "--  352_1p17.jpg\n",
      "--  353_1p11.jpg\n",
      "--  353_1p12.jpg\n",
      "--  353_1p13.jpg\n",
      "--  353_1p14.jpg\n",
      "--  353_1p15.jpg\n",
      "--  353_1p16.jpg\n",
      "--  353_1p17.jpg\n",
      "--  353_2p11.jpg\n",
      "--  353_2p12.jpg\n",
      "--  353_2p13.jpg\n",
      "--  353_2p14.jpg\n",
      "--  353_2p15.jpg\n",
      "--  353_2p16.jpg\n",
      "--  353_2p17.jpg\n",
      "--  354_1p11.jpg\n",
      "--  354_1p12.jpg\n",
      "--  354_1p13.jpg\n",
      "--  354_1p14.jpg\n",
      "--  354_1p15.jpg\n",
      "--  354_1p16.jpg\n",
      "--  354_1p17.jpg\n",
      "--  355_1p11.jpg\n",
      "--  355_1p12.jpg\n",
      "--  355_1p13.jpg\n",
      "--  355_1p14.jpg\n",
      "--  355_1p15.jpg\n",
      "--  355_1p16.jpg\n",
      "--  355_1p17.jpg\n",
      "--  357_1p11.jpg\n",
      "--  357_1p12.jpg\n",
      "--  357_1p13.jpg\n",
      "--  357_1p14.jpg\n",
      "--  357_1p15.jpg\n",
      "--  357_1p16.jpg\n",
      "--  357_1p17.jpg\n",
      "--  358_1p11.jpg\n",
      "--  358_1p12.jpg\n",
      "--  358_1p13.jpg\n",
      "--  358_1p14.jpg\n",
      "--  358_1p15.jpg\n",
      "--  358_1p16.jpg\n",
      "--  358_1p17.jpg\n",
      "--  358_1p21.jpg\n",
      "--  358_1p22.jpg\n",
      "--  358_1p23.jpg\n",
      "--  358_1p24.jpg\n",
      "--  358_1p25.jpg\n",
      "--  358_1p26.jpg\n",
      "--  358_1p27.jpg\n",
      "--  358_2p11.jpg\n",
      "--  358_2p12.jpg\n",
      "--  358_2p13.jpg\n",
      "--  358_2p14.jpg\n",
      "--  358_2p15.jpg\n",
      "--  358_2p16.jpg\n",
      "--  358_2p17.jpg\n",
      "--  35_1p11.jpg\n",
      "--  35_1p12.jpg\n",
      "--  35_1p13.jpg\n",
      "--  35_1p14.jpg\n",
      "--  35_1p15.jpg\n",
      "--  35_1p16.jpg\n",
      "--  35_1p17.jpg\n",
      "--  360_1p11.jpg\n",
      "--  360_1p12.jpg\n",
      "--  360_1p13.jpg\n",
      "--  360_1p14.jpg\n",
      "--  360_1p15.jpg\n",
      "--  360_1p16.jpg\n",
      "--  360_1p17.jpg\n",
      "--  360_2p11.jpg\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--  360_2p12.jpg\n",
      "--  360_2p13.jpg\n",
      "--  360_2p14.jpg\n",
      "--  360_2p15.jpg\n",
      "--  360_2p16.jpg\n",
      "--  360_2p17.jpg\n",
      "--  363_1p11.jpg\n",
      "--  363_1p12.jpg\n",
      "--  363_1p13.jpg\n",
      "--  363_1p14.jpg\n",
      "--  363_1p15.jpg\n",
      "--  363_1p16.jpg\n",
      "--  363_1p17.jpg\n",
      "--  367_1p11.jpg\n",
      "--  367_1p12.jpg\n",
      "--  367_1p13.jpg\n",
      "--  367_1p14.jpg\n",
      "--  367_1p15.jpg\n",
      "--  367_1p16.jpg\n",
      "--  367_1p17.jpg\n",
      "--  367_2p11.jpg\n",
      "--  367_2p12.jpg\n",
      "--  367_2p13.jpg\n",
      "--  367_2p14.jpg\n",
      "--  367_2p15.jpg\n",
      "--  367_2p16.jpg\n",
      "--  367_2p17.jpg\n",
      "--  368_1p11.jpg\n",
      "--  368_1p12.jpg\n",
      "--  368_1p13.jpg\n",
      "--  368_1p14.jpg\n",
      "--  368_1p15.jpg\n",
      "--  368_1p16.jpg\n",
      "--  368_1p17.jpg\n",
      "--  36_1p11.jpg\n",
      "--  36_1p12.jpg\n",
      "--  36_1p13.jpg\n",
      "--  36_1p14.jpg\n",
      "--  36_1p15.jpg\n",
      "--  36_1p16.jpg\n",
      "--  36_1p17.jpg\n",
      "--  370_1p11.jpg\n",
      "--  370_1p12.jpg\n",
      "--  370_1p13.jpg\n",
      "--  370_1p14.jpg\n",
      "--  370_1p15.jpg\n",
      "--  370_1p16.jpg\n",
      "--  370_1p17.jpg\n",
      "--  371_1p11.jpg\n",
      "--  371_1p12.jpg\n",
      "--  371_1p13.jpg\n",
      "--  371_1p14.jpg\n",
      "--  371_1p15.jpg\n",
      "--  371_1p16.jpg\n",
      "--  371_1p17.jpg\n",
      "--  371_2p11.jpg\n",
      "--  371_2p12.jpg\n",
      "--  371_2p13.jpg\n",
      "--  371_2p14.jpg\n",
      "--  371_2p15.jpg\n",
      "--  371_2p16.jpg\n",
      "--  371_2p17.jpg\n",
      "--  372_1p11.jpg\n",
      "--  372_1p12.jpg\n",
      "--  372_1p13.jpg\n",
      "--  372_1p14.jpg\n",
      "--  372_1p15.jpg\n",
      "--  372_1p16.jpg\n",
      "--  372_1p17.jpg\n",
      "--  374_1p11.jpg\n",
      "--  374_1p12.jpg\n",
      "--  374_1p13.jpg\n",
      "--  374_1p14.jpg\n",
      "--  374_1p15.jpg\n",
      "--  374_1p16.jpg\n",
      "--  374_1p17.jpg\n",
      "--  374_1p21.jpg\n",
      "--  374_1p22.jpg\n",
      "--  374_1p23.jpg\n",
      "--  374_1p24.jpg\n",
      "--  374_1p25.jpg\n",
      "--  374_1p26.jpg\n",
      "--  374_1p27.jpg\n",
      "--  379_1p11.jpg\n",
      "--  379_1p12.jpg\n",
      "--  379_1p13.jpg\n",
      "--  379_1p14.jpg\n",
      "--  379_1p15.jpg\n",
      "--  379_1p16.jpg\n",
      "--  379_1p17.jpg\n",
      "--  381_1p11.jpg\n",
      "--  381_1p12.jpg\n",
      "--  381_1p13.jpg\n",
      "--  381_1p14.jpg\n",
      "--  381_1p15.jpg\n",
      "--  381_1p16.jpg\n",
      "--  381_1p17.jpg\n",
      "--  381_2p11.jpg\n",
      "--  381_2p12.jpg\n",
      "--  381_2p13.jpg\n",
      "--  381_2p14.jpg\n",
      "--  381_2p15.jpg\n",
      "--  381_2p16.jpg\n",
      "--  381_2p17.jpg\n",
      "--  381_2p21.jpg\n",
      "--  381_2p22.jpg\n",
      "--  381_2p23.jpg\n",
      "--  381_2p24.jpg\n",
      "--  381_2p25.jpg\n",
      "--  381_2p26.jpg\n",
      "--  381_2p27.jpg\n",
      "--  383_1p11.jpg\n",
      "--  383_1p12.jpg\n",
      "--  383_1p13.jpg\n",
      "--  383_1p14.jpg\n",
      "--  383_1p15.jpg\n",
      "--  383_1p16.jpg\n",
      "--  383_1p17.jpg\n",
      "--  384_1p11.jpg\n",
      "--  384_1p12.jpg\n",
      "--  384_1p13.jpg\n",
      "--  384_1p14.jpg\n",
      "--  384_1p15.jpg\n",
      "--  384_1p16.jpg\n",
      "--  384_1p17.jpg\n",
      "--  384_1p21.jpg\n",
      "--  384_1p22.jpg\n",
      "--  384_1p23.jpg\n",
      "--  384_1p24.jpg\n",
      "--  384_1p25.jpg\n",
      "--  384_1p26.jpg\n",
      "--  384_1p27.jpg\n",
      "--  387_1p11.jpg\n",
      "--  387_1p12.jpg\n",
      "--  387_1p13.jpg\n",
      "--  387_1p14.jpg\n",
      "--  387_1p15.jpg\n",
      "--  387_1p16.jpg\n",
      "--  387_1p17.jpg\n",
      "--  387_2p11.jpg\n",
      "--  387_2p12.jpg\n",
      "--  387_2p13.jpg\n",
      "--  387_2p14.jpg\n",
      "--  387_2p15.jpg\n",
      "--  387_2p16.jpg\n",
      "--  387_2p17.jpg\n",
      "--  38_1p11.jpg\n",
      "--  38_1p12.jpg\n",
      "--  38_1p13.jpg\n",
      "--  38_1p14.jpg\n",
      "--  38_1p15.jpg\n",
      "--  38_1p16.jpg\n",
      "--  38_1p17.jpg\n",
      "--  390_1p11.jpg\n",
      "--  390_1p12.jpg\n",
      "--  390_1p13.jpg\n",
      "--  390_1p14.jpg\n",
      "--  390_1p15.jpg\n",
      "--  390_1p16.jpg\n",
      "--  390_1p17.jpg\n",
      "--  392_1p11.jpg\n",
      "--  392_1p12.jpg\n",
      "--  392_1p13.jpg\n",
      "--  392_1p14.jpg\n",
      "--  392_1p15.jpg\n",
      "--  392_1p16.jpg\n",
      "--  392_1p17.jpg\n",
      "--  394_1p11.jpg\n",
      "--  394_1p12.jpg\n",
      "--  394_1p13.jpg\n",
      "--  394_1p14.jpg\n",
      "--  394_1p15.jpg\n",
      "--  394_1p16.jpg\n",
      "--  394_1p17.jpg\n",
      "--  395_1p11.jpg\n",
      "--  395_1p12.jpg\n",
      "--  395_1p13.jpg\n",
      "--  395_1p14.jpg\n",
      "--  395_1p15.jpg\n",
      "--  395_1p16.jpg\n",
      "--  395_1p17.jpg\n",
      "--  395_2p11.jpg\n",
      "--  395_2p12.jpg\n",
      "--  395_2p13.jpg\n",
      "--  395_2p14.jpg\n",
      "--  395_2p15.jpg\n",
      "--  395_2p16.jpg\n",
      "--  395_2p17.jpg\n",
      "--  398_1p11.jpg\n",
      "--  398_1p12.jpg\n",
      "--  398_1p13.jpg\n",
      "--  398_1p14.jpg\n",
      "--  398_1p15.jpg\n",
      "--  398_1p16.jpg\n",
      "--  398_1p17.jpg\n",
      "--  399_1p11.jpg\n",
      "--  399_1p12.jpg\n",
      "--  399_1p13.jpg\n",
      "--  399_1p14.jpg\n",
      "--  399_1p15.jpg\n",
      "--  399_1p16.jpg\n",
      "--  399_1p17.jpg\n",
      "--  399_1p21.jpg\n",
      "--  399_1p22.jpg\n",
      "--  399_1p23.jpg\n",
      "--  399_1p24.jpg\n",
      "--  399_1p25.jpg\n",
      "--  399_1p26.jpg\n",
      "--  399_1p27.jpg\n",
      "--  39_1p11.jpg\n",
      "--  39_1p12.jpg\n",
      "--  39_1p13.jpg\n",
      "--  39_1p14.jpg\n",
      "--  39_1p15.jpg\n",
      "--  39_1p16.jpg\n",
      "--  39_1p17.jpg\n",
      "--  3_1p11.jpg\n",
      "--  3_1p12.jpg\n",
      "--  3_1p13.jpg\n",
      "--  3_1p14.jpg\n",
      "--  3_1p15.jpg\n",
      "--  3_1p16.jpg\n",
      "--  3_1p17.jpg\n",
      "--  400_1p11.jpg\n",
      "--  400_1p12.jpg\n",
      "--  400_1p13.jpg\n",
      "--  400_1p14.jpg\n",
      "--  400_1p15.jpg\n",
      "--  400_1p16.jpg\n",
      "--  400_1p17.jpg\n",
      "--  40_1p11.jpg\n",
      "--  40_1p12.jpg\n",
      "--  40_1p13.jpg\n",
      "--  40_1p14.jpg\n",
      "--  40_1p15.jpg\n",
      "--  40_1p16.jpg\n",
      "--  40_1p17.jpg\n",
      "--  41_1p11.jpg\n",
      "--  41_1p12.jpg\n",
      "--  41_1p13.jpg\n",
      "--  41_1p14.jpg\n",
      "--  41_1p15.jpg\n",
      "--  41_1p16.jpg\n",
      "--  41_1p17.jpg\n",
      "--  42_1p11.jpg\n",
      "--  42_1p12.jpg\n",
      "--  42_1p13.jpg\n",
      "--  42_1p14.jpg\n",
      "--  42_1p15.jpg\n",
      "--  42_1p16.jpg\n",
      "--  42_1p17.jpg\n",
      "--  43_1p11.jpg\n",
      "--  43_1p12.jpg\n",
      "--  43_1p13.jpg\n",
      "--  43_1p14.jpg\n",
      "--  43_1p15.jpg\n",
      "--  43_1p16.jpg\n",
      "--  43_1p17.jpg\n",
      "--  44_1p11.jpg\n",
      "--  44_1p12.jpg\n",
      "--  44_1p13.jpg\n",
      "--  44_1p14.jpg\n",
      "--  44_1p15.jpg\n",
      "--  44_1p16.jpg\n",
      "--  44_1p17.jpg\n",
      "--  45_1p11.jpg\n",
      "--  45_1p12.jpg\n",
      "--  45_1p13.jpg\n",
      "--  45_1p14.jpg\n",
      "--  45_1p15.jpg\n",
      "--  45_1p16.jpg\n",
      "--  45_1p17.jpg\n",
      "--  46_1.png\n",
      "--  46_1p11.jpg\n",
      "--  46_1p12.jpg\n",
      "--  46_1p13.jpg\n",
      "--  46_1p14.jpg\n",
      "--  46_1p15.jpg\n",
      "--  46_1p16.jpg\n",
      "--  46_1p17.jpg\n",
      "--  46_2.png\n",
      "--  46_3.png\n",
      "--  46_4.png\n",
      "--  46_5.png\n",
      "--  46_6.png\n",
      "--  46_7.png\n",
      "--  46_8.png\n",
      "--  46_9.png\n",
      "--  47_1.png\n",
      "--  47_1p11.jpg\n",
      "--  47_1p12.jpg\n",
      "--  47_1p13.jpg\n",
      "--  47_1p14.jpg\n",
      "--  47_1p15.jpg\n",
      "--  47_1p16.jpg\n",
      "--  47_1p17.jpg\n",
      "--  47_2.png\n",
      "--  47_3.png\n",
      "--  47_4.png\n",
      "--  47_5.png\n",
      "--  47_6.png\n",
      "--  47_7.png\n",
      "--  47_8.png\n",
      "--  47_9.png\n",
      "--  48_1p11.jpg\n",
      "--  48_1p12.jpg\n",
      "--  48_1p13.jpg\n",
      "--  48_1p14.jpg\n",
      "--  48_1p15.jpg\n",
      "--  48_1p16.jpg\n",
      "--  48_1p17.jpg\n",
      "--  49_1p11.jpg\n",
      "--  49_1p12.jpg\n",
      "--  49_1p13.jpg\n",
      "--  49_1p14.jpg\n",
      "--  49_1p15.jpg\n",
      "--  49_1p16.jpg\n",
      "--  49_1p17.jpg\n",
      "--  4_1p11.jpg\n",
      "--  4_1p12.jpg\n",
      "--  4_1p13.jpg\n",
      "--  4_1p14.jpg\n",
      "--  4_1p15.jpg\n",
      "--  4_1p16.jpg\n",
      "--  4_1p17.jpg\n",
      "--  50_1p11.jpg\n",
      "--  50_1p12.jpg\n",
      "--  50_1p13.jpg\n",
      "--  50_1p14.jpg\n",
      "--  50_1p15.jpg\n",
      "--  50_1p16.jpg\n",
      "--  50_1p17.jpg\n",
      "--  52_1p11.jpg\n",
      "--  52_1p12.jpg\n",
      "--  52_1p13.jpg\n",
      "--  52_1p14.jpg\n",
      "--  52_1p15.jpg\n",
      "--  52_1p16.jpg\n",
      "--  52_1p17.jpg\n",
      "--  53_1p11.jpg\n",
      "--  53_1p12.jpg\n",
      "--  53_1p13.jpg\n",
      "--  53_1p14.jpg\n",
      "--  53_1p15.jpg\n",
      "--  53_1p16.jpg\n",
      "--  53_1p17.jpg\n",
      "--  54_1p11.jpg\n",
      "--  54_1p12.jpg\n",
      "--  54_1p13.jpg\n",
      "--  54_1p14.jpg\n",
      "--  54_1p15.jpg\n",
      "--  54_1p16.jpg\n",
      "--  54_1p17.jpg\n",
      "--  55_1p11.jpg\n",
      "--  55_1p12.jpg\n",
      "--  55_1p13.jpg\n",
      "--  55_1p14.jpg\n",
      "--  55_1p15.jpg\n",
      "--  55_1p16.jpg\n",
      "--  55_1p17.jpg\n",
      "--  56_1p11.jpg\n",
      "--  56_1p12.jpg\n",
      "--  56_1p13.jpg\n",
      "--  56_1p14.jpg\n",
      "--  56_1p15.jpg\n",
      "--  56_1p16.jpg\n",
      "--  56_1p17.jpg\n",
      "--  57_1p11.jpg\n",
      "--  57_1p12.jpg\n",
      "--  57_1p13.jpg\n",
      "--  57_1p14.jpg\n",
      "--  57_1p15.jpg\n",
      "--  57_1p16.jpg\n",
      "--  57_1p17.jpg\n",
      "--  5_1.png\n",
      "--  5_1p11.jpg\n",
      "--  5_1p12.jpg\n",
      "--  5_1p13.jpg\n",
      "--  5_1p14.jpg\n",
      "--  5_1p15.jpg\n",
      "--  5_1p16.jpg\n",
      "--  5_1p17.jpg\n",
      "--  5_2.png\n",
      "--  5_3.png\n",
      "--  5_4.png\n",
      "--  5_5.png\n",
      "--  5_6.png\n",
      "--  5_7.png\n",
      "--  5_8.png\n",
      "--  5_9.png\n",
      "--  62_1.png\n",
      "--  62_2.png\n",
      "--  62_3.png\n",
      "--  62_4.png\n",
      "--  62_5.png\n",
      "--  62_6.png\n",
      "--  62_7.png\n",
      "--  62_8.png\n",
      "--  62_9.png\n",
      "--  65_1p11.jpg\n",
      "--  65_1p12.jpg\n",
      "--  65_1p13.jpg\n",
      "--  65_1p14.jpg\n",
      "--  65_1p15.jpg\n",
      "--  65_1p16.jpg\n",
      "--  65_1p17.jpg\n",
      "--  65_2p11.jpg\n",
      "--  65_2p12.jpg\n",
      "--  65_2p13.jpg\n",
      "--  65_2p14.jpg\n",
      "--  65_2p15.jpg\n",
      "--  65_2p16.jpg\n",
      "--  65_2p17.jpg\n",
      "--  67_1p11.jpg\n",
      "--  67_1p12.jpg\n",
      "--  67_1p13.jpg\n",
      "--  67_1p14.jpg\n",
      "--  67_1p15.jpg\n",
      "--  67_1p16.jpg\n",
      "--  67_1p17.jpg\n",
      "--  68_1p11.jpg\n",
      "--  68_1p12.jpg\n",
      "--  68_1p13.jpg\n",
      "--  68_1p14.jpg\n",
      "--  68_1p15.jpg\n",
      "--  68_1p16.jpg\n",
      "--  68_1p17.jpg\n",
      "--  68_1p21.jpg\n",
      "--  68_1p22.jpg\n",
      "--  68_1p23.jpg\n",
      "--  68_1p24.jpg\n",
      "--  68_1p25.jpg\n",
      "--  68_1p26.jpg\n",
      "--  68_1p27.jpg\n",
      "--  6_1.png\n",
      "--  6_1p11.jpg\n",
      "--  6_1p12.jpg\n",
      "--  6_1p13.jpg\n",
      "--  6_1p14.jpg\n",
      "--  6_1p15.jpg\n",
      "--  6_1p16.jpg\n",
      "--  6_1p17.jpg\n",
      "--  6_2.png\n",
      "--  6_3.png\n",
      "--  6_4.png\n",
      "--  6_5.png\n",
      "--  6_6.png\n",
      "--  6_7.png\n",
      "--  6_8.png\n",
      "--  6_9.png\n",
      "--  70_1p11.jpg\n",
      "--  70_1p12.jpg\n",
      "--  70_1p13.jpg\n",
      "--  70_1p14.jpg\n",
      "--  70_1p15.jpg\n",
      "--  70_1p16.jpg\n",
      "--  70_1p17.jpg\n",
      "--  70_1p21.jpg\n",
      "--  70_1p22.jpg\n",
      "--  70_1p23.jpg\n",
      "--  70_1p24.jpg\n",
      "--  70_1p25.jpg\n",
      "--  70_1p26.jpg\n",
      "--  70_1p27.jpg\n",
      "--  73_1p11.jpg\n",
      "--  73_1p12.jpg\n",
      "--  73_1p13.jpg\n",
      "--  73_1p14.jpg\n",
      "--  73_1p15.jpg\n",
      "--  73_1p16.jpg\n",
      "--  73_1p17.jpg\n",
      "--  74_1p11.jpg\n",
      "--  74_1p12.jpg\n",
      "--  74_1p13.jpg\n",
      "--  74_1p14.jpg\n",
      "--  74_1p15.jpg\n",
      "--  74_1p16.jpg\n",
      "--  74_1p17.jpg\n",
      "--  74_1p21.jpg\n",
      "--  74_1p22.jpg\n",
      "--  74_1p23.jpg\n",
      "--  74_1p24.jpg\n",
      "--  74_1p25.jpg\n",
      "--  74_1p26.jpg\n",
      "--  74_1p27.jpg\n",
      "--  76_1p11.jpg\n",
      "--  76_1p12.jpg\n",
      "--  76_1p13.jpg\n",
      "--  76_1p14.jpg\n",
      "--  76_1p15.jpg\n",
      "--  76_1p16.jpg\n",
      "--  76_1p17.jpg\n",
      "--  77_1.png\n",
      "--  77_2.png\n",
      "--  77_3.png\n",
      "--  77_4.png\n",
      "--  77_5.png\n",
      "--  77_6.png\n",
      "--  77_7.png\n",
      "--  77_8.png\n",
      "--  77_9.png\n",
      "--  78_1.png\n",
      "--  78_1p11.jpg\n",
      "--  78_1p12.jpg\n",
      "--  78_1p13.jpg\n",
      "--  78_1p14.jpg\n",
      "--  78_1p15.jpg\n",
      "--  78_1p16.jpg\n",
      "--  78_1p17.jpg\n",
      "--  78_1p21.jpg\n",
      "--  78_1p22.jpg\n",
      "--  78_1p23.jpg\n",
      "--  78_1p24.jpg\n",
      "--  78_1p25.jpg\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--  78_1p26.jpg\n",
      "--  78_1p27.jpg\n",
      "--  78_2.png\n",
      "--  78_3.png\n",
      "--  78_4.png\n",
      "--  78_5.png\n",
      "--  78_6.png\n",
      "--  78_7.png\n",
      "--  78_8.png\n",
      "--  78_9.png\n",
      "--  79_1p11.jpg\n",
      "--  79_1p12.jpg\n",
      "--  79_1p13.jpg\n",
      "--  79_1p14.jpg\n",
      "--  79_1p15.jpg\n",
      "--  79_1p16.jpg\n",
      "--  79_1p17.jpg\n",
      "--  79_1p21.jpg\n",
      "--  79_1p22.jpg\n",
      "--  79_1p23.jpg\n",
      "--  79_1p24.jpg\n",
      "--  79_1p25.jpg\n",
      "--  79_1p26.jpg\n",
      "--  79_1p27.jpg\n",
      "--  7_1p11.jpg\n",
      "--  7_1p12.jpg\n",
      "--  7_1p13.jpg\n",
      "--  7_1p14.jpg\n",
      "--  7_1p15.jpg\n",
      "--  7_1p16.jpg\n",
      "--  7_1p17.jpg\n",
      "--  80_1p11.jpg\n",
      "--  80_1p12.jpg\n",
      "--  80_1p13.jpg\n",
      "--  80_1p14.jpg\n",
      "--  80_1p15.jpg\n",
      "--  80_1p16.jpg\n",
      "--  80_1p17.jpg\n",
      "--  80_1p21.jpg\n",
      "--  80_1p22.jpg\n",
      "--  80_1p23.jpg\n",
      "--  80_1p24.jpg\n",
      "--  80_1p25.jpg\n",
      "--  80_1p26.jpg\n",
      "--  80_1p27.jpg\n",
      "--  80_2p11.jpg\n",
      "--  80_2p12.jpg\n",
      "--  80_2p13.jpg\n",
      "--  80_2p14.jpg\n",
      "--  80_2p15.jpg\n",
      "--  80_2p16.jpg\n",
      "--  80_2p17.jpg\n",
      "--  81_1p11.jpg\n",
      "--  81_1p12.jpg\n",
      "--  81_1p13.jpg\n",
      "--  81_1p14.jpg\n",
      "--  81_1p15.jpg\n",
      "--  81_1p16.jpg\n",
      "--  81_1p17.jpg\n",
      "--  81_2p11.jpg\n",
      "--  81_2p12.jpg\n",
      "--  81_2p13.jpg\n",
      "--  81_2p14.jpg\n",
      "--  81_2p15.jpg\n",
      "--  81_2p16.jpg\n",
      "--  81_2p17.jpg\n",
      "--  82_1p11.jpg\n",
      "--  82_1p12.jpg\n",
      "--  82_1p13.jpg\n",
      "--  82_1p14.jpg\n",
      "--  82_1p15.jpg\n",
      "--  82_1p16.jpg\n",
      "--  82_1p17.jpg\n",
      "--  82_1p21.jpg\n",
      "--  82_1p22.jpg\n",
      "--  82_1p23.jpg\n",
      "--  82_1p24.jpg\n",
      "--  82_1p25.jpg\n",
      "--  82_1p26.jpg\n",
      "--  82_1p27.jpg\n",
      "--  83_1p11.jpg\n",
      "--  83_1p12.jpg\n",
      "--  83_1p13.jpg\n",
      "--  83_1p14.jpg\n",
      "--  83_1p15.jpg\n",
      "--  83_1p16.jpg\n",
      "--  83_1p17.jpg\n",
      "--  83_1p21.jpg\n",
      "--  83_1p22.jpg\n",
      "--  83_1p23.jpg\n",
      "--  83_1p24.jpg\n",
      "--  83_1p25.jpg\n",
      "--  83_1p26.jpg\n",
      "--  83_1p27.jpg\n",
      "--  94_1.png\n",
      "--  94_2.png\n",
      "--  94_3.png\n",
      "--  94_4.png\n",
      "--  94_5.png\n",
      "--  94_6.png\n",
      "--  94_7.png\n",
      "--  94_8.png\n",
      "--  94_9.png\n"
     ]
    }
   ],
   "source": [
    "experiment_name = 'Thyroid_Combine'\n",
    "extract_features = True\n",
    "train_new_model = True\n",
    "img_folder = 'data/combined/train/'\n",
    "\n",
    "\n",
    "save_folder = os.path.join(os.getcwd(), experiment_name)\n",
    "\n",
    "if extract_features:\n",
    "    (X_train, y_train) = process_images(img_folder)\n",
    "    if not os.path.exists(save_folder):\n",
    "        os.makedirs(save_folder)\n",
    "    np.save(os.path.join(save_folder,'X_train') , X_train)\n",
    "    np.save(os.path.join(save_folder,'y_train') , y_train)\n",
    "else:\n",
    "    X_train = np.load(os.path.join(save_folder,'X_train.npy'))\n",
    "    y_train = np.load(os.path.join(save_folder,'y_train.npy'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4247 train samples\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 1)                 595       \n",
      "=================================================================\n",
      "Total params: 595\n",
      "Trainable params: 595\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 2548 samples, validate on 1699 samples\n",
      "Epoch 1/300\n",
      "2548/2548 [==============================] - 3s 1ms/step - loss: 0.7033 - binary_accuracy: 0.4847 - val_loss: 0.6407 - val_binary_accuracy: 0.7540\n",
      "Epoch 2/300\n",
      "2548/2548 [==============================] - 0s 13us/step - loss: 0.6857 - binary_accuracy: 0.6005 - val_loss: 0.7263 - val_binary_accuracy: 0.3420\n",
      "Train on 2548 samples, validate on 1699 samples\n",
      "Epoch 1/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.6731 - binary_accuracy: 0.5859 - val_loss: 0.8157 - val_binary_accuracy: 0.0736\n",
      "Epoch 2/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.6678 - binary_accuracy: 0.5757 - val_loss: 0.8977 - val_binary_accuracy: 0.0182\n",
      "Train on 2548 samples, validate on 1699 samples\n",
      "Epoch 1/300\n",
      "2548/2548 [==============================] - 0s 10us/step - loss: 0.6684 - binary_accuracy: 0.5765 - val_loss: 0.9591 - val_binary_accuracy: 0.0065\n",
      "Epoch 2/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.6717 - binary_accuracy: 0.5805 - val_loss: 0.9925 - val_binary_accuracy: 0.0047\n",
      "Train on 2548 samples, validate on 1699 samples\n",
      "Epoch 1/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.6740 - binary_accuracy: 0.5816 - val_loss: 0.9985 - val_binary_accuracy: 0.0035\n",
      "Epoch 2/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.6739 - binary_accuracy: 0.5824 - val_loss: 0.9825 - val_binary_accuracy: 0.0035\n",
      "Epoch 3/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.6714 - binary_accuracy: 0.5824 - val_loss: 0.9508 - val_binary_accuracy: 0.0041\n",
      "Epoch 4/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.6676 - binary_accuracy: 0.5820 - val_loss: 0.9098 - val_binary_accuracy: 0.0088\n",
      "Epoch 5/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.6637 - binary_accuracy: 0.5820 - val_loss: 0.8649 - val_binary_accuracy: 0.0288\n",
      "Epoch 6/300\n",
      "2548/2548 [==============================] - 0s 12us/step - loss: 0.6606 - binary_accuracy: 0.5808 - val_loss: 0.8208 - val_binary_accuracy: 0.0789\n",
      "Epoch 7/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.6588 - binary_accuracy: 0.5875 - val_loss: 0.7817 - val_binary_accuracy: 0.1701\n",
      "Epoch 8/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.6583 - binary_accuracy: 0.5883 - val_loss: 0.7507 - val_binary_accuracy: 0.2866\n",
      "Epoch 9/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.6584 - binary_accuracy: 0.5969 - val_loss: 0.7296 - val_binary_accuracy: 0.3838\n",
      "Epoch 10/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.6586 - binary_accuracy: 0.6111 - val_loss: 0.7191 - val_binary_accuracy: 0.4267\n",
      "Epoch 11/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.6583 - binary_accuracy: 0.6170 - val_loss: 0.7187 - val_binary_accuracy: 0.4279\n",
      "Epoch 12/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.6572 - binary_accuracy: 0.6185 - val_loss: 0.7270 - val_binary_accuracy: 0.4049\n",
      "Train on 2548 samples, validate on 1699 samples\n",
      "Epoch 1/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.6555 - binary_accuracy: 0.6193 - val_loss: 0.7423 - val_binary_accuracy: 0.3414\n",
      "Epoch 2/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.6536 - binary_accuracy: 0.6111 - val_loss: 0.7623 - val_binary_accuracy: 0.2702\n",
      "Train on 2548 samples, validate on 1699 samples\n",
      "Epoch 1/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.6517 - binary_accuracy: 0.5973 - val_loss: 0.7847 - val_binary_accuracy: 0.2025\n",
      "Epoch 2/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.6503 - binary_accuracy: 0.5926 - val_loss: 0.8068 - val_binary_accuracy: 0.1436\n",
      "Train on 2548 samples, validate on 1699 samples\n",
      "Epoch 1/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.6494 - binary_accuracy: 0.5903 - val_loss: 0.8260 - val_binary_accuracy: 0.1071\n",
      "Epoch 2/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.6488 - binary_accuracy: 0.5899 - val_loss: 0.8402 - val_binary_accuracy: 0.0848\n",
      "Train on 2548 samples, validate on 1699 samples\n",
      "Epoch 1/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.6484 - binary_accuracy: 0.5879 - val_loss: 0.8478 - val_binary_accuracy: 0.0800\n",
      "Epoch 2/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.6480 - binary_accuracy: 0.5903 - val_loss: 0.8484 - val_binary_accuracy: 0.0818\n",
      "Train on 2548 samples, validate on 1699 samples\n",
      "Epoch 1/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.6472 - binary_accuracy: 0.5879 - val_loss: 0.8422 - val_binary_accuracy: 0.0995\n",
      "Epoch 2/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.6462 - binary_accuracy: 0.5879 - val_loss: 0.8305 - val_binary_accuracy: 0.1254\n",
      "Epoch 3/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.6451 - binary_accuracy: 0.5867 - val_loss: 0.8150 - val_binary_accuracy: 0.1666\n",
      "Epoch 4/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.6439 - binary_accuracy: 0.5903 - val_loss: 0.7976 - val_binary_accuracy: 0.2184\n",
      "Epoch 5/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.6429 - binary_accuracy: 0.5918 - val_loss: 0.7802 - val_binary_accuracy: 0.2731\n",
      "Epoch 6/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.6421 - binary_accuracy: 0.6024 - val_loss: 0.7647 - val_binary_accuracy: 0.3243\n",
      "Epoch 7/300\n",
      "2548/2548 [==============================] - 0s 10us/step - loss: 0.6415 - binary_accuracy: 0.6064 - val_loss: 0.7525 - val_binary_accuracy: 0.3755\n",
      "Epoch 8/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.6409 - binary_accuracy: 0.6130 - val_loss: 0.7444 - val_binary_accuracy: 0.4020\n",
      "Epoch 9/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.6404 - binary_accuracy: 0.6221 - val_loss: 0.7409 - val_binary_accuracy: 0.4191\n",
      "Epoch 10/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.6398 - binary_accuracy: 0.6224 - val_loss: 0.7418 - val_binary_accuracy: 0.4197\n",
      "Train on 2548 samples, validate on 1699 samples\n",
      "Epoch 1/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.6391 - binary_accuracy: 0.6221 - val_loss: 0.7463 - val_binary_accuracy: 0.4108\n",
      "Epoch 2/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.6382 - binary_accuracy: 0.6209 - val_loss: 0.7536 - val_binary_accuracy: 0.3943\n",
      "Train on 2548 samples, validate on 1699 samples\n",
      "Epoch 1/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.6374 - binary_accuracy: 0.6185 - val_loss: 0.7624 - val_binary_accuracy: 0.3661\n",
      "Epoch 2/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.6367 - binary_accuracy: 0.6111 - val_loss: 0.7713 - val_binary_accuracy: 0.3443\n",
      "Train on 2548 samples, validate on 1699 samples\n",
      "Epoch 1/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.6361 - binary_accuracy: 0.6056 - val_loss: 0.7789 - val_binary_accuracy: 0.3267\n",
      "Epoch 2/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.6355 - binary_accuracy: 0.6032 - val_loss: 0.7844 - val_binary_accuracy: 0.3102\n",
      "Train on 2548 samples, validate on 1699 samples\n",
      "Epoch 1/300\n",
      "2548/2548 [==============================] - 0s 10us/step - loss: 0.6350 - binary_accuracy: 0.6028 - val_loss: 0.7868 - val_binary_accuracy: 0.3072\n",
      "Epoch 2/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.6345 - binary_accuracy: 0.6028 - val_loss: 0.7860 - val_binary_accuracy: 0.3167\n",
      "Epoch 3/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.6339 - binary_accuracy: 0.6016 - val_loss: 0.7823 - val_binary_accuracy: 0.3355\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.6333 - binary_accuracy: 0.6028 - val_loss: 0.7761 - val_binary_accuracy: 0.3567\n",
      "Epoch 5/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.6327 - binary_accuracy: 0.6079 - val_loss: 0.7684 - val_binary_accuracy: 0.3832\n",
      "Epoch 6/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.6321 - binary_accuracy: 0.6138 - val_loss: 0.7601 - val_binary_accuracy: 0.4161\n",
      "Epoch 7/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.6315 - binary_accuracy: 0.6197 - val_loss: 0.7524 - val_binary_accuracy: 0.4344\n",
      "Epoch 8/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.6310 - binary_accuracy: 0.6248 - val_loss: 0.7459 - val_binary_accuracy: 0.4509\n",
      "Epoch 9/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.6305 - binary_accuracy: 0.6287 - val_loss: 0.7414 - val_binary_accuracy: 0.4656\n",
      "Epoch 10/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.6300 - binary_accuracy: 0.6311 - val_loss: 0.7391 - val_binary_accuracy: 0.4750\n",
      "Epoch 11/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.6296 - binary_accuracy: 0.6311 - val_loss: 0.7389 - val_binary_accuracy: 0.4779\n",
      "Epoch 12/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.6291 - binary_accuracy: 0.6311 - val_loss: 0.7405 - val_binary_accuracy: 0.4762\n",
      "Train on 2548 samples, validate on 1699 samples\n",
      "Epoch 1/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.6285 - binary_accuracy: 0.6307 - val_loss: 0.7434 - val_binary_accuracy: 0.4703\n",
      "Epoch 2/300\n",
      "2548/2548 [==============================] - 0s 10us/step - loss: 0.6280 - binary_accuracy: 0.6299 - val_loss: 0.7470 - val_binary_accuracy: 0.4632\n",
      "Train on 2548 samples, validate on 1699 samples\n",
      "Epoch 1/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.6276 - binary_accuracy: 0.6303 - val_loss: 0.7505 - val_binary_accuracy: 0.4567\n",
      "Epoch 2/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.6271 - binary_accuracy: 0.6303 - val_loss: 0.7532 - val_binary_accuracy: 0.4503\n",
      "Train on 2548 samples, validate on 1699 samples\n",
      "Epoch 1/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.6267 - binary_accuracy: 0.6291 - val_loss: 0.7546 - val_binary_accuracy: 0.4479\n",
      "Epoch 2/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.6262 - binary_accuracy: 0.6295 - val_loss: 0.7546 - val_binary_accuracy: 0.4503\n",
      "Epoch 3/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.6258 - binary_accuracy: 0.6287 - val_loss: 0.7530 - val_binary_accuracy: 0.4567\n",
      "Epoch 4/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.6254 - binary_accuracy: 0.6283 - val_loss: 0.7501 - val_binary_accuracy: 0.4709\n",
      "Epoch 5/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.6249 - binary_accuracy: 0.6295 - val_loss: 0.7464 - val_binary_accuracy: 0.4785\n",
      "Epoch 6/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.6245 - binary_accuracy: 0.6330 - val_loss: 0.7422 - val_binary_accuracy: 0.4903\n",
      "Epoch 7/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.6241 - binary_accuracy: 0.6354 - val_loss: 0.7381 - val_binary_accuracy: 0.5032\n",
      "Epoch 8/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.6236 - binary_accuracy: 0.6358 - val_loss: 0.7346 - val_binary_accuracy: 0.5150\n",
      "Epoch 9/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.6232 - binary_accuracy: 0.6370 - val_loss: 0.7321 - val_binary_accuracy: 0.5191\n",
      "Epoch 10/300\n",
      "2548/2548 [==============================] - 0s 13us/step - loss: 0.6229 - binary_accuracy: 0.6358 - val_loss: 0.7306 - val_binary_accuracy: 0.5232\n",
      "Epoch 11/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.6225 - binary_accuracy: 0.6378 - val_loss: 0.7302 - val_binary_accuracy: 0.5238\n",
      "Epoch 12/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.6221 - binary_accuracy: 0.6374 - val_loss: 0.7306 - val_binary_accuracy: 0.5232\n",
      "Train on 2548 samples, validate on 1699 samples\n",
      "Epoch 1/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.6217 - binary_accuracy: 0.6374 - val_loss: 0.7317 - val_binary_accuracy: 0.5238\n",
      "Epoch 2/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.6213 - binary_accuracy: 0.6374 - val_loss: 0.7329 - val_binary_accuracy: 0.5232\n",
      "Train on 2548 samples, validate on 1699 samples\n",
      "Epoch 1/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.6209 - binary_accuracy: 0.6370 - val_loss: 0.7341 - val_binary_accuracy: 0.5221\n",
      "Epoch 2/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.6205 - binary_accuracy: 0.6362 - val_loss: 0.7348 - val_binary_accuracy: 0.5221\n",
      "Train on 2548 samples, validate on 1699 samples\n",
      "Epoch 1/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.6202 - binary_accuracy: 0.6350 - val_loss: 0.7348 - val_binary_accuracy: 0.5227\n",
      "Epoch 2/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.6198 - binary_accuracy: 0.6354 - val_loss: 0.7340 - val_binary_accuracy: 0.5250\n",
      "Epoch 3/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.6195 - binary_accuracy: 0.6378 - val_loss: 0.7326 - val_binary_accuracy: 0.5274\n",
      "Epoch 4/300\n",
      "2548/2548 [==============================] - 0s 13us/step - loss: 0.6191 - binary_accuracy: 0.6385 - val_loss: 0.7306 - val_binary_accuracy: 0.5315\n",
      "Epoch 5/300\n",
      "2548/2548 [==============================] - 0s 12us/step - loss: 0.6187 - binary_accuracy: 0.6393 - val_loss: 0.7283 - val_binary_accuracy: 0.5368\n",
      "Epoch 6/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.6184 - binary_accuracy: 0.6405 - val_loss: 0.7260 - val_binary_accuracy: 0.5427\n",
      "Epoch 7/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.6180 - binary_accuracy: 0.6413 - val_loss: 0.7239 - val_binary_accuracy: 0.5468\n",
      "Epoch 8/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.6177 - binary_accuracy: 0.6425 - val_loss: 0.7222 - val_binary_accuracy: 0.5509\n",
      "Epoch 9/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.6174 - binary_accuracy: 0.6425 - val_loss: 0.7210 - val_binary_accuracy: 0.5527\n",
      "Epoch 10/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.6170 - binary_accuracy: 0.6429 - val_loss: 0.7204 - val_binary_accuracy: 0.5539\n",
      "Epoch 11/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.6167 - binary_accuracy: 0.6429 - val_loss: 0.7202 - val_binary_accuracy: 0.5544\n",
      "Epoch 12/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.6164 - binary_accuracy: 0.6432 - val_loss: 0.7204 - val_binary_accuracy: 0.5550\n",
      "Train on 2548 samples, validate on 1699 samples\n",
      "Epoch 1/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.6161 - binary_accuracy: 0.6432 - val_loss: 0.7207 - val_binary_accuracy: 0.5550\n",
      "Epoch 2/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.6157 - binary_accuracy: 0.6440 - val_loss: 0.7209 - val_binary_accuracy: 0.5550\n",
      "Train on 2548 samples, validate on 1699 samples\n",
      "Epoch 1/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.6154 - binary_accuracy: 0.6444 - val_loss: 0.7208 - val_binary_accuracy: 0.5550\n",
      "Epoch 2/300\n",
      "2548/2548 [==============================] - 0s 12us/step - loss: 0.6151 - binary_accuracy: 0.6448 - val_loss: 0.7205 - val_binary_accuracy: 0.5562\n",
      "Epoch 3/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.6148 - binary_accuracy: 0.6448 - val_loss: 0.7197 - val_binary_accuracy: 0.5568\n",
      "Epoch 4/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.6145 - binary_accuracy: 0.6448 - val_loss: 0.7186 - val_binary_accuracy: 0.5586\n",
      "Epoch 5/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.6142 - binary_accuracy: 0.6444 - val_loss: 0.7173 - val_binary_accuracy: 0.5592\n",
      "Epoch 6/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.6139 - binary_accuracy: 0.6448 - val_loss: 0.7159 - val_binary_accuracy: 0.5603\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.6136 - binary_accuracy: 0.6456 - val_loss: 0.7145 - val_binary_accuracy: 0.5633\n",
      "Epoch 8/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.6133 - binary_accuracy: 0.6444 - val_loss: 0.7132 - val_binary_accuracy: 0.5656\n",
      "Epoch 9/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.6130 - binary_accuracy: 0.6460 - val_loss: 0.7122 - val_binary_accuracy: 0.5668\n",
      "Epoch 10/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.6127 - binary_accuracy: 0.6460 - val_loss: 0.7115 - val_binary_accuracy: 0.5674\n",
      "Epoch 11/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.6125 - binary_accuracy: 0.6472 - val_loss: 0.7110 - val_binary_accuracy: 0.5686\n",
      "Epoch 12/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.6122 - binary_accuracy: 0.6468 - val_loss: 0.7107 - val_binary_accuracy: 0.5686\n",
      "Epoch 13/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.6119 - binary_accuracy: 0.6468 - val_loss: 0.7105 - val_binary_accuracy: 0.5692\n",
      "Epoch 14/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.6116 - binary_accuracy: 0.6468 - val_loss: 0.7104 - val_binary_accuracy: 0.5697\n",
      "Epoch 15/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.6114 - binary_accuracy: 0.6464 - val_loss: 0.7101 - val_binary_accuracy: 0.5697\n",
      "Epoch 16/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.6111 - binary_accuracy: 0.6464 - val_loss: 0.7097 - val_binary_accuracy: 0.5709\n",
      "Epoch 17/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.6108 - binary_accuracy: 0.6468 - val_loss: 0.7092 - val_binary_accuracy: 0.5721\n",
      "Epoch 18/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.6106 - binary_accuracy: 0.6464 - val_loss: 0.7084 - val_binary_accuracy: 0.5750\n",
      "Epoch 19/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.6103 - binary_accuracy: 0.6484 - val_loss: 0.7075 - val_binary_accuracy: 0.5762\n",
      "Epoch 20/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.6100 - binary_accuracy: 0.6484 - val_loss: 0.7066 - val_binary_accuracy: 0.5792\n",
      "Epoch 21/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.6098 - binary_accuracy: 0.6476 - val_loss: 0.7056 - val_binary_accuracy: 0.5821\n",
      "Epoch 22/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.6095 - binary_accuracy: 0.6476 - val_loss: 0.7047 - val_binary_accuracy: 0.5845\n",
      "Epoch 23/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.6093 - binary_accuracy: 0.6476 - val_loss: 0.7040 - val_binary_accuracy: 0.5851\n",
      "Epoch 24/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.6090 - binary_accuracy: 0.6468 - val_loss: 0.7033 - val_binary_accuracy: 0.5880\n",
      "Epoch 25/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.6088 - binary_accuracy: 0.6476 - val_loss: 0.7029 - val_binary_accuracy: 0.5886\n",
      "Epoch 26/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.6085 - binary_accuracy: 0.6484 - val_loss: 0.7025 - val_binary_accuracy: 0.5898\n",
      "Epoch 27/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.6083 - binary_accuracy: 0.6487 - val_loss: 0.7022 - val_binary_accuracy: 0.5915\n",
      "Epoch 28/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.6081 - binary_accuracy: 0.6487 - val_loss: 0.7019 - val_binary_accuracy: 0.5939\n",
      "Epoch 29/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.6078 - binary_accuracy: 0.6487 - val_loss: 0.7015 - val_binary_accuracy: 0.5956\n",
      "Epoch 30/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.6076 - binary_accuracy: 0.6491 - val_loss: 0.7011 - val_binary_accuracy: 0.5974\n",
      "Epoch 31/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.6073 - binary_accuracy: 0.6499 - val_loss: 0.7005 - val_binary_accuracy: 0.5986\n",
      "Epoch 32/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.6071 - binary_accuracy: 0.6499 - val_loss: 0.6999 - val_binary_accuracy: 0.6004\n",
      "Epoch 33/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.6069 - binary_accuracy: 0.6499 - val_loss: 0.6993 - val_binary_accuracy: 0.6009\n",
      "Epoch 34/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.6067 - binary_accuracy: 0.6499 - val_loss: 0.6986 - val_binary_accuracy: 0.6027\n",
      "Epoch 35/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.6064 - binary_accuracy: 0.6503 - val_loss: 0.6979 - val_binary_accuracy: 0.6045\n",
      "Epoch 36/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.6062 - binary_accuracy: 0.6511 - val_loss: 0.6973 - val_binary_accuracy: 0.6051\n",
      "Epoch 37/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.6060 - binary_accuracy: 0.6511 - val_loss: 0.6967 - val_binary_accuracy: 0.6057\n",
      "Epoch 38/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.6058 - binary_accuracy: 0.6519 - val_loss: 0.6962 - val_binary_accuracy: 0.6074\n",
      "Epoch 39/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.6056 - binary_accuracy: 0.6523 - val_loss: 0.6958 - val_binary_accuracy: 0.6086\n",
      "Epoch 40/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.6053 - binary_accuracy: 0.6523 - val_loss: 0.6954 - val_binary_accuracy: 0.6092\n",
      "Epoch 41/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.6051 - binary_accuracy: 0.6523 - val_loss: 0.6951 - val_binary_accuracy: 0.6104\n",
      "Epoch 42/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.6049 - binary_accuracy: 0.6531 - val_loss: 0.6947 - val_binary_accuracy: 0.6109\n",
      "Epoch 43/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.6047 - binary_accuracy: 0.6535 - val_loss: 0.6943 - val_binary_accuracy: 0.6109\n",
      "Epoch 44/300\n",
      "2548/2548 [==============================] - 0s 12us/step - loss: 0.6045 - binary_accuracy: 0.6542 - val_loss: 0.6939 - val_binary_accuracy: 0.6109\n",
      "Epoch 45/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.6043 - binary_accuracy: 0.6542 - val_loss: 0.6935 - val_binary_accuracy: 0.6109\n",
      "Epoch 46/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.6041 - binary_accuracy: 0.6542 - val_loss: 0.6930 - val_binary_accuracy: 0.6115\n",
      "Epoch 47/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.6039 - binary_accuracy: 0.6546 - val_loss: 0.6924 - val_binary_accuracy: 0.6133\n",
      "Epoch 48/300\n",
      "2548/2548 [==============================] - 0s 12us/step - loss: 0.6037 - binary_accuracy: 0.6550 - val_loss: 0.6919 - val_binary_accuracy: 0.6139\n",
      "Epoch 49/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.6035 - binary_accuracy: 0.6558 - val_loss: 0.6914 - val_binary_accuracy: 0.6139\n",
      "Epoch 50/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.6033 - binary_accuracy: 0.6558 - val_loss: 0.6909 - val_binary_accuracy: 0.6145\n",
      "Epoch 51/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.6031 - binary_accuracy: 0.6558 - val_loss: 0.6905 - val_binary_accuracy: 0.6145\n",
      "Epoch 52/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.6029 - binary_accuracy: 0.6562 - val_loss: 0.6901 - val_binary_accuracy: 0.6151\n",
      "Epoch 53/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.6027 - binary_accuracy: 0.6558 - val_loss: 0.6898 - val_binary_accuracy: 0.6157\n",
      "Epoch 54/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.6025 - binary_accuracy: 0.6558 - val_loss: 0.6894 - val_binary_accuracy: 0.6162\n",
      "Epoch 55/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.6023 - binary_accuracy: 0.6558 - val_loss: 0.6891 - val_binary_accuracy: 0.6168\n",
      "Epoch 56/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.6021 - binary_accuracy: 0.6562 - val_loss: 0.6888 - val_binary_accuracy: 0.6168\n",
      "Epoch 57/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.6020 - binary_accuracy: 0.6562 - val_loss: 0.6884 - val_binary_accuracy: 0.6180\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 58/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.6018 - binary_accuracy: 0.6566 - val_loss: 0.6880 - val_binary_accuracy: 0.6186\n",
      "Epoch 59/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.6016 - binary_accuracy: 0.6566 - val_loss: 0.6876 - val_binary_accuracy: 0.6192\n",
      "Epoch 60/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.6014 - binary_accuracy: 0.6566 - val_loss: 0.6872 - val_binary_accuracy: 0.6192\n",
      "Epoch 61/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.6012 - binary_accuracy: 0.6574 - val_loss: 0.6868 - val_binary_accuracy: 0.6198\n",
      "Epoch 62/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.6011 - binary_accuracy: 0.6574 - val_loss: 0.6864 - val_binary_accuracy: 0.6204\n",
      "Epoch 63/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.6009 - binary_accuracy: 0.6582 - val_loss: 0.6860 - val_binary_accuracy: 0.6210\n",
      "Epoch 64/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.6007 - binary_accuracy: 0.6574 - val_loss: 0.6857 - val_binary_accuracy: 0.6210\n",
      "Epoch 65/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.6005 - binary_accuracy: 0.6574 - val_loss: 0.6853 - val_binary_accuracy: 0.6215\n",
      "Epoch 66/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.6003 - binary_accuracy: 0.6574 - val_loss: 0.6850 - val_binary_accuracy: 0.6221\n",
      "Epoch 67/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.6002 - binary_accuracy: 0.6574 - val_loss: 0.6847 - val_binary_accuracy: 0.6221\n",
      "Epoch 68/300\n",
      "2548/2548 [==============================] - 0s 12us/step - loss: 0.6000 - binary_accuracy: 0.6578 - val_loss: 0.6844 - val_binary_accuracy: 0.6221\n",
      "Epoch 69/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5998 - binary_accuracy: 0.6578 - val_loss: 0.6841 - val_binary_accuracy: 0.6227\n",
      "Epoch 70/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5997 - binary_accuracy: 0.6578 - val_loss: 0.6838 - val_binary_accuracy: 0.6227\n",
      "Epoch 71/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5995 - binary_accuracy: 0.6582 - val_loss: 0.6835 - val_binary_accuracy: 0.6227\n",
      "Epoch 72/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5993 - binary_accuracy: 0.6582 - val_loss: 0.6832 - val_binary_accuracy: 0.6233\n",
      "Epoch 73/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5992 - binary_accuracy: 0.6586 - val_loss: 0.6828 - val_binary_accuracy: 0.6245\n",
      "Epoch 74/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5990 - binary_accuracy: 0.6586 - val_loss: 0.6825 - val_binary_accuracy: 0.6257\n",
      "Epoch 75/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5988 - binary_accuracy: 0.6582 - val_loss: 0.6822 - val_binary_accuracy: 0.6268\n",
      "Epoch 76/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5987 - binary_accuracy: 0.6589 - val_loss: 0.6819 - val_binary_accuracy: 0.6274\n",
      "Epoch 77/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5985 - binary_accuracy: 0.6589 - val_loss: 0.6816 - val_binary_accuracy: 0.6274\n",
      "Epoch 78/300\n",
      "2548/2548 [==============================] - 0s 12us/step - loss: 0.5984 - binary_accuracy: 0.6589 - val_loss: 0.6813 - val_binary_accuracy: 0.6280\n",
      "Epoch 79/300\n",
      "2548/2548 [==============================] - 0s 14us/step - loss: 0.5982 - binary_accuracy: 0.6593 - val_loss: 0.6810 - val_binary_accuracy: 0.6298\n",
      "Epoch 80/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5980 - binary_accuracy: 0.6586 - val_loss: 0.6808 - val_binary_accuracy: 0.6315\n",
      "Epoch 81/300\n",
      "2548/2548 [==============================] - 0s 12us/step - loss: 0.5979 - binary_accuracy: 0.6589 - val_loss: 0.6805 - val_binary_accuracy: 0.6315\n",
      "Epoch 82/300\n",
      "2548/2548 [==============================] - 0s 12us/step - loss: 0.5977 - binary_accuracy: 0.6601 - val_loss: 0.6803 - val_binary_accuracy: 0.6327\n",
      "Epoch 83/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5976 - binary_accuracy: 0.6597 - val_loss: 0.6800 - val_binary_accuracy: 0.6327\n",
      "Epoch 84/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5974 - binary_accuracy: 0.6597 - val_loss: 0.6797 - val_binary_accuracy: 0.6351\n",
      "Epoch 85/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5973 - binary_accuracy: 0.6605 - val_loss: 0.6795 - val_binary_accuracy: 0.6345\n",
      "Epoch 86/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5971 - binary_accuracy: 0.6609 - val_loss: 0.6792 - val_binary_accuracy: 0.6345\n",
      "Epoch 87/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5970 - binary_accuracy: 0.6613 - val_loss: 0.6789 - val_binary_accuracy: 0.6345\n",
      "Epoch 88/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5968 - binary_accuracy: 0.6613 - val_loss: 0.6787 - val_binary_accuracy: 0.6357\n",
      "Epoch 89/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5967 - binary_accuracy: 0.6621 - val_loss: 0.6784 - val_binary_accuracy: 0.6357\n",
      "Epoch 90/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5965 - binary_accuracy: 0.6621 - val_loss: 0.6782 - val_binary_accuracy: 0.6363\n",
      "Epoch 91/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5964 - binary_accuracy: 0.6621 - val_loss: 0.6780 - val_binary_accuracy: 0.6363\n",
      "Epoch 92/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5962 - binary_accuracy: 0.6621 - val_loss: 0.6778 - val_binary_accuracy: 0.6363\n",
      "Epoch 93/300\n",
      "2548/2548 [==============================] - 0s 12us/step - loss: 0.5961 - binary_accuracy: 0.6625 - val_loss: 0.6775 - val_binary_accuracy: 0.6368\n",
      "Epoch 94/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5959 - binary_accuracy: 0.6625 - val_loss: 0.6773 - val_binary_accuracy: 0.6374\n",
      "Epoch 95/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5958 - binary_accuracy: 0.6629 - val_loss: 0.6771 - val_binary_accuracy: 0.6380\n",
      "Epoch 96/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5957 - binary_accuracy: 0.6633 - val_loss: 0.6769 - val_binary_accuracy: 0.6380\n",
      "Epoch 97/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5955 - binary_accuracy: 0.6633 - val_loss: 0.6767 - val_binary_accuracy: 0.6380\n",
      "Epoch 98/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5954 - binary_accuracy: 0.6629 - val_loss: 0.6765 - val_binary_accuracy: 0.6386\n",
      "Epoch 99/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5952 - binary_accuracy: 0.6633 - val_loss: 0.6763 - val_binary_accuracy: 0.6398\n",
      "Epoch 100/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5951 - binary_accuracy: 0.6633 - val_loss: 0.6760 - val_binary_accuracy: 0.6398\n",
      "Epoch 101/300\n",
      "2548/2548 [==============================] - 0s 15us/step - loss: 0.5950 - binary_accuracy: 0.6633 - val_loss: 0.6758 - val_binary_accuracy: 0.6398\n",
      "Epoch 102/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5948 - binary_accuracy: 0.6633 - val_loss: 0.6756 - val_binary_accuracy: 0.6398\n",
      "Epoch 103/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5947 - binary_accuracy: 0.6629 - val_loss: 0.6755 - val_binary_accuracy: 0.6410\n",
      "Epoch 104/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5945 - binary_accuracy: 0.6629 - val_loss: 0.6753 - val_binary_accuracy: 0.6410\n",
      "Epoch 105/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5944 - binary_accuracy: 0.6637 - val_loss: 0.6751 - val_binary_accuracy: 0.6416\n",
      "Epoch 106/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5943 - binary_accuracy: 0.6641 - val_loss: 0.6749 - val_binary_accuracy: 0.6416\n",
      "Epoch 107/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5941 - binary_accuracy: 0.6637 - val_loss: 0.6747 - val_binary_accuracy: 0.6433\n",
      "Epoch 108/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5940 - binary_accuracy: 0.6637 - val_loss: 0.6746 - val_binary_accuracy: 0.6457\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 109/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5939 - binary_accuracy: 0.6641 - val_loss: 0.6744 - val_binary_accuracy: 0.6457\n",
      "Epoch 110/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5937 - binary_accuracy: 0.6641 - val_loss: 0.6742 - val_binary_accuracy: 0.6457\n",
      "Epoch 111/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5936 - binary_accuracy: 0.6644 - val_loss: 0.6740 - val_binary_accuracy: 0.6463\n",
      "Epoch 112/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5935 - binary_accuracy: 0.6648 - val_loss: 0.6739 - val_binary_accuracy: 0.6463\n",
      "Epoch 113/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5933 - binary_accuracy: 0.6648 - val_loss: 0.6737 - val_binary_accuracy: 0.6457\n",
      "Epoch 114/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5932 - binary_accuracy: 0.6648 - val_loss: 0.6735 - val_binary_accuracy: 0.6463\n",
      "Epoch 115/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5931 - binary_accuracy: 0.6648 - val_loss: 0.6734 - val_binary_accuracy: 0.6463\n",
      "Epoch 116/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5930 - binary_accuracy: 0.6648 - val_loss: 0.6732 - val_binary_accuracy: 0.6469\n",
      "Epoch 117/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5928 - binary_accuracy: 0.6656 - val_loss: 0.6731 - val_binary_accuracy: 0.6474\n",
      "Epoch 118/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5927 - binary_accuracy: 0.6656 - val_loss: 0.6729 - val_binary_accuracy: 0.6480\n",
      "Epoch 119/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5926 - binary_accuracy: 0.6656 - val_loss: 0.6728 - val_binary_accuracy: 0.6486\n",
      "Epoch 120/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5924 - binary_accuracy: 0.6660 - val_loss: 0.6726 - val_binary_accuracy: 0.6486\n",
      "Epoch 121/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5923 - binary_accuracy: 0.6656 - val_loss: 0.6725 - val_binary_accuracy: 0.6492\n",
      "Epoch 122/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5922 - binary_accuracy: 0.6656 - val_loss: 0.6724 - val_binary_accuracy: 0.6498\n",
      "Epoch 123/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5921 - binary_accuracy: 0.6656 - val_loss: 0.6722 - val_binary_accuracy: 0.6510\n",
      "Epoch 124/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5919 - binary_accuracy: 0.6664 - val_loss: 0.6721 - val_binary_accuracy: 0.6510\n",
      "Epoch 125/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5918 - binary_accuracy: 0.6668 - val_loss: 0.6720 - val_binary_accuracy: 0.6510\n",
      "Epoch 126/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5917 - binary_accuracy: 0.6668 - val_loss: 0.6718 - val_binary_accuracy: 0.6521\n",
      "Epoch 127/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5916 - binary_accuracy: 0.6664 - val_loss: 0.6717 - val_binary_accuracy: 0.6539\n",
      "Epoch 128/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5914 - binary_accuracy: 0.6676 - val_loss: 0.6716 - val_binary_accuracy: 0.6545\n",
      "Epoch 129/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5913 - binary_accuracy: 0.6676 - val_loss: 0.6715 - val_binary_accuracy: 0.6545\n",
      "Epoch 130/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5912 - binary_accuracy: 0.6680 - val_loss: 0.6714 - val_binary_accuracy: 0.6545\n",
      "Epoch 131/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5911 - binary_accuracy: 0.6680 - val_loss: 0.6712 - val_binary_accuracy: 0.6551\n",
      "Epoch 132/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5910 - binary_accuracy: 0.6684 - val_loss: 0.6711 - val_binary_accuracy: 0.6551\n",
      "Epoch 133/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5908 - binary_accuracy: 0.6684 - val_loss: 0.6710 - val_binary_accuracy: 0.6551\n",
      "Epoch 134/300\n",
      "2548/2548 [==============================] - 0s 14us/step - loss: 0.5907 - binary_accuracy: 0.6692 - val_loss: 0.6709 - val_binary_accuracy: 0.6551\n",
      "Epoch 135/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5906 - binary_accuracy: 0.6692 - val_loss: 0.6708 - val_binary_accuracy: 0.6551\n",
      "Epoch 136/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5905 - binary_accuracy: 0.6695 - val_loss: 0.6707 - val_binary_accuracy: 0.6551\n",
      "Epoch 137/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5904 - binary_accuracy: 0.6695 - val_loss: 0.6706 - val_binary_accuracy: 0.6551\n",
      "Epoch 138/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5903 - binary_accuracy: 0.6695 - val_loss: 0.6705 - val_binary_accuracy: 0.6557\n",
      "Epoch 139/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5901 - binary_accuracy: 0.6699 - val_loss: 0.6704 - val_binary_accuracy: 0.6563\n",
      "Epoch 140/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5900 - binary_accuracy: 0.6699 - val_loss: 0.6703 - val_binary_accuracy: 0.6563\n",
      "Epoch 141/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5899 - binary_accuracy: 0.6699 - val_loss: 0.6702 - val_binary_accuracy: 0.6563\n",
      "Epoch 142/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5898 - binary_accuracy: 0.6703 - val_loss: 0.6701 - val_binary_accuracy: 0.6569\n",
      "Epoch 143/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5897 - binary_accuracy: 0.6707 - val_loss: 0.6700 - val_binary_accuracy: 0.6569\n",
      "Epoch 144/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5896 - binary_accuracy: 0.6707 - val_loss: 0.6699 - val_binary_accuracy: 0.6569\n",
      "Epoch 145/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5894 - binary_accuracy: 0.6707 - val_loss: 0.6698 - val_binary_accuracy: 0.6569\n",
      "Epoch 146/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5893 - binary_accuracy: 0.6707 - val_loss: 0.6698 - val_binary_accuracy: 0.6569\n",
      "Epoch 147/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5892 - binary_accuracy: 0.6707 - val_loss: 0.6697 - val_binary_accuracy: 0.6569\n",
      "Epoch 148/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5891 - binary_accuracy: 0.6711 - val_loss: 0.6696 - val_binary_accuracy: 0.6569\n",
      "Epoch 149/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5890 - binary_accuracy: 0.6711 - val_loss: 0.6695 - val_binary_accuracy: 0.6569\n",
      "Epoch 150/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5889 - binary_accuracy: 0.6711 - val_loss: 0.6694 - val_binary_accuracy: 0.6592\n",
      "Epoch 151/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5888 - binary_accuracy: 0.6711 - val_loss: 0.6694 - val_binary_accuracy: 0.6592\n",
      "Epoch 152/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5887 - binary_accuracy: 0.6711 - val_loss: 0.6693 - val_binary_accuracy: 0.6592\n",
      "Epoch 153/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5885 - binary_accuracy: 0.6711 - val_loss: 0.6692 - val_binary_accuracy: 0.6598\n",
      "Epoch 154/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5884 - binary_accuracy: 0.6711 - val_loss: 0.6691 - val_binary_accuracy: 0.6598\n",
      "Epoch 155/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5883 - binary_accuracy: 0.6715 - val_loss: 0.6691 - val_binary_accuracy: 0.6598\n",
      "Epoch 156/300\n",
      "2548/2548 [==============================] - 0s 12us/step - loss: 0.5882 - binary_accuracy: 0.6715 - val_loss: 0.6690 - val_binary_accuracy: 0.6592\n",
      "Epoch 157/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5881 - binary_accuracy: 0.6715 - val_loss: 0.6689 - val_binary_accuracy: 0.6598\n",
      "Epoch 158/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5880 - binary_accuracy: 0.6723 - val_loss: 0.6689 - val_binary_accuracy: 0.6592\n",
      "Epoch 159/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5879 - binary_accuracy: 0.6719 - val_loss: 0.6688 - val_binary_accuracy: 0.6592\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 160/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5878 - binary_accuracy: 0.6723 - val_loss: 0.6688 - val_binary_accuracy: 0.6598\n",
      "Epoch 161/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5877 - binary_accuracy: 0.6719 - val_loss: 0.6687 - val_binary_accuracy: 0.6610\n",
      "Epoch 162/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5876 - binary_accuracy: 0.6719 - val_loss: 0.6687 - val_binary_accuracy: 0.6610\n",
      "Epoch 163/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5874 - binary_accuracy: 0.6719 - val_loss: 0.6686 - val_binary_accuracy: 0.6610\n",
      "Epoch 164/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5873 - binary_accuracy: 0.6715 - val_loss: 0.6685 - val_binary_accuracy: 0.6610\n",
      "Epoch 165/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5872 - binary_accuracy: 0.6711 - val_loss: 0.6685 - val_binary_accuracy: 0.6604\n",
      "Epoch 166/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5871 - binary_accuracy: 0.6715 - val_loss: 0.6684 - val_binary_accuracy: 0.6604\n",
      "Epoch 167/300\n",
      "2548/2548 [==============================] - 0s 12us/step - loss: 0.5870 - binary_accuracy: 0.6715 - val_loss: 0.6684 - val_binary_accuracy: 0.6598\n",
      "Epoch 168/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5869 - binary_accuracy: 0.6715 - val_loss: 0.6683 - val_binary_accuracy: 0.6598\n",
      "Epoch 169/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5868 - binary_accuracy: 0.6715 - val_loss: 0.6683 - val_binary_accuracy: 0.6604\n",
      "Epoch 170/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5867 - binary_accuracy: 0.6719 - val_loss: 0.6683 - val_binary_accuracy: 0.6604\n",
      "Epoch 171/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5866 - binary_accuracy: 0.6727 - val_loss: 0.6682 - val_binary_accuracy: 0.6604\n",
      "Epoch 172/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5865 - binary_accuracy: 0.6731 - val_loss: 0.6682 - val_binary_accuracy: 0.6598\n",
      "Epoch 173/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5864 - binary_accuracy: 0.6731 - val_loss: 0.6681 - val_binary_accuracy: 0.6598\n",
      "Epoch 174/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5863 - binary_accuracy: 0.6731 - val_loss: 0.6681 - val_binary_accuracy: 0.6598\n",
      "Epoch 175/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5862 - binary_accuracy: 0.6735 - val_loss: 0.6681 - val_binary_accuracy: 0.6598\n",
      "Epoch 176/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5861 - binary_accuracy: 0.6735 - val_loss: 0.6680 - val_binary_accuracy: 0.6598\n",
      "Epoch 177/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5860 - binary_accuracy: 0.6739 - val_loss: 0.6680 - val_binary_accuracy: 0.6598\n",
      "Epoch 178/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5859 - binary_accuracy: 0.6739 - val_loss: 0.6679 - val_binary_accuracy: 0.6592\n",
      "Epoch 179/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5858 - binary_accuracy: 0.6739 - val_loss: 0.6679 - val_binary_accuracy: 0.6592\n",
      "Epoch 180/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5857 - binary_accuracy: 0.6739 - val_loss: 0.6679 - val_binary_accuracy: 0.6592\n",
      "Epoch 181/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5856 - binary_accuracy: 0.6739 - val_loss: 0.6679 - val_binary_accuracy: 0.6592\n",
      "Epoch 182/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5855 - binary_accuracy: 0.6743 - val_loss: 0.6678 - val_binary_accuracy: 0.6598\n",
      "Epoch 183/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5854 - binary_accuracy: 0.6743 - val_loss: 0.6678 - val_binary_accuracy: 0.6598\n",
      "Epoch 184/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5853 - binary_accuracy: 0.6746 - val_loss: 0.6678 - val_binary_accuracy: 0.6604\n",
      "Epoch 185/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5852 - binary_accuracy: 0.6746 - val_loss: 0.6677 - val_binary_accuracy: 0.6604\n",
      "Epoch 186/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5851 - binary_accuracy: 0.6746 - val_loss: 0.6677 - val_binary_accuracy: 0.6616\n",
      "Epoch 187/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5850 - binary_accuracy: 0.6754 - val_loss: 0.6677 - val_binary_accuracy: 0.6610\n",
      "Epoch 188/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5849 - binary_accuracy: 0.6754 - val_loss: 0.6677 - val_binary_accuracy: 0.6610\n",
      "Epoch 189/300\n",
      "2548/2548 [==============================] - 0s 12us/step - loss: 0.5848 - binary_accuracy: 0.6754 - val_loss: 0.6677 - val_binary_accuracy: 0.6610\n",
      "Epoch 190/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5847 - binary_accuracy: 0.6758 - val_loss: 0.6676 - val_binary_accuracy: 0.6610\n",
      "Epoch 191/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5846 - binary_accuracy: 0.6754 - val_loss: 0.6676 - val_binary_accuracy: 0.6610\n",
      "Epoch 192/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5845 - binary_accuracy: 0.6754 - val_loss: 0.6676 - val_binary_accuracy: 0.6616\n",
      "Epoch 193/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5844 - binary_accuracy: 0.6758 - val_loss: 0.6676 - val_binary_accuracy: 0.6622\n",
      "Epoch 194/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5843 - binary_accuracy: 0.6754 - val_loss: 0.6676 - val_binary_accuracy: 0.6622\n",
      "Epoch 195/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5842 - binary_accuracy: 0.6758 - val_loss: 0.6676 - val_binary_accuracy: 0.6622\n",
      "Epoch 196/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5841 - binary_accuracy: 0.6758 - val_loss: 0.6675 - val_binary_accuracy: 0.6616\n",
      "Epoch 197/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5840 - binary_accuracy: 0.6758 - val_loss: 0.6675 - val_binary_accuracy: 0.6610\n",
      "Epoch 198/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5839 - binary_accuracy: 0.6758 - val_loss: 0.6675 - val_binary_accuracy: 0.6610\n",
      "Epoch 199/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5838 - binary_accuracy: 0.6758 - val_loss: 0.6675 - val_binary_accuracy: 0.6616\n",
      "Epoch 200/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5837 - binary_accuracy: 0.6758 - val_loss: 0.6675 - val_binary_accuracy: 0.6616\n",
      "Epoch 201/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5836 - binary_accuracy: 0.6754 - val_loss: 0.6675 - val_binary_accuracy: 0.6616\n",
      "Epoch 202/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5835 - binary_accuracy: 0.6754 - val_loss: 0.6675 - val_binary_accuracy: 0.6622\n",
      "Epoch 203/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5834 - binary_accuracy: 0.6754 - val_loss: 0.6675 - val_binary_accuracy: 0.6622\n",
      "Epoch 204/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5833 - binary_accuracy: 0.6762 - val_loss: 0.6675 - val_binary_accuracy: 0.6627\n",
      "Epoch 205/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5832 - binary_accuracy: 0.6770 - val_loss: 0.6675 - val_binary_accuracy: 0.6627\n",
      "Epoch 206/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5831 - binary_accuracy: 0.6770 - val_loss: 0.6675 - val_binary_accuracy: 0.6627\n",
      "Epoch 207/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5830 - binary_accuracy: 0.6774 - val_loss: 0.6674 - val_binary_accuracy: 0.6627\n",
      "Epoch 208/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5829 - binary_accuracy: 0.6766 - val_loss: 0.6674 - val_binary_accuracy: 0.6622\n",
      "Epoch 209/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5828 - binary_accuracy: 0.6766 - val_loss: 0.6674 - val_binary_accuracy: 0.6616\n",
      "Epoch 210/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5827 - binary_accuracy: 0.6766 - val_loss: 0.6674 - val_binary_accuracy: 0.6610\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 211/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5826 - binary_accuracy: 0.6766 - val_loss: 0.6674 - val_binary_accuracy: 0.6604\n",
      "Epoch 212/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5825 - binary_accuracy: 0.6762 - val_loss: 0.6674 - val_binary_accuracy: 0.6604\n",
      "Train on 2548 samples, validate on 1699 samples\n",
      "Epoch 1/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5825 - binary_accuracy: 0.6762 - val_loss: 0.6674 - val_binary_accuracy: 0.6604\n",
      "Epoch 2/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5824 - binary_accuracy: 0.6766 - val_loss: 0.6674 - val_binary_accuracy: 0.6604\n",
      "Train on 2548 samples, validate on 1699 samples\n",
      "Epoch 1/300\n",
      "2548/2548 [==============================] - 0s 10us/step - loss: 0.5823 - binary_accuracy: 0.6766 - val_loss: 0.6675 - val_binary_accuracy: 0.6604\n",
      "Epoch 2/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5822 - binary_accuracy: 0.6766 - val_loss: 0.6675 - val_binary_accuracy: 0.6604\n",
      "Train on 2548 samples, validate on 1699 samples\n",
      "Epoch 1/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5821 - binary_accuracy: 0.6770 - val_loss: 0.6675 - val_binary_accuracy: 0.6604\n",
      "Epoch 2/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5820 - binary_accuracy: 0.6770 - val_loss: 0.6675 - val_binary_accuracy: 0.6610\n",
      "Train on 2548 samples, validate on 1699 samples\n",
      "Epoch 1/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5819 - binary_accuracy: 0.6778 - val_loss: 0.6675 - val_binary_accuracy: 0.6622\n",
      "Epoch 2/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5818 - binary_accuracy: 0.6778 - val_loss: 0.6675 - val_binary_accuracy: 0.6616\n",
      "Train on 2548 samples, validate on 1699 samples\n",
      "Epoch 1/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5817 - binary_accuracy: 0.6774 - val_loss: 0.6675 - val_binary_accuracy: 0.6616\n",
      "Epoch 2/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5816 - binary_accuracy: 0.6786 - val_loss: 0.6675 - val_binary_accuracy: 0.6616\n",
      "Train on 2548 samples, validate on 1699 samples\n",
      "Epoch 1/300\n",
      "2548/2548 [==============================] - 0s 10us/step - loss: 0.5815 - binary_accuracy: 0.6786 - val_loss: 0.6675 - val_binary_accuracy: 0.6622\n",
      "Epoch 2/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5815 - binary_accuracy: 0.6790 - val_loss: 0.6675 - val_binary_accuracy: 0.6622\n",
      "Train on 2548 samples, validate on 1699 samples\n",
      "Epoch 1/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5814 - binary_accuracy: 0.6786 - val_loss: 0.6675 - val_binary_accuracy: 0.6627\n",
      "Epoch 2/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5813 - binary_accuracy: 0.6786 - val_loss: 0.6675 - val_binary_accuracy: 0.6627\n",
      "Train on 2548 samples, validate on 1699 samples\n",
      "Epoch 1/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5812 - binary_accuracy: 0.6786 - val_loss: 0.6675 - val_binary_accuracy: 0.6627\n",
      "Epoch 2/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5811 - binary_accuracy: 0.6797 - val_loss: 0.6676 - val_binary_accuracy: 0.6627\n",
      "Train on 2548 samples, validate on 1699 samples\n",
      "Epoch 1/300\n",
      "2548/2548 [==============================] - 0s 10us/step - loss: 0.5810 - binary_accuracy: 0.6801 - val_loss: 0.6676 - val_binary_accuracy: 0.6622\n",
      "Epoch 2/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5809 - binary_accuracy: 0.6801 - val_loss: 0.6676 - val_binary_accuracy: 0.6622\n",
      "Train on 2548 samples, validate on 1699 samples\n",
      "Epoch 1/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5808 - binary_accuracy: 0.6801 - val_loss: 0.6676 - val_binary_accuracy: 0.6627\n",
      "Epoch 2/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5807 - binary_accuracy: 0.6805 - val_loss: 0.6676 - val_binary_accuracy: 0.6627\n",
      "Train on 2548 samples, validate on 1699 samples\n",
      "Epoch 1/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5806 - binary_accuracy: 0.6801 - val_loss: 0.6676 - val_binary_accuracy: 0.6633\n",
      "Epoch 2/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5806 - binary_accuracy: 0.6801 - val_loss: 0.6676 - val_binary_accuracy: 0.6639\n",
      "Train on 2548 samples, validate on 1699 samples\n",
      "Epoch 1/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5805 - binary_accuracy: 0.6805 - val_loss: 0.6676 - val_binary_accuracy: 0.6639\n",
      "Epoch 2/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5804 - binary_accuracy: 0.6805 - val_loss: 0.6677 - val_binary_accuracy: 0.6639\n",
      "Train on 2548 samples, validate on 1699 samples\n",
      "Epoch 1/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5803 - binary_accuracy: 0.6805 - val_loss: 0.6677 - val_binary_accuracy: 0.6639\n",
      "Epoch 2/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5802 - binary_accuracy: 0.6805 - val_loss: 0.6677 - val_binary_accuracy: 0.6639\n",
      "Train on 2548 samples, validate on 1699 samples\n",
      "Epoch 1/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5801 - binary_accuracy: 0.6801 - val_loss: 0.6677 - val_binary_accuracy: 0.6639\n",
      "Epoch 2/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5800 - binary_accuracy: 0.6801 - val_loss: 0.6677 - val_binary_accuracy: 0.6633\n",
      "Train on 2548 samples, validate on 1699 samples\n",
      "Epoch 1/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5800 - binary_accuracy: 0.6805 - val_loss: 0.6678 - val_binary_accuracy: 0.6633\n",
      "Epoch 2/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5799 - binary_accuracy: 0.6801 - val_loss: 0.6678 - val_binary_accuracy: 0.6633\n",
      "Train on 2548 samples, validate on 1699 samples\n",
      "Epoch 1/300\n",
      "2548/2548 [==============================] - 0s 10us/step - loss: 0.5798 - binary_accuracy: 0.6797 - val_loss: 0.6678 - val_binary_accuracy: 0.6627\n",
      "Epoch 2/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5797 - binary_accuracy: 0.6797 - val_loss: 0.6678 - val_binary_accuracy: 0.6627\n",
      "Train on 2548 samples, validate on 1699 samples\n",
      "Epoch 1/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5796 - binary_accuracy: 0.6794 - val_loss: 0.6678 - val_binary_accuracy: 0.6627\n",
      "Epoch 2/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5795 - binary_accuracy: 0.6797 - val_loss: 0.6679 - val_binary_accuracy: 0.6627\n",
      "Train on 2548 samples, validate on 1699 samples\n",
      "Epoch 1/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5794 - binary_accuracy: 0.6794 - val_loss: 0.6679 - val_binary_accuracy: 0.6627\n",
      "Epoch 2/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5794 - binary_accuracy: 0.6794 - val_loss: 0.6679 - val_binary_accuracy: 0.6627\n",
      "Train on 2548 samples, validate on 1699 samples\n",
      "Epoch 1/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5793 - binary_accuracy: 0.6794 - val_loss: 0.6679 - val_binary_accuracy: 0.6633\n",
      "Epoch 2/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5792 - binary_accuracy: 0.6794 - val_loss: 0.6679 - val_binary_accuracy: 0.6633\n",
      "Train on 2548 samples, validate on 1699 samples\n",
      "Epoch 1/300\n",
      "2548/2548 [==============================] - 0s 10us/step - loss: 0.5791 - binary_accuracy: 0.6801 - val_loss: 0.6680 - val_binary_accuracy: 0.6633\n",
      "Epoch 2/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5790 - binary_accuracy: 0.6801 - val_loss: 0.6680 - val_binary_accuracy: 0.6639\n",
      "Train on 2548 samples, validate on 1699 samples\n",
      "Epoch 1/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5789 - binary_accuracy: 0.6801 - val_loss: 0.6680 - val_binary_accuracy: 0.6639\n",
      "Epoch 2/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5789 - binary_accuracy: 0.6797 - val_loss: 0.6680 - val_binary_accuracy: 0.6639\n",
      "Train on 2548 samples, validate on 1699 samples\n",
      "Epoch 1/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5788 - binary_accuracy: 0.6797 - val_loss: 0.6681 - val_binary_accuracy: 0.6645\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5787 - binary_accuracy: 0.6797 - val_loss: 0.6681 - val_binary_accuracy: 0.6645\n",
      "Train on 2548 samples, validate on 1699 samples\n",
      "Epoch 1/300\n",
      "2548/2548 [==============================] - 0s 10us/step - loss: 0.5786 - binary_accuracy: 0.6790 - val_loss: 0.6681 - val_binary_accuracy: 0.6645\n",
      "Epoch 2/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5785 - binary_accuracy: 0.6790 - val_loss: 0.6681 - val_binary_accuracy: 0.6645\n",
      "Train on 2548 samples, validate on 1699 samples\n",
      "Epoch 1/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5784 - binary_accuracy: 0.6794 - val_loss: 0.6682 - val_binary_accuracy: 0.6645\n",
      "Epoch 2/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5784 - binary_accuracy: 0.6797 - val_loss: 0.6682 - val_binary_accuracy: 0.6645\n",
      "Train on 2548 samples, validate on 1699 samples\n",
      "Epoch 1/300\n",
      "2548/2548 [==============================] - 0s 10us/step - loss: 0.5783 - binary_accuracy: 0.6801 - val_loss: 0.6682 - val_binary_accuracy: 0.6645\n",
      "Epoch 2/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5782 - binary_accuracy: 0.6801 - val_loss: 0.6683 - val_binary_accuracy: 0.6645\n",
      "Train on 2548 samples, validate on 1699 samples\n",
      "Epoch 1/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5781 - binary_accuracy: 0.6801 - val_loss: 0.6683 - val_binary_accuracy: 0.6633\n",
      "Epoch 2/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5780 - binary_accuracy: 0.6801 - val_loss: 0.6683 - val_binary_accuracy: 0.6633\n",
      "Train on 2548 samples, validate on 1699 samples\n",
      "Epoch 1/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5779 - binary_accuracy: 0.6801 - val_loss: 0.6683 - val_binary_accuracy: 0.6639\n",
      "Epoch 2/300\n",
      "2548/2548 [==============================] - 0s 12us/step - loss: 0.5779 - binary_accuracy: 0.6801 - val_loss: 0.6684 - val_binary_accuracy: 0.6633\n",
      "Train on 2548 samples, validate on 1699 samples\n",
      "Epoch 1/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5778 - binary_accuracy: 0.6805 - val_loss: 0.6684 - val_binary_accuracy: 0.6622\n",
      "Epoch 2/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5777 - binary_accuracy: 0.6805 - val_loss: 0.6684 - val_binary_accuracy: 0.6622\n",
      "Train on 2548 samples, validate on 1699 samples\n",
      "Epoch 1/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5776 - binary_accuracy: 0.6805 - val_loss: 0.6685 - val_binary_accuracy: 0.6627\n",
      "Epoch 2/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5775 - binary_accuracy: 0.6805 - val_loss: 0.6685 - val_binary_accuracy: 0.6633\n",
      "Train on 2548 samples, validate on 1699 samples\n",
      "Epoch 1/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5775 - binary_accuracy: 0.6801 - val_loss: 0.6685 - val_binary_accuracy: 0.6627\n",
      "Epoch 2/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5774 - binary_accuracy: 0.6805 - val_loss: 0.6686 - val_binary_accuracy: 0.6627\n",
      "Train on 2548 samples, validate on 1699 samples\n",
      "Epoch 1/300\n",
      "2548/2548 [==============================] - 0s 10us/step - loss: 0.5773 - binary_accuracy: 0.6805 - val_loss: 0.6686 - val_binary_accuracy: 0.6627\n",
      "Epoch 2/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5772 - binary_accuracy: 0.6805 - val_loss: 0.6686 - val_binary_accuracy: 0.6633\n",
      "Train on 2548 samples, validate on 1699 samples\n",
      "Epoch 1/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5771 - binary_accuracy: 0.6805 - val_loss: 0.6686 - val_binary_accuracy: 0.6633\n",
      "Epoch 2/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5771 - binary_accuracy: 0.6805 - val_loss: 0.6687 - val_binary_accuracy: 0.6639\n",
      "Train on 2548 samples, validate on 1699 samples\n",
      "Epoch 1/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5770 - binary_accuracy: 0.6805 - val_loss: 0.6687 - val_binary_accuracy: 0.6639\n",
      "Epoch 2/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5769 - binary_accuracy: 0.6805 - val_loss: 0.6687 - val_binary_accuracy: 0.6639\n",
      "Train on 2548 samples, validate on 1699 samples\n",
      "Epoch 1/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5768 - binary_accuracy: 0.6813 - val_loss: 0.6688 - val_binary_accuracy: 0.6639\n",
      "Epoch 2/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5768 - binary_accuracy: 0.6813 - val_loss: 0.6688 - val_binary_accuracy: 0.6639\n",
      "Train on 2548 samples, validate on 1699 samples\n",
      "Epoch 1/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5767 - binary_accuracy: 0.6809 - val_loss: 0.6688 - val_binary_accuracy: 0.6639\n",
      "Epoch 2/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5766 - binary_accuracy: 0.6809 - val_loss: 0.6689 - val_binary_accuracy: 0.6639\n",
      "Train on 2548 samples, validate on 1699 samples\n",
      "Epoch 1/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5765 - binary_accuracy: 0.6809 - val_loss: 0.6689 - val_binary_accuracy: 0.6633\n",
      "Epoch 2/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5764 - binary_accuracy: 0.6805 - val_loss: 0.6689 - val_binary_accuracy: 0.6627\n",
      "Train on 2548 samples, validate on 1699 samples\n",
      "Epoch 1/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5764 - binary_accuracy: 0.6805 - val_loss: 0.6690 - val_binary_accuracy: 0.6633\n",
      "Epoch 2/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5763 - binary_accuracy: 0.6805 - val_loss: 0.6690 - val_binary_accuracy: 0.6633\n",
      "Train on 2548 samples, validate on 1699 samples\n",
      "Epoch 1/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5762 - binary_accuracy: 0.6809 - val_loss: 0.6691 - val_binary_accuracy: 0.6633\n",
      "Epoch 2/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5761 - binary_accuracy: 0.6809 - val_loss: 0.6691 - val_binary_accuracy: 0.6633\n",
      "Train on 2548 samples, validate on 1699 samples\n",
      "Epoch 1/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5761 - binary_accuracy: 0.6809 - val_loss: 0.6691 - val_binary_accuracy: 0.6633\n",
      "Epoch 2/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5760 - binary_accuracy: 0.6813 - val_loss: 0.6692 - val_binary_accuracy: 0.6639\n",
      "Train on 2548 samples, validate on 1699 samples\n",
      "Epoch 1/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5759 - binary_accuracy: 0.6813 - val_loss: 0.6692 - val_binary_accuracy: 0.6639\n",
      "Epoch 2/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5758 - binary_accuracy: 0.6817 - val_loss: 0.6692 - val_binary_accuracy: 0.6633\n",
      "Train on 2548 samples, validate on 1699 samples\n",
      "Epoch 1/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5758 - binary_accuracy: 0.6813 - val_loss: 0.6693 - val_binary_accuracy: 0.6627\n",
      "Epoch 2/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5757 - binary_accuracy: 0.6813 - val_loss: 0.6693 - val_binary_accuracy: 0.6622\n",
      "Train on 2548 samples, validate on 1699 samples\n",
      "Epoch 1/300\n",
      "2548/2548 [==============================] - 0s 10us/step - loss: 0.5756 - binary_accuracy: 0.6813 - val_loss: 0.6693 - val_binary_accuracy: 0.6627\n",
      "Epoch 2/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5755 - binary_accuracy: 0.6809 - val_loss: 0.6694 - val_binary_accuracy: 0.6633\n",
      "Train on 2548 samples, validate on 1699 samples\n",
      "Epoch 1/300\n",
      "2548/2548 [==============================] - 0s 10us/step - loss: 0.5755 - binary_accuracy: 0.6813 - val_loss: 0.6694 - val_binary_accuracy: 0.6633\n",
      "Epoch 2/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5754 - binary_accuracy: 0.6813 - val_loss: 0.6695 - val_binary_accuracy: 0.6633\n",
      "Train on 2548 samples, validate on 1699 samples\n",
      "Epoch 1/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5753 - binary_accuracy: 0.6813 - val_loss: 0.6695 - val_binary_accuracy: 0.6633\n",
      "Epoch 2/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5752 - binary_accuracy: 0.6813 - val_loss: 0.6695 - val_binary_accuracy: 0.6627\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2548 samples, validate on 1699 samples\n",
      "Epoch 1/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5752 - binary_accuracy: 0.6813 - val_loss: 0.6696 - val_binary_accuracy: 0.6622\n",
      "Epoch 2/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5751 - binary_accuracy: 0.6817 - val_loss: 0.6696 - val_binary_accuracy: 0.6616\n",
      "Train on 2548 samples, validate on 1699 samples\n",
      "Epoch 1/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5750 - binary_accuracy: 0.6817 - val_loss: 0.6696 - val_binary_accuracy: 0.6622\n",
      "Epoch 2/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5749 - binary_accuracy: 0.6817 - val_loss: 0.6697 - val_binary_accuracy: 0.6622\n",
      "Train on 2548 samples, validate on 1699 samples\n",
      "Epoch 1/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5749 - binary_accuracy: 0.6813 - val_loss: 0.6697 - val_binary_accuracy: 0.6622\n",
      "Epoch 2/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5748 - binary_accuracy: 0.6817 - val_loss: 0.6698 - val_binary_accuracy: 0.6627\n",
      "Train on 2548 samples, validate on 1699 samples\n",
      "Epoch 1/300\n",
      "2548/2548 [==============================] - 0s 10us/step - loss: 0.5747 - binary_accuracy: 0.6821 - val_loss: 0.6698 - val_binary_accuracy: 0.6633\n",
      "Epoch 2/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5746 - binary_accuracy: 0.6821 - val_loss: 0.6698 - val_binary_accuracy: 0.6633\n",
      "Train on 2548 samples, validate on 1699 samples\n",
      "Epoch 1/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5746 - binary_accuracy: 0.6825 - val_loss: 0.6699 - val_binary_accuracy: 0.6633\n",
      "Epoch 2/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5745 - binary_accuracy: 0.6829 - val_loss: 0.6699 - val_binary_accuracy: 0.6633\n",
      "Train on 2548 samples, validate on 1699 samples\n",
      "Epoch 1/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5744 - binary_accuracy: 0.6829 - val_loss: 0.6700 - val_binary_accuracy: 0.6633\n",
      "Epoch 2/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5744 - binary_accuracy: 0.6829 - val_loss: 0.6700 - val_binary_accuracy: 0.6627\n",
      "Train on 2548 samples, validate on 1699 samples\n",
      "Epoch 1/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5743 - binary_accuracy: 0.6833 - val_loss: 0.6700 - val_binary_accuracy: 0.6627\n",
      "Epoch 2/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5742 - binary_accuracy: 0.6833 - val_loss: 0.6701 - val_binary_accuracy: 0.6622\n",
      "Train on 2548 samples, validate on 1699 samples\n",
      "Epoch 1/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5741 - binary_accuracy: 0.6833 - val_loss: 0.6701 - val_binary_accuracy: 0.6622\n",
      "Epoch 2/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5741 - binary_accuracy: 0.6829 - val_loss: 0.6702 - val_binary_accuracy: 0.6627\n",
      "Train on 2548 samples, validate on 1699 samples\n",
      "Epoch 1/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5740 - binary_accuracy: 0.6845 - val_loss: 0.6702 - val_binary_accuracy: 0.6633\n",
      "Epoch 2/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5739 - binary_accuracy: 0.6845 - val_loss: 0.6702 - val_binary_accuracy: 0.6633\n",
      "Train on 2548 samples, validate on 1699 samples\n",
      "Epoch 1/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5739 - binary_accuracy: 0.6845 - val_loss: 0.6703 - val_binary_accuracy: 0.6633\n",
      "Epoch 2/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5738 - binary_accuracy: 0.6845 - val_loss: 0.6703 - val_binary_accuracy: 0.6633\n",
      "Train on 2548 samples, validate on 1699 samples\n",
      "Epoch 1/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5737 - binary_accuracy: 0.6845 - val_loss: 0.6704 - val_binary_accuracy: 0.6633\n",
      "Epoch 2/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5736 - binary_accuracy: 0.6849 - val_loss: 0.6704 - val_binary_accuracy: 0.6633\n",
      "Train on 2548 samples, validate on 1699 samples\n",
      "Epoch 1/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5736 - binary_accuracy: 0.6849 - val_loss: 0.6704 - val_binary_accuracy: 0.6633\n",
      "Epoch 2/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5735 - binary_accuracy: 0.6849 - val_loss: 0.6705 - val_binary_accuracy: 0.6633\n",
      "Train on 2548 samples, validate on 1699 samples\n",
      "Epoch 1/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5734 - binary_accuracy: 0.6841 - val_loss: 0.6705 - val_binary_accuracy: 0.6633\n",
      "Epoch 2/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5734 - binary_accuracy: 0.6833 - val_loss: 0.6706 - val_binary_accuracy: 0.6633\n",
      "Train on 2548 samples, validate on 1699 samples\n",
      "Epoch 1/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5733 - binary_accuracy: 0.6833 - val_loss: 0.6706 - val_binary_accuracy: 0.6645\n",
      "Epoch 2/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5732 - binary_accuracy: 0.6825 - val_loss: 0.6706 - val_binary_accuracy: 0.6639\n",
      "Train on 2548 samples, validate on 1699 samples\n",
      "Epoch 1/300\n",
      "2548/2548 [==============================] - 0s 10us/step - loss: 0.5732 - binary_accuracy: 0.6825 - val_loss: 0.6707 - val_binary_accuracy: 0.6633\n",
      "Epoch 2/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5731 - binary_accuracy: 0.6825 - val_loss: 0.6707 - val_binary_accuracy: 0.6633\n",
      "Train on 2548 samples, validate on 1699 samples\n",
      "Epoch 1/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5730 - binary_accuracy: 0.6825 - val_loss: 0.6708 - val_binary_accuracy: 0.6633\n",
      "Epoch 2/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5729 - binary_accuracy: 0.6825 - val_loss: 0.6708 - val_binary_accuracy: 0.6645\n",
      "Train on 2548 samples, validate on 1699 samples\n",
      "Epoch 1/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5729 - binary_accuracy: 0.6833 - val_loss: 0.6709 - val_binary_accuracy: 0.6639\n",
      "Epoch 2/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5728 - binary_accuracy: 0.6841 - val_loss: 0.6709 - val_binary_accuracy: 0.6639\n",
      "Train on 2548 samples, validate on 1699 samples\n",
      "Epoch 1/300\n",
      "2548/2548 [==============================] - 0s 10us/step - loss: 0.5727 - binary_accuracy: 0.6841 - val_loss: 0.6709 - val_binary_accuracy: 0.6645\n",
      "Epoch 2/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5727 - binary_accuracy: 0.6837 - val_loss: 0.6710 - val_binary_accuracy: 0.6639\n",
      "Train on 2548 samples, validate on 1699 samples\n",
      "Epoch 1/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5726 - binary_accuracy: 0.6837 - val_loss: 0.6710 - val_binary_accuracy: 0.6639\n",
      "Epoch 2/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5725 - binary_accuracy: 0.6837 - val_loss: 0.6711 - val_binary_accuracy: 0.6645\n",
      "Train on 2548 samples, validate on 1699 samples\n",
      "Epoch 1/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5725 - binary_accuracy: 0.6841 - val_loss: 0.6711 - val_binary_accuracy: 0.6645\n",
      "Epoch 2/300\n",
      "2548/2548 [==============================] - 0s 12us/step - loss: 0.5724 - binary_accuracy: 0.6841 - val_loss: 0.6712 - val_binary_accuracy: 0.6645\n",
      "Train on 2548 samples, validate on 1699 samples\n",
      "Epoch 1/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5723 - binary_accuracy: 0.6841 - val_loss: 0.6712 - val_binary_accuracy: 0.6645\n",
      "Epoch 2/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5723 - binary_accuracy: 0.6837 - val_loss: 0.6712 - val_binary_accuracy: 0.6645\n",
      "Train on 2548 samples, validate on 1699 samples\n",
      "Epoch 1/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5722 - binary_accuracy: 0.6841 - val_loss: 0.6713 - val_binary_accuracy: 0.6645\n",
      "Epoch 2/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5721 - binary_accuracy: 0.6845 - val_loss: 0.6713 - val_binary_accuracy: 0.6645\n",
      "Train on 2548 samples, validate on 1699 samples\n",
      "Epoch 1/300\n",
      "2548/2548 [==============================] - 0s 10us/step - loss: 0.5721 - binary_accuracy: 0.6845 - val_loss: 0.6714 - val_binary_accuracy: 0.6645\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5720 - binary_accuracy: 0.6841 - val_loss: 0.6714 - val_binary_accuracy: 0.6651\n",
      "Train on 2548 samples, validate on 1699 samples\n",
      "Epoch 1/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5719 - binary_accuracy: 0.6849 - val_loss: 0.6715 - val_binary_accuracy: 0.6645\n",
      "Epoch 2/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5719 - binary_accuracy: 0.6852 - val_loss: 0.6715 - val_binary_accuracy: 0.6645\n",
      "Train on 2548 samples, validate on 1699 samples\n",
      "Epoch 1/300\n",
      "2548/2548 [==============================] - 0s 10us/step - loss: 0.5718 - binary_accuracy: 0.6860 - val_loss: 0.6715 - val_binary_accuracy: 0.6645\n",
      "Epoch 2/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5717 - binary_accuracy: 0.6860 - val_loss: 0.6716 - val_binary_accuracy: 0.6645\n",
      "Train on 2548 samples, validate on 1699 samples\n",
      "Epoch 1/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5717 - binary_accuracy: 0.6860 - val_loss: 0.6716 - val_binary_accuracy: 0.6645\n",
      "Epoch 2/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5716 - binary_accuracy: 0.6860 - val_loss: 0.6717 - val_binary_accuracy: 0.6645\n",
      "Train on 2548 samples, validate on 1699 samples\n",
      "Epoch 1/300\n",
      "2548/2548 [==============================] - 0s 10us/step - loss: 0.5715 - binary_accuracy: 0.6868 - val_loss: 0.6717 - val_binary_accuracy: 0.6651\n",
      "Epoch 2/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5715 - binary_accuracy: 0.6872 - val_loss: 0.6718 - val_binary_accuracy: 0.6651\n",
      "Train on 2548 samples, validate on 1699 samples\n",
      "Epoch 1/300\n",
      "2548/2548 [==============================] - 0s 10us/step - loss: 0.5714 - binary_accuracy: 0.6876 - val_loss: 0.6718 - val_binary_accuracy: 0.6651\n",
      "Epoch 2/300\n",
      "2548/2548 [==============================] - 0s 10us/step - loss: 0.5713 - binary_accuracy: 0.6876 - val_loss: 0.6718 - val_binary_accuracy: 0.6657\n",
      "Train on 2548 samples, validate on 1699 samples\n",
      "Epoch 1/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5713 - binary_accuracy: 0.6880 - val_loss: 0.6719 - val_binary_accuracy: 0.6657\n",
      "Epoch 2/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5712 - binary_accuracy: 0.6884 - val_loss: 0.6719 - val_binary_accuracy: 0.6663\n",
      "Train on 2548 samples, validate on 1699 samples\n",
      "Epoch 1/300\n",
      "2548/2548 [==============================] - 0s 10us/step - loss: 0.5711 - binary_accuracy: 0.6884 - val_loss: 0.6720 - val_binary_accuracy: 0.6663\n",
      "Epoch 2/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5711 - binary_accuracy: 0.6884 - val_loss: 0.6720 - val_binary_accuracy: 0.6669\n",
      "Train on 2548 samples, validate on 1699 samples\n",
      "Epoch 1/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5710 - binary_accuracy: 0.6884 - val_loss: 0.6721 - val_binary_accuracy: 0.6669\n",
      "Epoch 2/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5710 - binary_accuracy: 0.6884 - val_loss: 0.6721 - val_binary_accuracy: 0.6669\n",
      "Train on 2548 samples, validate on 1699 samples\n",
      "Epoch 1/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5709 - binary_accuracy: 0.6888 - val_loss: 0.6722 - val_binary_accuracy: 0.6675\n",
      "Epoch 2/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5708 - binary_accuracy: 0.6888 - val_loss: 0.6722 - val_binary_accuracy: 0.6675\n",
      "Train on 2548 samples, validate on 1699 samples\n",
      "Epoch 1/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5708 - binary_accuracy: 0.6888 - val_loss: 0.6722 - val_binary_accuracy: 0.6680\n",
      "Epoch 2/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5707 - binary_accuracy: 0.6892 - val_loss: 0.6723 - val_binary_accuracy: 0.6680\n",
      "Train on 2548 samples, validate on 1699 samples\n",
      "Epoch 1/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5706 - binary_accuracy: 0.6892 - val_loss: 0.6723 - val_binary_accuracy: 0.6686\n",
      "Epoch 2/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5706 - binary_accuracy: 0.6896 - val_loss: 0.6724 - val_binary_accuracy: 0.6686\n",
      "Train on 2548 samples, validate on 1699 samples\n",
      "Epoch 1/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5705 - binary_accuracy: 0.6896 - val_loss: 0.6724 - val_binary_accuracy: 0.6686\n",
      "Epoch 2/300\n",
      "2548/2548 [==============================] - 0s 13us/step - loss: 0.5704 - binary_accuracy: 0.6900 - val_loss: 0.6725 - val_binary_accuracy: 0.6686\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_2 (Dense)              (None, 1)                 595       \n",
      "=================================================================\n",
      "Total params: 595\n",
      "Trainable params: 595\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 2548 samples, validate on 1699 samples\n",
      "Epoch 1/300\n",
      "2548/2548 [==============================] - 0s 66us/step - loss: 0.7168 - binary_accuracy: 0.4278 - val_loss: 0.6463 - val_binary_accuracy: 0.7887\n",
      "Epoch 2/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.6973 - binary_accuracy: 0.4929 - val_loss: 0.7326 - val_binary_accuracy: 0.2749\n",
      "Train on 2548 samples, validate on 1699 samples\n",
      "Epoch 1/300\n",
      "2548/2548 [==============================] - 0s 10us/step - loss: 0.6823 - binary_accuracy: 0.5714 - val_loss: 0.8246 - val_binary_accuracy: 0.0212\n",
      "Epoch 2/300\n",
      "2548/2548 [==============================] - 0s 13us/step - loss: 0.6747 - binary_accuracy: 0.5808 - val_loss: 0.9127 - val_binary_accuracy: 5.8858e-04\n",
      "Train on 2548 samples, validate on 1699 samples\n",
      "Epoch 1/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.6738 - binary_accuracy: 0.5832 - val_loss: 0.9852 - val_binary_accuracy: 0.0000e+00\n",
      "Epoch 2/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.6769 - binary_accuracy: 0.5828 - val_loss: 1.0326 - val_binary_accuracy: 0.0000e+00\n",
      "Train on 2548 samples, validate on 1699 samples\n",
      "Epoch 1/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.6803 - binary_accuracy: 0.5828 - val_loss: 1.0524 - val_binary_accuracy: 0.0000e+00\n",
      "Epoch 2/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.6817 - binary_accuracy: 0.5828 - val_loss: 1.0477 - val_binary_accuracy: 0.0000e+00\n",
      "Epoch 3/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.6805 - binary_accuracy: 0.5828 - val_loss: 1.0240 - val_binary_accuracy: 0.0000e+00\n",
      "Epoch 4/300\n",
      "2548/2548 [==============================] - 0s 10us/step - loss: 0.6773 - binary_accuracy: 0.5828 - val_loss: 0.9873 - val_binary_accuracy: 0.0000e+00\n",
      "Epoch 5/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.6732 - binary_accuracy: 0.5828 - val_loss: 0.9431 - val_binary_accuracy: 0.0000e+00\n",
      "Epoch 6/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.6693 - binary_accuracy: 0.5828 - val_loss: 0.8964 - val_binary_accuracy: 5.8858e-04\n",
      "Epoch 7/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.6665 - binary_accuracy: 0.5824 - val_loss: 0.8515 - val_binary_accuracy: 0.0094\n",
      "Epoch 8/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.6649 - binary_accuracy: 0.5820 - val_loss: 0.8120 - val_binary_accuracy: 0.0465\n",
      "Epoch 9/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.6645 - binary_accuracy: 0.5840 - val_loss: 0.7808 - val_binary_accuracy: 0.1107\n",
      "Epoch 10/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.6648 - binary_accuracy: 0.5903 - val_loss: 0.7594 - val_binary_accuracy: 0.1848\n",
      "Epoch 11/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.6652 - binary_accuracy: 0.5926 - val_loss: 0.7484 - val_binary_accuracy: 0.2237\n",
      "Epoch 12/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.6649 - binary_accuracy: 0.5946 - val_loss: 0.7473 - val_binary_accuracy: 0.2343\n",
      "Epoch 13/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.6640 - binary_accuracy: 0.5958 - val_loss: 0.7549 - val_binary_accuracy: 0.2078\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2548 samples, validate on 1699 samples\n",
      "Epoch 1/300\n",
      "2548/2548 [==============================] - 0s 10us/step - loss: 0.6624 - binary_accuracy: 0.5989 - val_loss: 0.7696 - val_binary_accuracy: 0.1630\n",
      "Epoch 2/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.6604 - binary_accuracy: 0.5934 - val_loss: 0.7894 - val_binary_accuracy: 0.1077\n",
      "Train on 2548 samples, validate on 1699 samples\n",
      "Epoch 1/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.6585 - binary_accuracy: 0.5903 - val_loss: 0.8120 - val_binary_accuracy: 0.0618\n",
      "Epoch 2/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.6570 - binary_accuracy: 0.5859 - val_loss: 0.8350 - val_binary_accuracy: 0.0318\n",
      "Train on 2548 samples, validate on 1699 samples\n",
      "Epoch 1/300\n",
      "2548/2548 [==============================] - 0s 10us/step - loss: 0.6559 - binary_accuracy: 0.5805 - val_loss: 0.8559 - val_binary_accuracy: 0.0194\n",
      "Epoch 2/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.6553 - binary_accuracy: 0.5824 - val_loss: 0.8725 - val_binary_accuracy: 0.0112\n",
      "Train on 2548 samples, validate on 1699 samples\n",
      "Epoch 1/300\n",
      "2548/2548 [==============================] - 0s 10us/step - loss: 0.6549 - binary_accuracy: 0.5820 - val_loss: 0.8832 - val_binary_accuracy: 0.0094\n",
      "Epoch 2/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.6545 - binary_accuracy: 0.5820 - val_loss: 0.8871 - val_binary_accuracy: 0.0094\n",
      "Train on 2548 samples, validate on 1699 samples\n",
      "Epoch 1/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.6539 - binary_accuracy: 0.5828 - val_loss: 0.8841 - val_binary_accuracy: 0.0100\n",
      "Epoch 2/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.6530 - binary_accuracy: 0.5824 - val_loss: 0.8752 - val_binary_accuracy: 0.0153\n",
      "Epoch 3/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.6519 - binary_accuracy: 0.5820 - val_loss: 0.8615 - val_binary_accuracy: 0.0230\n",
      "Epoch 4/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.6508 - binary_accuracy: 0.5805 - val_loss: 0.8448 - val_binary_accuracy: 0.0424\n",
      "Epoch 5/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.6496 - binary_accuracy: 0.5789 - val_loss: 0.8269 - val_binary_accuracy: 0.0689\n",
      "Epoch 6/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.6486 - binary_accuracy: 0.5832 - val_loss: 0.8098 - val_binary_accuracy: 0.1024\n",
      "Epoch 7/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.6478 - binary_accuracy: 0.5852 - val_loss: 0.7948 - val_binary_accuracy: 0.1471\n",
      "Epoch 8/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.6472 - binary_accuracy: 0.5883 - val_loss: 0.7834 - val_binary_accuracy: 0.1848\n",
      "Epoch 9/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.6467 - binary_accuracy: 0.5938 - val_loss: 0.7760 - val_binary_accuracy: 0.2148\n",
      "Epoch 10/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.6461 - binary_accuracy: 0.6001 - val_loss: 0.7731 - val_binary_accuracy: 0.2325\n",
      "Epoch 11/300\n",
      "2548/2548 [==============================] - 0s 10us/step - loss: 0.6454 - binary_accuracy: 0.6020 - val_loss: 0.7743 - val_binary_accuracy: 0.2343\n",
      "Train on 2548 samples, validate on 1699 samples\n",
      "Epoch 1/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.6446 - binary_accuracy: 0.6016 - val_loss: 0.7790 - val_binary_accuracy: 0.2254\n",
      "Epoch 2/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.6438 - binary_accuracy: 0.6009 - val_loss: 0.7862 - val_binary_accuracy: 0.2048\n",
      "Train on 2548 samples, validate on 1699 samples\n",
      "Epoch 1/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.6429 - binary_accuracy: 0.5958 - val_loss: 0.7948 - val_binary_accuracy: 0.1830\n",
      "Epoch 2/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.6421 - binary_accuracy: 0.5922 - val_loss: 0.8035 - val_binary_accuracy: 0.1566\n",
      "Train on 2548 samples, validate on 1699 samples\n",
      "Epoch 1/300\n",
      "2548/2548 [==============================] - 0s 10us/step - loss: 0.6415 - binary_accuracy: 0.5899 - val_loss: 0.8111 - val_binary_accuracy: 0.1460\n",
      "Epoch 2/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.6408 - binary_accuracy: 0.5867 - val_loss: 0.8166 - val_binary_accuracy: 0.1342\n",
      "Train on 2548 samples, validate on 1699 samples\n",
      "Epoch 1/300\n",
      "2548/2548 [==============================] - 0s 10us/step - loss: 0.6403 - binary_accuracy: 0.5867 - val_loss: 0.8193 - val_binary_accuracy: 0.1330\n",
      "Epoch 2/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.6397 - binary_accuracy: 0.5867 - val_loss: 0.8190 - val_binary_accuracy: 0.1424\n",
      "Epoch 3/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.6391 - binary_accuracy: 0.5879 - val_loss: 0.8157 - val_binary_accuracy: 0.1507\n",
      "Epoch 4/300\n",
      "2548/2548 [==============================] - 0s 10us/step - loss: 0.6384 - binary_accuracy: 0.5891 - val_loss: 0.8100 - val_binary_accuracy: 0.1719\n",
      "Epoch 5/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.6377 - binary_accuracy: 0.5911 - val_loss: 0.8026 - val_binary_accuracy: 0.2048\n",
      "Epoch 6/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.6370 - binary_accuracy: 0.5938 - val_loss: 0.7945 - val_binary_accuracy: 0.2307\n",
      "Epoch 7/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.6364 - binary_accuracy: 0.5989 - val_loss: 0.7864 - val_binary_accuracy: 0.2631\n",
      "Epoch 8/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.6358 - binary_accuracy: 0.6071 - val_loss: 0.7794 - val_binary_accuracy: 0.2984\n",
      "Epoch 9/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.6352 - binary_accuracy: 0.6068 - val_loss: 0.7739 - val_binary_accuracy: 0.3167\n",
      "Epoch 10/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.6347 - binary_accuracy: 0.6107 - val_loss: 0.7704 - val_binary_accuracy: 0.3290\n",
      "Epoch 11/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.6341 - binary_accuracy: 0.6126 - val_loss: 0.7690 - val_binary_accuracy: 0.3355\n",
      "Epoch 12/300\n",
      "2548/2548 [==============================] - 0s 16us/step - loss: 0.6335 - binary_accuracy: 0.6122 - val_loss: 0.7695 - val_binary_accuracy: 0.3384\n",
      "Train on 2548 samples, validate on 1699 samples\n",
      "Epoch 1/300\n",
      "2548/2548 [==============================] - 0s 10us/step - loss: 0.6330 - binary_accuracy: 0.6130 - val_loss: 0.7715 - val_binary_accuracy: 0.3361\n",
      "Epoch 2/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.6324 - binary_accuracy: 0.6126 - val_loss: 0.7744 - val_binary_accuracy: 0.3308\n",
      "Train on 2548 samples, validate on 1699 samples\n",
      "Epoch 1/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.6318 - binary_accuracy: 0.6107 - val_loss: 0.7777 - val_binary_accuracy: 0.3267\n",
      "Epoch 2/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.6312 - binary_accuracy: 0.6083 - val_loss: 0.7806 - val_binary_accuracy: 0.3237\n",
      "Train on 2548 samples, validate on 1699 samples\n",
      "Epoch 1/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.6307 - binary_accuracy: 0.6068 - val_loss: 0.7827 - val_binary_accuracy: 0.3220\n",
      "Epoch 2/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.6302 - binary_accuracy: 0.6064 - val_loss: 0.7835 - val_binary_accuracy: 0.3225\n",
      "Train on 2548 samples, validate on 1699 samples\n",
      "Epoch 1/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.6297 - binary_accuracy: 0.6064 - val_loss: 0.7829 - val_binary_accuracy: 0.3261\n",
      "Epoch 2/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.6291 - binary_accuracy: 0.6079 - val_loss: 0.7809 - val_binary_accuracy: 0.3314\n",
      "Epoch 3/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.6286 - binary_accuracy: 0.6122 - val_loss: 0.7778 - val_binary_accuracy: 0.3461\n",
      "Epoch 4/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.6281 - binary_accuracy: 0.6130 - val_loss: 0.7739 - val_binary_accuracy: 0.3661\n",
      "Epoch 5/300\n",
      "2548/2548 [==============================] - 0s 14us/step - loss: 0.6276 - binary_accuracy: 0.6146 - val_loss: 0.7697 - val_binary_accuracy: 0.3861\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.6271 - binary_accuracy: 0.6154 - val_loss: 0.7657 - val_binary_accuracy: 0.4002\n",
      "Epoch 7/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.6266 - binary_accuracy: 0.6205 - val_loss: 0.7621 - val_binary_accuracy: 0.4126\n",
      "Epoch 8/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.6261 - binary_accuracy: 0.6217 - val_loss: 0.7595 - val_binary_accuracy: 0.4220\n",
      "Epoch 9/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.6256 - binary_accuracy: 0.6268 - val_loss: 0.7578 - val_binary_accuracy: 0.4279\n",
      "Epoch 10/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.6252 - binary_accuracy: 0.6268 - val_loss: 0.7570 - val_binary_accuracy: 0.4326\n",
      "Epoch 11/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.6247 - binary_accuracy: 0.6283 - val_loss: 0.7571 - val_binary_accuracy: 0.4332\n",
      "Train on 2548 samples, validate on 1699 samples\n",
      "Epoch 1/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.6242 - binary_accuracy: 0.6283 - val_loss: 0.7578 - val_binary_accuracy: 0.4332\n",
      "Epoch 2/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.6237 - binary_accuracy: 0.6291 - val_loss: 0.7587 - val_binary_accuracy: 0.4338\n",
      "Train on 2548 samples, validate on 1699 samples\n",
      "Epoch 1/300\n",
      "2548/2548 [==============================] - 0s 10us/step - loss: 0.6233 - binary_accuracy: 0.6287 - val_loss: 0.7596 - val_binary_accuracy: 0.4356\n",
      "Epoch 2/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.6228 - binary_accuracy: 0.6279 - val_loss: 0.7601 - val_binary_accuracy: 0.4367\n",
      "Train on 2548 samples, validate on 1699 samples\n",
      "Epoch 1/300\n",
      "2548/2548 [==============================] - 0s 10us/step - loss: 0.6224 - binary_accuracy: 0.6279 - val_loss: 0.7601 - val_binary_accuracy: 0.4397\n",
      "Epoch 2/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.6219 - binary_accuracy: 0.6279 - val_loss: 0.7594 - val_binary_accuracy: 0.4426\n",
      "Epoch 3/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.6215 - binary_accuracy: 0.6291 - val_loss: 0.7580 - val_binary_accuracy: 0.4497\n",
      "Epoch 4/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.6211 - binary_accuracy: 0.6295 - val_loss: 0.7561 - val_binary_accuracy: 0.4550\n",
      "Epoch 5/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.6206 - binary_accuracy: 0.6315 - val_loss: 0.7538 - val_binary_accuracy: 0.4591\n",
      "Epoch 6/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.6202 - binary_accuracy: 0.6319 - val_loss: 0.7514 - val_binary_accuracy: 0.4656\n",
      "Epoch 7/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.6198 - binary_accuracy: 0.6327 - val_loss: 0.7491 - val_binary_accuracy: 0.4738\n",
      "Epoch 8/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.6194 - binary_accuracy: 0.6338 - val_loss: 0.7471 - val_binary_accuracy: 0.4803\n",
      "Epoch 9/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.6190 - binary_accuracy: 0.6350 - val_loss: 0.7455 - val_binary_accuracy: 0.4856\n",
      "Epoch 10/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.6186 - binary_accuracy: 0.6370 - val_loss: 0.7444 - val_binary_accuracy: 0.4897\n",
      "Epoch 11/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.6182 - binary_accuracy: 0.6374 - val_loss: 0.7438 - val_binary_accuracy: 0.4909\n",
      "Epoch 12/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.6178 - binary_accuracy: 0.6397 - val_loss: 0.7435 - val_binary_accuracy: 0.4926\n",
      "Epoch 13/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.6174 - binary_accuracy: 0.6401 - val_loss: 0.7434 - val_binary_accuracy: 0.4938\n",
      "Epoch 14/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.6170 - binary_accuracy: 0.6413 - val_loss: 0.7434 - val_binary_accuracy: 0.4968\n",
      "Epoch 15/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.6166 - binary_accuracy: 0.6432 - val_loss: 0.7433 - val_binary_accuracy: 0.4979\n",
      "Epoch 16/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.6162 - binary_accuracy: 0.6432 - val_loss: 0.7430 - val_binary_accuracy: 0.5003\n",
      "Epoch 17/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.6158 - binary_accuracy: 0.6440 - val_loss: 0.7423 - val_binary_accuracy: 0.5026\n",
      "Epoch 18/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.6155 - binary_accuracy: 0.6456 - val_loss: 0.7414 - val_binary_accuracy: 0.5050\n",
      "Epoch 19/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.6151 - binary_accuracy: 0.6444 - val_loss: 0.7401 - val_binary_accuracy: 0.5091\n",
      "Epoch 20/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.6147 - binary_accuracy: 0.6452 - val_loss: 0.7387 - val_binary_accuracy: 0.5138\n",
      "Epoch 21/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.6144 - binary_accuracy: 0.6456 - val_loss: 0.7372 - val_binary_accuracy: 0.5191\n",
      "Epoch 22/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.6140 - binary_accuracy: 0.6456 - val_loss: 0.7357 - val_binary_accuracy: 0.5238\n",
      "Epoch 23/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.6136 - binary_accuracy: 0.6452 - val_loss: 0.7343 - val_binary_accuracy: 0.5309\n",
      "Epoch 24/300\n",
      "2548/2548 [==============================] - 0s 12us/step - loss: 0.6133 - binary_accuracy: 0.6464 - val_loss: 0.7331 - val_binary_accuracy: 0.5338\n",
      "Epoch 25/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.6129 - binary_accuracy: 0.6464 - val_loss: 0.7322 - val_binary_accuracy: 0.5386\n",
      "Epoch 26/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.6126 - binary_accuracy: 0.6480 - val_loss: 0.7315 - val_binary_accuracy: 0.5415\n",
      "Epoch 27/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.6123 - binary_accuracy: 0.6487 - val_loss: 0.7310 - val_binary_accuracy: 0.5438\n",
      "Epoch 28/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.6119 - binary_accuracy: 0.6491 - val_loss: 0.7306 - val_binary_accuracy: 0.5456\n",
      "Epoch 29/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.6116 - binary_accuracy: 0.6499 - val_loss: 0.7302 - val_binary_accuracy: 0.5468\n",
      "Epoch 30/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.6113 - binary_accuracy: 0.6503 - val_loss: 0.7298 - val_binary_accuracy: 0.5474\n",
      "Epoch 31/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.6109 - binary_accuracy: 0.6499 - val_loss: 0.7293 - val_binary_accuracy: 0.5503\n",
      "Epoch 32/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.6106 - binary_accuracy: 0.6507 - val_loss: 0.7286 - val_binary_accuracy: 0.5521\n",
      "Epoch 33/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.6103 - binary_accuracy: 0.6503 - val_loss: 0.7277 - val_binary_accuracy: 0.5533\n",
      "Epoch 34/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.6100 - binary_accuracy: 0.6527 - val_loss: 0.7268 - val_binary_accuracy: 0.5556\n",
      "Epoch 35/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.6097 - binary_accuracy: 0.6527 - val_loss: 0.7257 - val_binary_accuracy: 0.5586\n",
      "Epoch 36/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.6093 - binary_accuracy: 0.6542 - val_loss: 0.7246 - val_binary_accuracy: 0.5597\n",
      "Epoch 37/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.6090 - binary_accuracy: 0.6554 - val_loss: 0.7236 - val_binary_accuracy: 0.5615\n",
      "Epoch 38/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.6087 - binary_accuracy: 0.6558 - val_loss: 0.7227 - val_binary_accuracy: 0.5627\n",
      "Epoch 39/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.6084 - binary_accuracy: 0.6570 - val_loss: 0.7218 - val_binary_accuracy: 0.5633\n",
      "Epoch 40/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.6081 - binary_accuracy: 0.6570 - val_loss: 0.7211 - val_binary_accuracy: 0.5650\n",
      "Epoch 41/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.6078 - binary_accuracy: 0.6578 - val_loss: 0.7205 - val_binary_accuracy: 0.5656\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 42/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.6076 - binary_accuracy: 0.6593 - val_loss: 0.7200 - val_binary_accuracy: 0.5662\n",
      "Epoch 43/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.6073 - binary_accuracy: 0.6593 - val_loss: 0.7195 - val_binary_accuracy: 0.5674\n",
      "Epoch 44/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.6070 - binary_accuracy: 0.6586 - val_loss: 0.7190 - val_binary_accuracy: 0.5686\n",
      "Epoch 45/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.6067 - binary_accuracy: 0.6589 - val_loss: 0.7184 - val_binary_accuracy: 0.5686\n",
      "Epoch 46/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.6064 - binary_accuracy: 0.6597 - val_loss: 0.7178 - val_binary_accuracy: 0.5697\n",
      "Epoch 47/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.6061 - binary_accuracy: 0.6593 - val_loss: 0.7172 - val_binary_accuracy: 0.5715\n",
      "Epoch 48/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.6059 - binary_accuracy: 0.6593 - val_loss: 0.7164 - val_binary_accuracy: 0.5733\n",
      "Epoch 49/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.6056 - binary_accuracy: 0.6593 - val_loss: 0.7156 - val_binary_accuracy: 0.5745\n",
      "Epoch 50/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.6053 - binary_accuracy: 0.6589 - val_loss: 0.7148 - val_binary_accuracy: 0.5768\n",
      "Epoch 51/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.6051 - binary_accuracy: 0.6589 - val_loss: 0.7140 - val_binary_accuracy: 0.5792\n",
      "Epoch 52/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.6048 - binary_accuracy: 0.6593 - val_loss: 0.7133 - val_binary_accuracy: 0.5803\n",
      "Epoch 53/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.6046 - binary_accuracy: 0.6589 - val_loss: 0.7126 - val_binary_accuracy: 0.5827\n",
      "Epoch 54/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.6043 - binary_accuracy: 0.6593 - val_loss: 0.7120 - val_binary_accuracy: 0.5833\n",
      "Epoch 55/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.6040 - binary_accuracy: 0.6597 - val_loss: 0.7114 - val_binary_accuracy: 0.5839\n",
      "Epoch 56/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.6038 - binary_accuracy: 0.6605 - val_loss: 0.7109 - val_binary_accuracy: 0.5851\n",
      "Epoch 57/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.6035 - binary_accuracy: 0.6609 - val_loss: 0.7104 - val_binary_accuracy: 0.5856\n",
      "Epoch 58/300\n",
      "2548/2548 [==============================] - 0s 12us/step - loss: 0.6033 - binary_accuracy: 0.6613 - val_loss: 0.7099 - val_binary_accuracy: 0.5862\n",
      "Epoch 59/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.6031 - binary_accuracy: 0.6621 - val_loss: 0.7093 - val_binary_accuracy: 0.5880\n",
      "Epoch 60/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.6028 - binary_accuracy: 0.6629 - val_loss: 0.7088 - val_binary_accuracy: 0.5892\n",
      "Epoch 61/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.6026 - binary_accuracy: 0.6637 - val_loss: 0.7082 - val_binary_accuracy: 0.5909\n",
      "Epoch 62/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.6023 - binary_accuracy: 0.6637 - val_loss: 0.7076 - val_binary_accuracy: 0.5927\n",
      "Epoch 63/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.6021 - binary_accuracy: 0.6641 - val_loss: 0.7070 - val_binary_accuracy: 0.5945\n",
      "Epoch 64/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.6019 - binary_accuracy: 0.6641 - val_loss: 0.7063 - val_binary_accuracy: 0.5945\n",
      "Epoch 65/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.6016 - binary_accuracy: 0.6637 - val_loss: 0.7057 - val_binary_accuracy: 0.5968\n",
      "Epoch 66/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.6014 - binary_accuracy: 0.6644 - val_loss: 0.7051 - val_binary_accuracy: 0.5968\n",
      "Epoch 67/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.6012 - binary_accuracy: 0.6644 - val_loss: 0.7046 - val_binary_accuracy: 0.5968\n",
      "Epoch 68/300\n",
      "2548/2548 [==============================] - 0s 12us/step - loss: 0.6010 - binary_accuracy: 0.6648 - val_loss: 0.7040 - val_binary_accuracy: 0.5980\n",
      "Epoch 69/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.6007 - binary_accuracy: 0.6644 - val_loss: 0.7035 - val_binary_accuracy: 0.5992\n",
      "Epoch 70/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.6005 - binary_accuracy: 0.6641 - val_loss: 0.7031 - val_binary_accuracy: 0.6004\n",
      "Epoch 71/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.6003 - binary_accuracy: 0.6641 - val_loss: 0.7026 - val_binary_accuracy: 0.6015\n",
      "Epoch 72/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.6001 - binary_accuracy: 0.6644 - val_loss: 0.7021 - val_binary_accuracy: 0.6015\n",
      "Epoch 73/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5999 - binary_accuracy: 0.6660 - val_loss: 0.7017 - val_binary_accuracy: 0.6027\n",
      "Epoch 74/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5997 - binary_accuracy: 0.6664 - val_loss: 0.7012 - val_binary_accuracy: 0.6027\n",
      "Epoch 75/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5995 - binary_accuracy: 0.6664 - val_loss: 0.7007 - val_binary_accuracy: 0.6033\n",
      "Epoch 76/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5993 - binary_accuracy: 0.6664 - val_loss: 0.7002 - val_binary_accuracy: 0.6039\n",
      "Epoch 77/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5991 - binary_accuracy: 0.6660 - val_loss: 0.6996 - val_binary_accuracy: 0.6045\n",
      "Epoch 78/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5988 - binary_accuracy: 0.6672 - val_loss: 0.6991 - val_binary_accuracy: 0.6057\n",
      "Epoch 79/300\n",
      "2548/2548 [==============================] - 0s 13us/step - loss: 0.5986 - binary_accuracy: 0.6672 - val_loss: 0.6986 - val_binary_accuracy: 0.6068\n",
      "Epoch 80/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5984 - binary_accuracy: 0.6680 - val_loss: 0.6982 - val_binary_accuracy: 0.6086\n",
      "Epoch 81/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5983 - binary_accuracy: 0.6684 - val_loss: 0.6977 - val_binary_accuracy: 0.6086\n",
      "Epoch 82/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5981 - binary_accuracy: 0.6680 - val_loss: 0.6973 - val_binary_accuracy: 0.6104\n",
      "Epoch 83/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5979 - binary_accuracy: 0.6680 - val_loss: 0.6968 - val_binary_accuracy: 0.6109\n",
      "Epoch 84/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5977 - binary_accuracy: 0.6688 - val_loss: 0.6964 - val_binary_accuracy: 0.6109\n",
      "Epoch 85/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5975 - binary_accuracy: 0.6695 - val_loss: 0.6960 - val_binary_accuracy: 0.6127\n",
      "Epoch 86/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5973 - binary_accuracy: 0.6699 - val_loss: 0.6956 - val_binary_accuracy: 0.6157\n",
      "Epoch 87/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5971 - binary_accuracy: 0.6699 - val_loss: 0.6952 - val_binary_accuracy: 0.6162\n",
      "Epoch 88/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5969 - binary_accuracy: 0.6699 - val_loss: 0.6948 - val_binary_accuracy: 0.6174\n",
      "Epoch 89/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5967 - binary_accuracy: 0.6699 - val_loss: 0.6943 - val_binary_accuracy: 0.6186\n",
      "Epoch 90/300\n",
      "2548/2548 [==============================] - 0s 12us/step - loss: 0.5966 - binary_accuracy: 0.6699 - val_loss: 0.6939 - val_binary_accuracy: 0.6192\n",
      "Epoch 91/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5964 - binary_accuracy: 0.6699 - val_loss: 0.6935 - val_binary_accuracy: 0.6192\n",
      "Epoch 92/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5962 - binary_accuracy: 0.6703 - val_loss: 0.6931 - val_binary_accuracy: 0.6204\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 93/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5960 - binary_accuracy: 0.6703 - val_loss: 0.6927 - val_binary_accuracy: 0.6215\n",
      "Epoch 94/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5959 - binary_accuracy: 0.6703 - val_loss: 0.6923 - val_binary_accuracy: 0.6215\n",
      "Epoch 95/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5957 - binary_accuracy: 0.6711 - val_loss: 0.6919 - val_binary_accuracy: 0.6221\n",
      "Epoch 96/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5955 - binary_accuracy: 0.6719 - val_loss: 0.6915 - val_binary_accuracy: 0.6221\n",
      "Epoch 97/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5953 - binary_accuracy: 0.6719 - val_loss: 0.6912 - val_binary_accuracy: 0.6221\n",
      "Epoch 98/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5952 - binary_accuracy: 0.6719 - val_loss: 0.6908 - val_binary_accuracy: 0.6227\n",
      "Epoch 99/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5950 - binary_accuracy: 0.6711 - val_loss: 0.6904 - val_binary_accuracy: 0.6233\n",
      "Epoch 100/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5948 - binary_accuracy: 0.6711 - val_loss: 0.6901 - val_binary_accuracy: 0.6245\n",
      "Epoch 101/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5947 - binary_accuracy: 0.6707 - val_loss: 0.6897 - val_binary_accuracy: 0.6251\n",
      "Epoch 102/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5945 - binary_accuracy: 0.6707 - val_loss: 0.6894 - val_binary_accuracy: 0.6268\n",
      "Epoch 103/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5943 - binary_accuracy: 0.6707 - val_loss: 0.6890 - val_binary_accuracy: 0.6268\n",
      "Epoch 104/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5942 - binary_accuracy: 0.6711 - val_loss: 0.6887 - val_binary_accuracy: 0.6268\n",
      "Epoch 105/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5940 - binary_accuracy: 0.6711 - val_loss: 0.6883 - val_binary_accuracy: 0.6280\n",
      "Epoch 106/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5939 - binary_accuracy: 0.6719 - val_loss: 0.6880 - val_binary_accuracy: 0.6280\n",
      "Epoch 107/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5937 - binary_accuracy: 0.6719 - val_loss: 0.6877 - val_binary_accuracy: 0.6286\n",
      "Epoch 108/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5935 - binary_accuracy: 0.6723 - val_loss: 0.6873 - val_binary_accuracy: 0.6286\n",
      "Epoch 109/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5934 - binary_accuracy: 0.6723 - val_loss: 0.6870 - val_binary_accuracy: 0.6286\n",
      "Epoch 110/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5932 - binary_accuracy: 0.6727 - val_loss: 0.6867 - val_binary_accuracy: 0.6292\n",
      "Epoch 111/300\n",
      "2548/2548 [==============================] - 0s 12us/step - loss: 0.5931 - binary_accuracy: 0.6731 - val_loss: 0.6864 - val_binary_accuracy: 0.6304\n",
      "Epoch 112/300\n",
      "2548/2548 [==============================] - 0s 13us/step - loss: 0.5929 - binary_accuracy: 0.6731 - val_loss: 0.6861 - val_binary_accuracy: 0.6310\n",
      "Epoch 113/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5928 - binary_accuracy: 0.6731 - val_loss: 0.6858 - val_binary_accuracy: 0.6310\n",
      "Epoch 114/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5926 - binary_accuracy: 0.6731 - val_loss: 0.6855 - val_binary_accuracy: 0.6315\n",
      "Epoch 115/300\n",
      "2548/2548 [==============================] - 0s 12us/step - loss: 0.5925 - binary_accuracy: 0.6735 - val_loss: 0.6852 - val_binary_accuracy: 0.6333\n",
      "Epoch 116/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5923 - binary_accuracy: 0.6735 - val_loss: 0.6849 - val_binary_accuracy: 0.6333\n",
      "Epoch 117/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5922 - binary_accuracy: 0.6739 - val_loss: 0.6846 - val_binary_accuracy: 0.6333\n",
      "Epoch 118/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5920 - binary_accuracy: 0.6739 - val_loss: 0.6843 - val_binary_accuracy: 0.6339\n",
      "Epoch 119/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5919 - binary_accuracy: 0.6739 - val_loss: 0.6840 - val_binary_accuracy: 0.6345\n",
      "Epoch 120/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5918 - binary_accuracy: 0.6743 - val_loss: 0.6838 - val_binary_accuracy: 0.6339\n",
      "Epoch 121/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5916 - binary_accuracy: 0.6746 - val_loss: 0.6835 - val_binary_accuracy: 0.6339\n",
      "Epoch 122/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5915 - binary_accuracy: 0.6746 - val_loss: 0.6832 - val_binary_accuracy: 0.6345\n",
      "Epoch 123/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5913 - binary_accuracy: 0.6746 - val_loss: 0.6830 - val_binary_accuracy: 0.6345\n",
      "Epoch 124/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5912 - binary_accuracy: 0.6754 - val_loss: 0.6827 - val_binary_accuracy: 0.6345\n",
      "Epoch 125/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5911 - binary_accuracy: 0.6754 - val_loss: 0.6824 - val_binary_accuracy: 0.6345\n",
      "Epoch 126/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5909 - binary_accuracy: 0.6754 - val_loss: 0.6822 - val_binary_accuracy: 0.6351\n",
      "Epoch 127/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5908 - binary_accuracy: 0.6754 - val_loss: 0.6819 - val_binary_accuracy: 0.6363\n",
      "Epoch 128/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5907 - binary_accuracy: 0.6754 - val_loss: 0.6817 - val_binary_accuracy: 0.6368\n",
      "Epoch 129/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5905 - binary_accuracy: 0.6754 - val_loss: 0.6814 - val_binary_accuracy: 0.6368\n",
      "Epoch 130/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5904 - binary_accuracy: 0.6750 - val_loss: 0.6812 - val_binary_accuracy: 0.6368\n",
      "Epoch 131/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5903 - binary_accuracy: 0.6754 - val_loss: 0.6810 - val_binary_accuracy: 0.6374\n",
      "Epoch 132/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5901 - binary_accuracy: 0.6750 - val_loss: 0.6807 - val_binary_accuracy: 0.6374\n",
      "Epoch 133/300\n",
      "2548/2548 [==============================] - 0s 13us/step - loss: 0.5900 - binary_accuracy: 0.6750 - val_loss: 0.6805 - val_binary_accuracy: 0.6386\n",
      "Epoch 134/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5899 - binary_accuracy: 0.6754 - val_loss: 0.6803 - val_binary_accuracy: 0.6386\n",
      "Epoch 135/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5897 - binary_accuracy: 0.6762 - val_loss: 0.6800 - val_binary_accuracy: 0.6386\n",
      "Epoch 136/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5896 - binary_accuracy: 0.6762 - val_loss: 0.6798 - val_binary_accuracy: 0.6380\n",
      "Epoch 137/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5895 - binary_accuracy: 0.6762 - val_loss: 0.6796 - val_binary_accuracy: 0.6380\n",
      "Epoch 138/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5893 - binary_accuracy: 0.6762 - val_loss: 0.6794 - val_binary_accuracy: 0.6374\n",
      "Epoch 139/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5892 - binary_accuracy: 0.6762 - val_loss: 0.6792 - val_binary_accuracy: 0.6374\n",
      "Epoch 140/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5891 - binary_accuracy: 0.6766 - val_loss: 0.6790 - val_binary_accuracy: 0.6374\n",
      "Epoch 141/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5890 - binary_accuracy: 0.6770 - val_loss: 0.6788 - val_binary_accuracy: 0.6374\n",
      "Epoch 142/300\n",
      "2548/2548 [==============================] - 0s 12us/step - loss: 0.5888 - binary_accuracy: 0.6770 - val_loss: 0.6786 - val_binary_accuracy: 0.6374\n",
      "Epoch 143/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5887 - binary_accuracy: 0.6770 - val_loss: 0.6784 - val_binary_accuracy: 0.6374\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 144/300\n",
      "2548/2548 [==============================] - 0s 12us/step - loss: 0.5886 - binary_accuracy: 0.6770 - val_loss: 0.6782 - val_binary_accuracy: 0.6374\n",
      "Epoch 145/300\n",
      "2548/2548 [==============================] - 0s 12us/step - loss: 0.5885 - binary_accuracy: 0.6770 - val_loss: 0.6780 - val_binary_accuracy: 0.6392\n",
      "Epoch 146/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5884 - binary_accuracy: 0.6778 - val_loss: 0.6778 - val_binary_accuracy: 0.6392\n",
      "Epoch 147/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5882 - binary_accuracy: 0.6778 - val_loss: 0.6776 - val_binary_accuracy: 0.6392\n",
      "Epoch 148/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5881 - binary_accuracy: 0.6778 - val_loss: 0.6774 - val_binary_accuracy: 0.6398\n",
      "Epoch 149/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5880 - binary_accuracy: 0.6778 - val_loss: 0.6772 - val_binary_accuracy: 0.6404\n",
      "Epoch 150/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5879 - binary_accuracy: 0.6782 - val_loss: 0.6771 - val_binary_accuracy: 0.6404\n",
      "Epoch 151/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5878 - binary_accuracy: 0.6782 - val_loss: 0.6769 - val_binary_accuracy: 0.6410\n",
      "Epoch 152/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5876 - binary_accuracy: 0.6782 - val_loss: 0.6767 - val_binary_accuracy: 0.6410\n",
      "Epoch 153/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5875 - binary_accuracy: 0.6778 - val_loss: 0.6765 - val_binary_accuracy: 0.6421\n",
      "Epoch 154/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5874 - binary_accuracy: 0.6782 - val_loss: 0.6764 - val_binary_accuracy: 0.6421\n",
      "Epoch 155/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5873 - binary_accuracy: 0.6782 - val_loss: 0.6762 - val_binary_accuracy: 0.6427\n",
      "Epoch 156/300\n",
      "2548/2548 [==============================] - 0s 14us/step - loss: 0.5872 - binary_accuracy: 0.6786 - val_loss: 0.6761 - val_binary_accuracy: 0.6427\n",
      "Epoch 157/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5871 - binary_accuracy: 0.6786 - val_loss: 0.6759 - val_binary_accuracy: 0.6433\n",
      "Epoch 158/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5870 - binary_accuracy: 0.6790 - val_loss: 0.6757 - val_binary_accuracy: 0.6433\n",
      "Epoch 159/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5868 - binary_accuracy: 0.6790 - val_loss: 0.6756 - val_binary_accuracy: 0.6433\n",
      "Epoch 160/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5867 - binary_accuracy: 0.6794 - val_loss: 0.6754 - val_binary_accuracy: 0.6433\n",
      "Epoch 161/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5866 - binary_accuracy: 0.6790 - val_loss: 0.6753 - val_binary_accuracy: 0.6433\n",
      "Epoch 162/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5865 - binary_accuracy: 0.6790 - val_loss: 0.6751 - val_binary_accuracy: 0.6439\n",
      "Epoch 163/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5864 - binary_accuracy: 0.6790 - val_loss: 0.6750 - val_binary_accuracy: 0.6451\n",
      "Epoch 164/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5863 - binary_accuracy: 0.6790 - val_loss: 0.6748 - val_binary_accuracy: 0.6451\n",
      "Epoch 165/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5862 - binary_accuracy: 0.6790 - val_loss: 0.6747 - val_binary_accuracy: 0.6451\n",
      "Epoch 166/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5861 - binary_accuracy: 0.6786 - val_loss: 0.6746 - val_binary_accuracy: 0.6451\n",
      "Epoch 167/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5860 - binary_accuracy: 0.6786 - val_loss: 0.6744 - val_binary_accuracy: 0.6451\n",
      "Epoch 168/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5859 - binary_accuracy: 0.6790 - val_loss: 0.6743 - val_binary_accuracy: 0.6451\n",
      "Epoch 169/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5857 - binary_accuracy: 0.6790 - val_loss: 0.6742 - val_binary_accuracy: 0.6451\n",
      "Epoch 170/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5856 - binary_accuracy: 0.6790 - val_loss: 0.6740 - val_binary_accuracy: 0.6457\n",
      "Epoch 171/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5855 - binary_accuracy: 0.6790 - val_loss: 0.6739 - val_binary_accuracy: 0.6457\n",
      "Epoch 172/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5854 - binary_accuracy: 0.6786 - val_loss: 0.6738 - val_binary_accuracy: 0.6463\n",
      "Epoch 173/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5853 - binary_accuracy: 0.6786 - val_loss: 0.6737 - val_binary_accuracy: 0.6457\n",
      "Epoch 174/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5852 - binary_accuracy: 0.6790 - val_loss: 0.6735 - val_binary_accuracy: 0.6457\n",
      "Epoch 175/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5851 - binary_accuracy: 0.6790 - val_loss: 0.6734 - val_binary_accuracy: 0.6457\n",
      "Epoch 176/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5850 - binary_accuracy: 0.6790 - val_loss: 0.6733 - val_binary_accuracy: 0.6463\n",
      "Epoch 177/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5849 - binary_accuracy: 0.6794 - val_loss: 0.6732 - val_binary_accuracy: 0.6463\n",
      "Epoch 178/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5848 - binary_accuracy: 0.6794 - val_loss: 0.6731 - val_binary_accuracy: 0.6463\n",
      "Epoch 179/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5847 - binary_accuracy: 0.6794 - val_loss: 0.6730 - val_binary_accuracy: 0.6463\n",
      "Epoch 180/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5846 - binary_accuracy: 0.6794 - val_loss: 0.6729 - val_binary_accuracy: 0.6463\n",
      "Epoch 181/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5845 - binary_accuracy: 0.6794 - val_loss: 0.6728 - val_binary_accuracy: 0.6457\n",
      "Epoch 182/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5844 - binary_accuracy: 0.6794 - val_loss: 0.6727 - val_binary_accuracy: 0.6457\n",
      "Epoch 183/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5843 - binary_accuracy: 0.6794 - val_loss: 0.6726 - val_binary_accuracy: 0.6457\n",
      "Epoch 184/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5842 - binary_accuracy: 0.6794 - val_loss: 0.6725 - val_binary_accuracy: 0.6457\n",
      "Epoch 185/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5841 - binary_accuracy: 0.6790 - val_loss: 0.6724 - val_binary_accuracy: 0.6457\n",
      "Epoch 186/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5840 - binary_accuracy: 0.6790 - val_loss: 0.6723 - val_binary_accuracy: 0.6474\n",
      "Epoch 187/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5839 - binary_accuracy: 0.6797 - val_loss: 0.6722 - val_binary_accuracy: 0.6474\n",
      "Epoch 188/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5838 - binary_accuracy: 0.6805 - val_loss: 0.6721 - val_binary_accuracy: 0.6498\n",
      "Epoch 189/300\n",
      "2548/2548 [==============================] - 0s 14us/step - loss: 0.5837 - binary_accuracy: 0.6805 - val_loss: 0.6720 - val_binary_accuracy: 0.6504\n",
      "Epoch 190/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5836 - binary_accuracy: 0.6805 - val_loss: 0.6719 - val_binary_accuracy: 0.6504\n",
      "Epoch 191/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5835 - binary_accuracy: 0.6805 - val_loss: 0.6718 - val_binary_accuracy: 0.6504\n",
      "Epoch 192/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5834 - binary_accuracy: 0.6809 - val_loss: 0.6717 - val_binary_accuracy: 0.6504\n",
      "Epoch 193/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5833 - binary_accuracy: 0.6809 - val_loss: 0.6717 - val_binary_accuracy: 0.6510\n",
      "Epoch 194/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5832 - binary_accuracy: 0.6809 - val_loss: 0.6716 - val_binary_accuracy: 0.6504\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 195/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5831 - binary_accuracy: 0.6805 - val_loss: 0.6715 - val_binary_accuracy: 0.6504\n",
      "Epoch 196/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5830 - binary_accuracy: 0.6809 - val_loss: 0.6714 - val_binary_accuracy: 0.6504\n",
      "Epoch 197/300\n",
      "2548/2548 [==============================] - 0s 13us/step - loss: 0.5829 - binary_accuracy: 0.6809 - val_loss: 0.6713 - val_binary_accuracy: 0.6516\n",
      "Epoch 198/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5828 - binary_accuracy: 0.6813 - val_loss: 0.6713 - val_binary_accuracy: 0.6521\n",
      "Epoch 199/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5827 - binary_accuracy: 0.6813 - val_loss: 0.6712 - val_binary_accuracy: 0.6539\n",
      "Epoch 200/300\n",
      "2548/2548 [==============================] - 0s 13us/step - loss: 0.5826 - binary_accuracy: 0.6817 - val_loss: 0.6711 - val_binary_accuracy: 0.6539\n",
      "Epoch 201/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5825 - binary_accuracy: 0.6821 - val_loss: 0.6710 - val_binary_accuracy: 0.6545\n",
      "Epoch 202/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5824 - binary_accuracy: 0.6821 - val_loss: 0.6710 - val_binary_accuracy: 0.6545\n",
      "Epoch 203/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5823 - binary_accuracy: 0.6817 - val_loss: 0.6709 - val_binary_accuracy: 0.6551\n",
      "Epoch 204/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5822 - binary_accuracy: 0.6829 - val_loss: 0.6708 - val_binary_accuracy: 0.6551\n",
      "Epoch 205/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5822 - binary_accuracy: 0.6829 - val_loss: 0.6708 - val_binary_accuracy: 0.6551\n",
      "Epoch 206/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5821 - binary_accuracy: 0.6833 - val_loss: 0.6707 - val_binary_accuracy: 0.6551\n",
      "Epoch 207/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5820 - binary_accuracy: 0.6833 - val_loss: 0.6707 - val_binary_accuracy: 0.6551\n",
      "Epoch 208/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5819 - binary_accuracy: 0.6833 - val_loss: 0.6706 - val_binary_accuracy: 0.6563\n",
      "Epoch 209/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5818 - binary_accuracy: 0.6833 - val_loss: 0.6705 - val_binary_accuracy: 0.6563\n",
      "Epoch 210/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5817 - binary_accuracy: 0.6833 - val_loss: 0.6705 - val_binary_accuracy: 0.6563\n",
      "Epoch 211/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5816 - binary_accuracy: 0.6833 - val_loss: 0.6704 - val_binary_accuracy: 0.6563\n",
      "Epoch 212/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5815 - binary_accuracy: 0.6837 - val_loss: 0.6704 - val_binary_accuracy: 0.6551\n",
      "Epoch 213/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5814 - binary_accuracy: 0.6833 - val_loss: 0.6703 - val_binary_accuracy: 0.6551\n",
      "Epoch 214/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5813 - binary_accuracy: 0.6833 - val_loss: 0.6703 - val_binary_accuracy: 0.6551\n",
      "Epoch 215/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5812 - binary_accuracy: 0.6833 - val_loss: 0.6702 - val_binary_accuracy: 0.6545\n",
      "Epoch 216/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5811 - binary_accuracy: 0.6833 - val_loss: 0.6702 - val_binary_accuracy: 0.6545\n",
      "Epoch 217/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5811 - binary_accuracy: 0.6833 - val_loss: 0.6701 - val_binary_accuracy: 0.6545\n",
      "Epoch 218/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5810 - binary_accuracy: 0.6833 - val_loss: 0.6701 - val_binary_accuracy: 0.6545\n",
      "Epoch 219/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5809 - binary_accuracy: 0.6833 - val_loss: 0.6700 - val_binary_accuracy: 0.6551\n",
      "Epoch 220/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5808 - binary_accuracy: 0.6833 - val_loss: 0.6700 - val_binary_accuracy: 0.6557\n",
      "Epoch 221/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5807 - binary_accuracy: 0.6833 - val_loss: 0.6699 - val_binary_accuracy: 0.6557\n",
      "Epoch 222/300\n",
      "2548/2548 [==============================] - 0s 12us/step - loss: 0.5806 - binary_accuracy: 0.6833 - val_loss: 0.6699 - val_binary_accuracy: 0.6557\n",
      "Epoch 223/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5805 - binary_accuracy: 0.6829 - val_loss: 0.6699 - val_binary_accuracy: 0.6551\n",
      "Epoch 224/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5804 - binary_accuracy: 0.6829 - val_loss: 0.6698 - val_binary_accuracy: 0.6545\n",
      "Epoch 225/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5804 - binary_accuracy: 0.6829 - val_loss: 0.6698 - val_binary_accuracy: 0.6551\n",
      "Epoch 226/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5803 - binary_accuracy: 0.6829 - val_loss: 0.6698 - val_binary_accuracy: 0.6557\n",
      "Epoch 227/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5802 - binary_accuracy: 0.6829 - val_loss: 0.6697 - val_binary_accuracy: 0.6557\n",
      "Epoch 228/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5801 - binary_accuracy: 0.6829 - val_loss: 0.6697 - val_binary_accuracy: 0.6557\n",
      "Epoch 229/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5800 - binary_accuracy: 0.6833 - val_loss: 0.6697 - val_binary_accuracy: 0.6563\n",
      "Epoch 230/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5799 - binary_accuracy: 0.6837 - val_loss: 0.6696 - val_binary_accuracy: 0.6563\n",
      "Epoch 231/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5798 - binary_accuracy: 0.6841 - val_loss: 0.6696 - val_binary_accuracy: 0.6574\n",
      "Epoch 232/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5797 - binary_accuracy: 0.6841 - val_loss: 0.6696 - val_binary_accuracy: 0.6574\n",
      "Epoch 233/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5797 - binary_accuracy: 0.6841 - val_loss: 0.6695 - val_binary_accuracy: 0.6574\n",
      "Epoch 234/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5796 - binary_accuracy: 0.6841 - val_loss: 0.6695 - val_binary_accuracy: 0.6574\n",
      "Epoch 235/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5795 - binary_accuracy: 0.6849 - val_loss: 0.6695 - val_binary_accuracy: 0.6574\n",
      "Epoch 236/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5794 - binary_accuracy: 0.6849 - val_loss: 0.6695 - val_binary_accuracy: 0.6580\n",
      "Epoch 237/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5793 - binary_accuracy: 0.6849 - val_loss: 0.6694 - val_binary_accuracy: 0.6580\n",
      "Epoch 238/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5792 - binary_accuracy: 0.6849 - val_loss: 0.6694 - val_binary_accuracy: 0.6580\n",
      "Epoch 239/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5792 - binary_accuracy: 0.6852 - val_loss: 0.6694 - val_binary_accuracy: 0.6574\n",
      "Epoch 240/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5791 - binary_accuracy: 0.6852 - val_loss: 0.6694 - val_binary_accuracy: 0.6574\n",
      "Epoch 241/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5790 - binary_accuracy: 0.6852 - val_loss: 0.6693 - val_binary_accuracy: 0.6569\n",
      "Epoch 242/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5789 - binary_accuracy: 0.6849 - val_loss: 0.6693 - val_binary_accuracy: 0.6569\n",
      "Epoch 243/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5788 - binary_accuracy: 0.6852 - val_loss: 0.6693 - val_binary_accuracy: 0.6574\n",
      "Epoch 244/300\n",
      "2548/2548 [==============================] - 0s 12us/step - loss: 0.5787 - binary_accuracy: 0.6856 - val_loss: 0.6693 - val_binary_accuracy: 0.6580\n",
      "Epoch 245/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5787 - binary_accuracy: 0.6860 - val_loss: 0.6693 - val_binary_accuracy: 0.6580\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 246/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5786 - binary_accuracy: 0.6860 - val_loss: 0.6693 - val_binary_accuracy: 0.6580\n",
      "Epoch 247/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5785 - binary_accuracy: 0.6860 - val_loss: 0.6692 - val_binary_accuracy: 0.6580\n",
      "Epoch 248/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5784 - binary_accuracy: 0.6860 - val_loss: 0.6692 - val_binary_accuracy: 0.6580\n",
      "Epoch 249/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5783 - binary_accuracy: 0.6860 - val_loss: 0.6692 - val_binary_accuracy: 0.6580\n",
      "Epoch 250/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5782 - binary_accuracy: 0.6864 - val_loss: 0.6692 - val_binary_accuracy: 0.6580\n",
      "Epoch 251/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5782 - binary_accuracy: 0.6868 - val_loss: 0.6692 - val_binary_accuracy: 0.6586\n",
      "Epoch 252/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5781 - binary_accuracy: 0.6864 - val_loss: 0.6692 - val_binary_accuracy: 0.6586\n",
      "Epoch 253/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5780 - binary_accuracy: 0.6868 - val_loss: 0.6692 - val_binary_accuracy: 0.6586\n",
      "Epoch 254/300\n",
      "2548/2548 [==============================] - 0s 13us/step - loss: 0.5779 - binary_accuracy: 0.6868 - val_loss: 0.6692 - val_binary_accuracy: 0.6598\n",
      "Epoch 255/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5778 - binary_accuracy: 0.6872 - val_loss: 0.6692 - val_binary_accuracy: 0.6598\n",
      "Epoch 256/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5778 - binary_accuracy: 0.6872 - val_loss: 0.6692 - val_binary_accuracy: 0.6598\n",
      "Epoch 257/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5777 - binary_accuracy: 0.6876 - val_loss: 0.6691 - val_binary_accuracy: 0.6598\n",
      "Epoch 258/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5776 - binary_accuracy: 0.6872 - val_loss: 0.6691 - val_binary_accuracy: 0.6598\n",
      "Epoch 259/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5775 - binary_accuracy: 0.6872 - val_loss: 0.6691 - val_binary_accuracy: 0.6592\n",
      "Epoch 260/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5774 - binary_accuracy: 0.6872 - val_loss: 0.6691 - val_binary_accuracy: 0.6598\n",
      "Epoch 261/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5774 - binary_accuracy: 0.6872 - val_loss: 0.6691 - val_binary_accuracy: 0.6598\n",
      "Epoch 262/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5773 - binary_accuracy: 0.6876 - val_loss: 0.6691 - val_binary_accuracy: 0.6598\n",
      "Epoch 263/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5772 - binary_accuracy: 0.6876 - val_loss: 0.6691 - val_binary_accuracy: 0.6598\n",
      "Epoch 264/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5771 - binary_accuracy: 0.6876 - val_loss: 0.6691 - val_binary_accuracy: 0.6604\n",
      "Epoch 265/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5771 - binary_accuracy: 0.6880 - val_loss: 0.6691 - val_binary_accuracy: 0.6616\n",
      "Epoch 266/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5770 - binary_accuracy: 0.6880 - val_loss: 0.6691 - val_binary_accuracy: 0.6616\n",
      "Train on 2548 samples, validate on 1699 samples\n",
      "Epoch 1/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5769 - binary_accuracy: 0.6880 - val_loss: 0.6691 - val_binary_accuracy: 0.6616\n",
      "Epoch 2/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5768 - binary_accuracy: 0.6884 - val_loss: 0.6691 - val_binary_accuracy: 0.6627\n",
      "Train on 2548 samples, validate on 1699 samples\n",
      "Epoch 1/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5767 - binary_accuracy: 0.6880 - val_loss: 0.6691 - val_binary_accuracy: 0.6633\n",
      "Epoch 2/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5767 - binary_accuracy: 0.6880 - val_loss: 0.6691 - val_binary_accuracy: 0.6622\n",
      "Train on 2548 samples, validate on 1699 samples\n",
      "Epoch 1/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5766 - binary_accuracy: 0.6880 - val_loss: 0.6691 - val_binary_accuracy: 0.6627\n",
      "Epoch 2/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5765 - binary_accuracy: 0.6884 - val_loss: 0.6691 - val_binary_accuracy: 0.6627\n",
      "Train on 2548 samples, validate on 1699 samples\n",
      "Epoch 1/300\n",
      "2548/2548 [==============================] - 0s 10us/step - loss: 0.5764 - binary_accuracy: 0.6884 - val_loss: 0.6691 - val_binary_accuracy: 0.6616\n",
      "Epoch 2/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5764 - binary_accuracy: 0.6884 - val_loss: 0.6692 - val_binary_accuracy: 0.6616\n",
      "Train on 2548 samples, validate on 1699 samples\n",
      "Epoch 1/300\n",
      "2548/2548 [==============================] - 0s 10us/step - loss: 0.5763 - binary_accuracy: 0.6888 - val_loss: 0.6692 - val_binary_accuracy: 0.6616\n",
      "Epoch 2/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5762 - binary_accuracy: 0.6896 - val_loss: 0.6692 - val_binary_accuracy: 0.6622\n",
      "Train on 2548 samples, validate on 1699 samples\n",
      "Epoch 1/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5761 - binary_accuracy: 0.6896 - val_loss: 0.6692 - val_binary_accuracy: 0.6622\n",
      "Epoch 2/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5761 - binary_accuracy: 0.6896 - val_loss: 0.6692 - val_binary_accuracy: 0.6616\n",
      "Train on 2548 samples, validate on 1699 samples\n",
      "Epoch 1/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5760 - binary_accuracy: 0.6900 - val_loss: 0.6692 - val_binary_accuracy: 0.6616\n",
      "Epoch 2/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5759 - binary_accuracy: 0.6900 - val_loss: 0.6692 - val_binary_accuracy: 0.6616\n",
      "Train on 2548 samples, validate on 1699 samples\n",
      "Epoch 1/300\n",
      "2548/2548 [==============================] - 0s 10us/step - loss: 0.5758 - binary_accuracy: 0.6903 - val_loss: 0.6692 - val_binary_accuracy: 0.6616\n",
      "Epoch 2/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5758 - binary_accuracy: 0.6907 - val_loss: 0.6692 - val_binary_accuracy: 0.6616\n",
      "Train on 2548 samples, validate on 1699 samples\n",
      "Epoch 1/300\n",
      "2548/2548 [==============================] - 0s 10us/step - loss: 0.5757 - binary_accuracy: 0.6907 - val_loss: 0.6692 - val_binary_accuracy: 0.6616\n",
      "Epoch 2/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5756 - binary_accuracy: 0.6911 - val_loss: 0.6692 - val_binary_accuracy: 0.6622\n",
      "Train on 2548 samples, validate on 1699 samples\n",
      "Epoch 1/300\n",
      "2548/2548 [==============================] - 0s 10us/step - loss: 0.5755 - binary_accuracy: 0.6911 - val_loss: 0.6693 - val_binary_accuracy: 0.6622\n",
      "Epoch 2/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5755 - binary_accuracy: 0.6907 - val_loss: 0.6693 - val_binary_accuracy: 0.6622\n",
      "Train on 2548 samples, validate on 1699 samples\n",
      "Epoch 1/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5754 - binary_accuracy: 0.6907 - val_loss: 0.6693 - val_binary_accuracy: 0.6622\n",
      "Epoch 2/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5753 - binary_accuracy: 0.6903 - val_loss: 0.6693 - val_binary_accuracy: 0.6622\n",
      "Train on 2548 samples, validate on 1699 samples\n",
      "Epoch 1/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5752 - binary_accuracy: 0.6900 - val_loss: 0.6693 - val_binary_accuracy: 0.6622\n",
      "Epoch 2/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5752 - binary_accuracy: 0.6900 - val_loss: 0.6693 - val_binary_accuracy: 0.6622\n",
      "Train on 2548 samples, validate on 1699 samples\n",
      "Epoch 1/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5751 - binary_accuracy: 0.6903 - val_loss: 0.6693 - val_binary_accuracy: 0.6627\n",
      "Epoch 2/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5750 - binary_accuracy: 0.6896 - val_loss: 0.6694 - val_binary_accuracy: 0.6627\n",
      "Train on 2548 samples, validate on 1699 samples\n",
      "Epoch 1/300\n",
      "2548/2548 [==============================] - 0s 10us/step - loss: 0.5749 - binary_accuracy: 0.6896 - val_loss: 0.6694 - val_binary_accuracy: 0.6627\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5749 - binary_accuracy: 0.6896 - val_loss: 0.6694 - val_binary_accuracy: 0.6622\n",
      "Train on 2548 samples, validate on 1699 samples\n",
      "Epoch 1/300\n",
      "2548/2548 [==============================] - 0s 10us/step - loss: 0.5748 - binary_accuracy: 0.6892 - val_loss: 0.6694 - val_binary_accuracy: 0.6627\n",
      "Epoch 2/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5747 - binary_accuracy: 0.6888 - val_loss: 0.6694 - val_binary_accuracy: 0.6627\n",
      "Train on 2548 samples, validate on 1699 samples\n",
      "Epoch 1/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5747 - binary_accuracy: 0.6892 - val_loss: 0.6694 - val_binary_accuracy: 0.6627\n",
      "Epoch 2/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5746 - binary_accuracy: 0.6892 - val_loss: 0.6695 - val_binary_accuracy: 0.6627\n",
      "Train on 2548 samples, validate on 1699 samples\n",
      "Epoch 1/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5745 - binary_accuracy: 0.6892 - val_loss: 0.6695 - val_binary_accuracy: 0.6633\n",
      "Epoch 2/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5744 - binary_accuracy: 0.6896 - val_loss: 0.6695 - val_binary_accuracy: 0.6639\n",
      "Train on 2548 samples, validate on 1699 samples\n",
      "Epoch 1/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5744 - binary_accuracy: 0.6896 - val_loss: 0.6695 - val_binary_accuracy: 0.6645\n",
      "Epoch 2/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5743 - binary_accuracy: 0.6896 - val_loss: 0.6695 - val_binary_accuracy: 0.6651\n",
      "Train on 2548 samples, validate on 1699 samples\n",
      "Epoch 1/300\n",
      "2548/2548 [==============================] - 0s 12us/step - loss: 0.5742 - binary_accuracy: 0.6896 - val_loss: 0.6696 - val_binary_accuracy: 0.6651\n",
      "Epoch 2/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5742 - binary_accuracy: 0.6896 - val_loss: 0.6696 - val_binary_accuracy: 0.6657\n",
      "Train on 2548 samples, validate on 1699 samples\n",
      "Epoch 1/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5741 - binary_accuracy: 0.6892 - val_loss: 0.6696 - val_binary_accuracy: 0.6663\n",
      "Epoch 2/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5740 - binary_accuracy: 0.6900 - val_loss: 0.6696 - val_binary_accuracy: 0.6680\n",
      "Train on 2548 samples, validate on 1699 samples\n",
      "Epoch 1/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5739 - binary_accuracy: 0.6900 - val_loss: 0.6697 - val_binary_accuracy: 0.6680\n",
      "Epoch 2/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5739 - binary_accuracy: 0.6896 - val_loss: 0.6697 - val_binary_accuracy: 0.6680\n",
      "Train on 2548 samples, validate on 1699 samples\n",
      "Epoch 1/300\n",
      "2548/2548 [==============================] - 0s 10us/step - loss: 0.5738 - binary_accuracy: 0.6900 - val_loss: 0.6697 - val_binary_accuracy: 0.6686\n",
      "Epoch 2/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5737 - binary_accuracy: 0.6903 - val_loss: 0.6697 - val_binary_accuracy: 0.6686\n",
      "Train on 2548 samples, validate on 1699 samples\n",
      "Epoch 1/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5737 - binary_accuracy: 0.6900 - val_loss: 0.6697 - val_binary_accuracy: 0.6686\n",
      "Epoch 2/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5736 - binary_accuracy: 0.6903 - val_loss: 0.6698 - val_binary_accuracy: 0.6686\n",
      "Train on 2548 samples, validate on 1699 samples\n",
      "Epoch 1/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5735 - binary_accuracy: 0.6903 - val_loss: 0.6698 - val_binary_accuracy: 0.6686\n",
      "Epoch 2/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5735 - binary_accuracy: 0.6903 - val_loss: 0.6698 - val_binary_accuracy: 0.6680\n",
      "Train on 2548 samples, validate on 1699 samples\n",
      "Epoch 1/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5734 - binary_accuracy: 0.6903 - val_loss: 0.6698 - val_binary_accuracy: 0.6680\n",
      "Epoch 2/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5733 - binary_accuracy: 0.6903 - val_loss: 0.6699 - val_binary_accuracy: 0.6675\n",
      "Train on 2548 samples, validate on 1699 samples\n",
      "Epoch 1/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5733 - binary_accuracy: 0.6907 - val_loss: 0.6699 - val_binary_accuracy: 0.6675\n",
      "Epoch 2/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5732 - binary_accuracy: 0.6907 - val_loss: 0.6699 - val_binary_accuracy: 0.6669\n",
      "Train on 2548 samples, validate on 1699 samples\n",
      "Epoch 1/300\n",
      "2548/2548 [==============================] - 0s 10us/step - loss: 0.5731 - binary_accuracy: 0.6911 - val_loss: 0.6700 - val_binary_accuracy: 0.6669\n",
      "Epoch 2/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5730 - binary_accuracy: 0.6907 - val_loss: 0.6700 - val_binary_accuracy: 0.6669\n",
      "Train on 2548 samples, validate on 1699 samples\n",
      "Epoch 1/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5730 - binary_accuracy: 0.6907 - val_loss: 0.6700 - val_binary_accuracy: 0.6669\n",
      "Epoch 2/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5729 - binary_accuracy: 0.6907 - val_loss: 0.6700 - val_binary_accuracy: 0.6669\n",
      "Train on 2548 samples, validate on 1699 samples\n",
      "Epoch 1/300\n",
      "2548/2548 [==============================] - 0s 10us/step - loss: 0.5728 - binary_accuracy: 0.6907 - val_loss: 0.6701 - val_binary_accuracy: 0.6669\n",
      "Epoch 2/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5728 - binary_accuracy: 0.6907 - val_loss: 0.6701 - val_binary_accuracy: 0.6663\n",
      "Train on 2548 samples, validate on 1699 samples\n",
      "Epoch 1/300\n",
      "2548/2548 [==============================] - 0s 10us/step - loss: 0.5727 - binary_accuracy: 0.6907 - val_loss: 0.6701 - val_binary_accuracy: 0.6657\n",
      "Epoch 2/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5726 - binary_accuracy: 0.6903 - val_loss: 0.6702 - val_binary_accuracy: 0.6657\n",
      "Train on 2548 samples, validate on 1699 samples\n",
      "Epoch 1/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5726 - binary_accuracy: 0.6903 - val_loss: 0.6702 - val_binary_accuracy: 0.6657\n",
      "Epoch 2/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5725 - binary_accuracy: 0.6900 - val_loss: 0.6702 - val_binary_accuracy: 0.6651\n",
      "Train on 2548 samples, validate on 1699 samples\n",
      "Epoch 1/300\n",
      "2548/2548 [==============================] - 0s 10us/step - loss: 0.5724 - binary_accuracy: 0.6896 - val_loss: 0.6702 - val_binary_accuracy: 0.6645\n",
      "Epoch 2/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5724 - binary_accuracy: 0.6896 - val_loss: 0.6703 - val_binary_accuracy: 0.6645\n",
      "Train on 2548 samples, validate on 1699 samples\n",
      "Epoch 1/300\n",
      "2548/2548 [==============================] - 0s 10us/step - loss: 0.5723 - binary_accuracy: 0.6900 - val_loss: 0.6703 - val_binary_accuracy: 0.6651\n",
      "Epoch 2/300\n",
      "2548/2548 [==============================] - 0s 10us/step - loss: 0.5722 - binary_accuracy: 0.6896 - val_loss: 0.6703 - val_binary_accuracy: 0.6651\n",
      "Train on 2548 samples, validate on 1699 samples\n",
      "Epoch 1/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5722 - binary_accuracy: 0.6896 - val_loss: 0.6704 - val_binary_accuracy: 0.6651\n",
      "Epoch 2/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5721 - binary_accuracy: 0.6896 - val_loss: 0.6704 - val_binary_accuracy: 0.6651\n",
      "Train on 2548 samples, validate on 1699 samples\n",
      "Epoch 1/300\n",
      "2548/2548 [==============================] - 0s 10us/step - loss: 0.5720 - binary_accuracy: 0.6896 - val_loss: 0.6704 - val_binary_accuracy: 0.6651\n",
      "Epoch 2/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5720 - binary_accuracy: 0.6896 - val_loss: 0.6705 - val_binary_accuracy: 0.6651\n",
      "Train on 2548 samples, validate on 1699 samples\n",
      "Epoch 1/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5719 - binary_accuracy: 0.6896 - val_loss: 0.6705 - val_binary_accuracy: 0.6651\n",
      "Epoch 2/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5718 - binary_accuracy: 0.6892 - val_loss: 0.6705 - val_binary_accuracy: 0.6651\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2548 samples, validate on 1699 samples\n",
      "Epoch 1/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5718 - binary_accuracy: 0.6896 - val_loss: 0.6706 - val_binary_accuracy: 0.6645\n",
      "Epoch 2/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5717 - binary_accuracy: 0.6896 - val_loss: 0.6706 - val_binary_accuracy: 0.6645\n",
      "Train on 2548 samples, validate on 1699 samples\n",
      "Epoch 1/300\n",
      "2548/2548 [==============================] - 0s 10us/step - loss: 0.5717 - binary_accuracy: 0.6896 - val_loss: 0.6706 - val_binary_accuracy: 0.6639\n",
      "Epoch 2/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5716 - binary_accuracy: 0.6900 - val_loss: 0.6707 - val_binary_accuracy: 0.6639\n",
      "Train on 2548 samples, validate on 1699 samples\n",
      "Epoch 1/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5715 - binary_accuracy: 0.6900 - val_loss: 0.6707 - val_binary_accuracy: 0.6639\n",
      "Epoch 2/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5715 - binary_accuracy: 0.6900 - val_loss: 0.6707 - val_binary_accuracy: 0.6639\n",
      "Train on 2548 samples, validate on 1699 samples\n",
      "Epoch 1/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5714 - binary_accuracy: 0.6896 - val_loss: 0.6708 - val_binary_accuracy: 0.6645\n",
      "Epoch 2/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5713 - binary_accuracy: 0.6900 - val_loss: 0.6708 - val_binary_accuracy: 0.6651\n",
      "Train on 2548 samples, validate on 1699 samples\n",
      "Epoch 1/300\n",
      "2548/2548 [==============================] - 0s 10us/step - loss: 0.5713 - binary_accuracy: 0.6900 - val_loss: 0.6708 - val_binary_accuracy: 0.6651\n",
      "Epoch 2/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5712 - binary_accuracy: 0.6900 - val_loss: 0.6709 - val_binary_accuracy: 0.6645\n",
      "Train on 2548 samples, validate on 1699 samples\n",
      "Epoch 1/300\n",
      "2548/2548 [==============================] - 0s 10us/step - loss: 0.5711 - binary_accuracy: 0.6900 - val_loss: 0.6709 - val_binary_accuracy: 0.6651\n",
      "Epoch 2/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5711 - binary_accuracy: 0.6896 - val_loss: 0.6709 - val_binary_accuracy: 0.6651\n",
      "Train on 2548 samples, validate on 1699 samples\n",
      "Epoch 1/300\n",
      "2548/2548 [==============================] - 0s 10us/step - loss: 0.5710 - binary_accuracy: 0.6896 - val_loss: 0.6710 - val_binary_accuracy: 0.6651\n",
      "Epoch 2/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5709 - binary_accuracy: 0.6900 - val_loss: 0.6710 - val_binary_accuracy: 0.6651\n",
      "Train on 2548 samples, validate on 1699 samples\n",
      "Epoch 1/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5709 - binary_accuracy: 0.6900 - val_loss: 0.6710 - val_binary_accuracy: 0.6657\n",
      "Epoch 2/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5708 - binary_accuracy: 0.6903 - val_loss: 0.6711 - val_binary_accuracy: 0.6657\n",
      "Train on 2548 samples, validate on 1699 samples\n",
      "Epoch 1/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5708 - binary_accuracy: 0.6907 - val_loss: 0.6711 - val_binary_accuracy: 0.6651\n",
      "Epoch 2/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5707 - binary_accuracy: 0.6903 - val_loss: 0.6711 - val_binary_accuracy: 0.6657\n",
      "Train on 2548 samples, validate on 1699 samples\n",
      "Epoch 1/300\n",
      "2548/2548 [==============================] - 0s 10us/step - loss: 0.5706 - binary_accuracy: 0.6907 - val_loss: 0.6712 - val_binary_accuracy: 0.6663\n",
      "Epoch 2/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5706 - binary_accuracy: 0.6907 - val_loss: 0.6712 - val_binary_accuracy: 0.6663\n",
      "Train on 2548 samples, validate on 1699 samples\n",
      "Epoch 1/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5705 - binary_accuracy: 0.6907 - val_loss: 0.6713 - val_binary_accuracy: 0.6657\n",
      "Epoch 2/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5704 - binary_accuracy: 0.6911 - val_loss: 0.6713 - val_binary_accuracy: 0.6657\n",
      "Train on 2548 samples, validate on 1699 samples\n",
      "Epoch 1/300\n",
      "2548/2548 [==============================] - 0s 10us/step - loss: 0.5704 - binary_accuracy: 0.6915 - val_loss: 0.6713 - val_binary_accuracy: 0.6651\n",
      "Epoch 2/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5703 - binary_accuracy: 0.6919 - val_loss: 0.6714 - val_binary_accuracy: 0.6651\n",
      "Train on 2548 samples, validate on 1699 samples\n",
      "Epoch 1/300\n",
      "2548/2548 [==============================] - 0s 10us/step - loss: 0.5703 - binary_accuracy: 0.6923 - val_loss: 0.6714 - val_binary_accuracy: 0.6651\n",
      "Epoch 2/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5702 - binary_accuracy: 0.6923 - val_loss: 0.6714 - val_binary_accuracy: 0.6651\n",
      "Train on 2548 samples, validate on 1699 samples\n",
      "Epoch 1/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5701 - binary_accuracy: 0.6923 - val_loss: 0.6715 - val_binary_accuracy: 0.6651\n",
      "Epoch 2/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5701 - binary_accuracy: 0.6931 - val_loss: 0.6715 - val_binary_accuracy: 0.6657\n",
      "Train on 2548 samples, validate on 1699 samples\n",
      "Epoch 1/300\n",
      "2548/2548 [==============================] - 0s 10us/step - loss: 0.5700 - binary_accuracy: 0.6931 - val_loss: 0.6716 - val_binary_accuracy: 0.6657\n",
      "Epoch 2/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5700 - binary_accuracy: 0.6931 - val_loss: 0.6716 - val_binary_accuracy: 0.6657\n",
      "Train on 2548 samples, validate on 1699 samples\n",
      "Epoch 1/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5699 - binary_accuracy: 0.6935 - val_loss: 0.6716 - val_binary_accuracy: 0.6657\n",
      "Epoch 2/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5698 - binary_accuracy: 0.6935 - val_loss: 0.6717 - val_binary_accuracy: 0.6657\n",
      "Train on 2548 samples, validate on 1699 samples\n",
      "Epoch 1/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5698 - binary_accuracy: 0.6939 - val_loss: 0.6717 - val_binary_accuracy: 0.6657\n",
      "Epoch 2/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5697 - binary_accuracy: 0.6954 - val_loss: 0.6717 - val_binary_accuracy: 0.6663\n",
      "Train on 2548 samples, validate on 1699 samples\n",
      "Epoch 1/300\n",
      "2548/2548 [==============================] - 0s 10us/step - loss: 0.5696 - binary_accuracy: 0.6958 - val_loss: 0.6718 - val_binary_accuracy: 0.6663\n",
      "Epoch 2/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5696 - binary_accuracy: 0.6958 - val_loss: 0.6718 - val_binary_accuracy: 0.6669\n",
      "Train on 2548 samples, validate on 1699 samples\n",
      "Epoch 1/300\n",
      "2548/2548 [==============================] - 0s 14us/step - loss: 0.5695 - binary_accuracy: 0.6954 - val_loss: 0.6719 - val_binary_accuracy: 0.6675\n",
      "Epoch 2/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5695 - binary_accuracy: 0.6954 - val_loss: 0.6719 - val_binary_accuracy: 0.6675\n",
      "Train on 2548 samples, validate on 1699 samples\n",
      "Epoch 1/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5694 - binary_accuracy: 0.6954 - val_loss: 0.6719 - val_binary_accuracy: 0.6675\n",
      "Epoch 2/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5693 - binary_accuracy: 0.6954 - val_loss: 0.6720 - val_binary_accuracy: 0.6675\n",
      "Train on 2548 samples, validate on 1699 samples\n",
      "Epoch 1/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5693 - binary_accuracy: 0.6954 - val_loss: 0.6720 - val_binary_accuracy: 0.6669\n",
      "Epoch 2/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5692 - binary_accuracy: 0.6954 - val_loss: 0.6721 - val_binary_accuracy: 0.6669\n",
      "Train on 2548 samples, validate on 1699 samples\n",
      "Epoch 1/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5692 - binary_accuracy: 0.6951 - val_loss: 0.6721 - val_binary_accuracy: 0.6680\n",
      "Epoch 2/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5691 - binary_accuracy: 0.6951 - val_loss: 0.6721 - val_binary_accuracy: 0.6680\n",
      "Train on 2548 samples, validate on 1699 samples\n",
      "Epoch 1/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5690 - binary_accuracy: 0.6951 - val_loss: 0.6722 - val_binary_accuracy: 0.6680\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5690 - binary_accuracy: 0.6951 - val_loss: 0.6722 - val_binary_accuracy: 0.6686\n",
      "Train on 2548 samples, validate on 1699 samples\n",
      "Epoch 1/300\n",
      "2548/2548 [==============================] - 0s 10us/step - loss: 0.5689 - binary_accuracy: 0.6951 - val_loss: 0.6723 - val_binary_accuracy: 0.6680\n",
      "Epoch 2/300\n",
      "2548/2548 [==============================] - 0s 14us/step - loss: 0.5689 - binary_accuracy: 0.6943 - val_loss: 0.6723 - val_binary_accuracy: 0.6680\n",
      "Train on 2548 samples, validate on 1699 samples\n",
      "Epoch 1/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5688 - binary_accuracy: 0.6943 - val_loss: 0.6723 - val_binary_accuracy: 0.6680\n",
      "Epoch 2/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5688 - binary_accuracy: 0.6943 - val_loss: 0.6724 - val_binary_accuracy: 0.6680\n",
      "Train on 2548 samples, validate on 1699 samples\n",
      "Epoch 1/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5687 - binary_accuracy: 0.6943 - val_loss: 0.6724 - val_binary_accuracy: 0.6680\n",
      "Epoch 2/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5686 - binary_accuracy: 0.6951 - val_loss: 0.6725 - val_binary_accuracy: 0.6669\n",
      "Train on 2548 samples, validate on 1699 samples\n",
      "Epoch 1/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5686 - binary_accuracy: 0.6954 - val_loss: 0.6725 - val_binary_accuracy: 0.6669\n",
      "Epoch 2/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5685 - binary_accuracy: 0.6954 - val_loss: 0.6725 - val_binary_accuracy: 0.6669\n",
      "Train on 2548 samples, validate on 1699 samples\n",
      "Epoch 1/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5685 - binary_accuracy: 0.6954 - val_loss: 0.6726 - val_binary_accuracy: 0.6675\n",
      "Epoch 2/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5684 - binary_accuracy: 0.6958 - val_loss: 0.6726 - val_binary_accuracy: 0.6680\n",
      "Train on 2548 samples, validate on 1699 samples\n",
      "Epoch 1/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5683 - binary_accuracy: 0.6958 - val_loss: 0.6727 - val_binary_accuracy: 0.6680\n",
      "Epoch 2/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5683 - binary_accuracy: 0.6958 - val_loss: 0.6727 - val_binary_accuracy: 0.6680\n",
      "Train on 2548 samples, validate on 1699 samples\n",
      "Epoch 1/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5682 - binary_accuracy: 0.6958 - val_loss: 0.6728 - val_binary_accuracy: 0.6675\n",
      "Epoch 2/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5682 - binary_accuracy: 0.6962 - val_loss: 0.6728 - val_binary_accuracy: 0.6663\n",
      "Train on 2548 samples, validate on 1699 samples\n",
      "Epoch 1/300\n",
      "2548/2548 [==============================] - 0s 10us/step - loss: 0.5681 - binary_accuracy: 0.6962 - val_loss: 0.6728 - val_binary_accuracy: 0.6663\n",
      "Epoch 2/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5681 - binary_accuracy: 0.6966 - val_loss: 0.6729 - val_binary_accuracy: 0.6663\n",
      "Train on 2548 samples, validate on 1699 samples\n",
      "Epoch 1/300\n",
      "2548/2548 [==============================] - 0s 10us/step - loss: 0.5680 - binary_accuracy: 0.6966 - val_loss: 0.6729 - val_binary_accuracy: 0.6663\n",
      "Epoch 2/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5679 - binary_accuracy: 0.6966 - val_loss: 0.6730 - val_binary_accuracy: 0.6663\n",
      "Train on 2548 samples, validate on 1699 samples\n",
      "Epoch 1/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5679 - binary_accuracy: 0.6966 - val_loss: 0.6730 - val_binary_accuracy: 0.6663\n",
      "Epoch 2/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5678 - binary_accuracy: 0.6966 - val_loss: 0.6730 - val_binary_accuracy: 0.6669\n",
      "Train on 2548 samples, validate on 1699 samples\n",
      "Epoch 1/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5678 - binary_accuracy: 0.6966 - val_loss: 0.6731 - val_binary_accuracy: 0.6669\n",
      "Epoch 2/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5677 - binary_accuracy: 0.6966 - val_loss: 0.6731 - val_binary_accuracy: 0.6669\n",
      "Train on 2548 samples, validate on 1699 samples\n",
      "Epoch 1/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5677 - binary_accuracy: 0.6970 - val_loss: 0.6732 - val_binary_accuracy: 0.6663\n",
      "Epoch 2/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5676 - binary_accuracy: 0.6974 - val_loss: 0.6732 - val_binary_accuracy: 0.6663\n",
      "Train on 2548 samples, validate on 1699 samples\n",
      "Epoch 1/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5676 - binary_accuracy: 0.6974 - val_loss: 0.6733 - val_binary_accuracy: 0.6663\n",
      "Epoch 2/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5675 - binary_accuracy: 0.6974 - val_loss: 0.6733 - val_binary_accuracy: 0.6663\n",
      "Train on 2548 samples, validate on 1699 samples\n",
      "Epoch 1/300\n",
      "2548/2548 [==============================] - 0s 10us/step - loss: 0.5674 - binary_accuracy: 0.6974 - val_loss: 0.6733 - val_binary_accuracy: 0.6663\n",
      "Epoch 2/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5674 - binary_accuracy: 0.6974 - val_loss: 0.6734 - val_binary_accuracy: 0.6663\n",
      "Train on 2548 samples, validate on 1699 samples\n",
      "Epoch 1/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5673 - binary_accuracy: 0.6974 - val_loss: 0.6734 - val_binary_accuracy: 0.6669\n",
      "Epoch 2/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5673 - binary_accuracy: 0.6974 - val_loss: 0.6735 - val_binary_accuracy: 0.6669\n",
      "Train on 2548 samples, validate on 1699 samples\n",
      "Epoch 1/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5672 - binary_accuracy: 0.6974 - val_loss: 0.6735 - val_binary_accuracy: 0.6669\n",
      "Epoch 2/300\n",
      "2548/2548 [==============================] - 0s 12us/step - loss: 0.5672 - binary_accuracy: 0.6974 - val_loss: 0.6736 - val_binary_accuracy: 0.6669\n",
      "Train on 2548 samples, validate on 1699 samples\n",
      "Epoch 1/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5671 - binary_accuracy: 0.6974 - val_loss: 0.6736 - val_binary_accuracy: 0.6669\n",
      "Epoch 2/300\n",
      "2548/2548 [==============================] - 0s 10us/step - loss: 0.5671 - binary_accuracy: 0.6962 - val_loss: 0.6736 - val_binary_accuracy: 0.6669\n",
      "Train on 2548 samples, validate on 1699 samples\n",
      "Epoch 1/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5670 - binary_accuracy: 0.6962 - val_loss: 0.6737 - val_binary_accuracy: 0.6669\n",
      "Epoch 2/300\n",
      "2548/2548 [==============================] - 0s 13us/step - loss: 0.5669 - binary_accuracy: 0.6962 - val_loss: 0.6737 - val_binary_accuracy: 0.6663\n",
      "Train on 2548 samples, validate on 1699 samples\n",
      "Epoch 1/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5669 - binary_accuracy: 0.6962 - val_loss: 0.6738 - val_binary_accuracy: 0.6663\n",
      "Epoch 2/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5668 - binary_accuracy: 0.6962 - val_loss: 0.6738 - val_binary_accuracy: 0.6663\n",
      "Train on 2548 samples, validate on 1699 samples\n",
      "Epoch 1/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5668 - binary_accuracy: 0.6966 - val_loss: 0.6739 - val_binary_accuracy: 0.6663\n",
      "Epoch 2/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5667 - binary_accuracy: 0.6970 - val_loss: 0.6739 - val_binary_accuracy: 0.6669\n",
      "Train on 2548 samples, validate on 1699 samples\n",
      "Epoch 1/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5667 - binary_accuracy: 0.6966 - val_loss: 0.6739 - val_binary_accuracy: 0.6669\n",
      "Epoch 2/300\n",
      "2548/2548 [==============================] - 0s 11us/step - loss: 0.5666 - binary_accuracy: 0.6966 - val_loss: 0.6740 - val_binary_accuracy: 0.6669\n"
     ]
    }
   ],
   "source": [
    "if train_new_model:\n",
    "    model, scaler = train_model(X_train, y_train)\n",
    "    if not os.path.exists(save_folder):\n",
    "        os.makedirs(save_folder)\n",
    "    for idx,model_i in enumerate(model):\n",
    "        json_string = model_i.to_json()\n",
    "        open(os.path.join(save_folder,'model_'+str(idx)+'.json'), 'w').write(json_string)\n",
    "        model_i.save_weights(os.path.join(save_folder,'model_'+str(idx)+'.h5'))\n",
    "        with open(os.path.join(save_folder,'scaler.pkl'), 'wb') as f:\n",
    "            pickle.dump(scaler, f, pickle.HIGHEST_PROTOCOL)\n",
    "else:\n",
    "    model = []\n",
    "    for idx in range(9):\n",
    "        model.append(model_from_json(open(os.path.join(save_folder,'model_'+str(idx)+'.json')).read()))\n",
    "        model[idx].load_weights(os.path.join(save_folder,'model_'+str(idx)+'.h5'))\n",
    "    with open(os.path.join(save_folder,'scaler.pkl'), 'rb') as input:\n",
    "        scaler = pickle.load(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_norm = scaler.fit_transform(X_train)\n",
    "y_pred = np.hstack( ( model[i_class].predict(X_train_norm) for i_class in range(2) ) )\n",
    "y_pred = np.argmax(y_pred, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = np.sum(y_train==y_pred)/len(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAINING ACCURACY: 68.2%\n",
      "Normalized confusion matrix\n",
      "[[0.75488215 0.24511785]\n",
      " [0.3577118  0.6422882 ]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print('TRAINING ACCURACY: %0.1f%%' % (accuracy*100))\n",
    "cnf_matrix = confusion_matrix(y_train, y_pred)\n",
    "plot_confusion_matrix(cnf_matrix, classes=os.listdir(img_folder), normalize=True,\n",
    "title='Normalized confusion matrix')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
